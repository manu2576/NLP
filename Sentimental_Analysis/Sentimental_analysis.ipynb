{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T20:28:58.351056Z",
     "start_time": "2024-09-08T20:28:58.344199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import chardet\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ],
   "id": "926701ea7a8ee5e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T20:28:58.358151Z",
     "start_time": "2024-09-08T20:28:58.354356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initializing directories\n",
    "mydir = '/Users/mnu/Desktop/NLP_task/Provided_data/Sentimental_analysis_data/StopWords'\n",
    "negativedictionaydir = '/Users/mnu/Desktop/NLP_task/Provided_data/Sentimental_analysis_data/MasterDictionary/negative-words.txt'\n",
    "positivedictionaydir = '/Users/mnu/Desktop/NLP_task/Provided_data/Sentimental_analysis_data/MasterDictionary/positive-words.txt'"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T20:28:58.771433Z",
     "start_time": "2024-09-08T20:28:58.360283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initializing datasets\n",
    "dataset = pd.read_csv('/Users/mnu/Desktop/NLP_task/Web_scraping/web_scraped_data.csv')\n",
    "# Define the column labels\n",
    "output_dataset = pd.read_excel('/Users/mnu/Desktop/NLP_task/Provided_data/Output_Data_Structure.xlsx')"
   ],
   "id": "8977d2095eb0b079",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T20:28:58.810938Z",
     "start_time": "2024-09-08T20:28:58.793373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load stop words from multiple files in a directory\n",
    "def load_stop_words(mydir):\n",
    "    stopwords = set()\n",
    "    folders = os.listdir(mydir)\n",
    "    \n",
    "    for folder in folders:\n",
    "        file_dir = os.path.join(mydir, folder)\n",
    "        \n",
    "        # Detect file encoding\n",
    "        with open(file_dir, 'rb') as file:\n",
    "            raw_data = file.read()\n",
    "            result = chardet.detect(raw_data)\n",
    "            encoding = result['encoding']\n",
    "\n",
    "        # Read stopwords and clean them\n",
    "        with open(file_dir, 'r') as file:\n",
    "            stop_words = file.read().splitlines()\n",
    "            for item in stop_words:\n",
    "                parts = item.split('|')\n",
    "                for part in parts:\n",
    "                    part = re.sub(r' ', '', part)\n",
    "                    stopwords.add(part.lower())\n",
    "    return stopwords\n",
    "\n",
    "# Load a list of words from a file\n",
    "def load_word_list(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        word_list = file.read().splitlines()\n",
    "    return word_list\n",
    "\n",
    "# Remove stop words from a given text\n",
    "def remove_stopwords(plaintext):\n",
    "    cleaned_word = [word for word in plaintext.split() if word.lower() not in stopwords]\n",
    "    return ' '.join(cleaned_word)\n",
    "\n",
    "# Perform sentiment analysis based on positive and negative word lists\n",
    "def positive_negative_score(text, positive_words, negative_words):\n",
    "    words = word_tokenize(text)\n",
    "    positive_score, negative_score = 0.0 , 0.0\n",
    "\n",
    "    for word in words:\n",
    "        if word in positive_words:\n",
    "            positive_score += 1\n",
    "        elif word in negative_words:\n",
    "            negative_score += 1\n",
    "            \n",
    "    return positive_score, negative_score\n",
    "\n",
    "# Count words after stopword removal\n",
    "def counts(text):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts([text])\n",
    "    return len(tokenizer.word_index)"
   ],
   "id": "4d925b84269efadf",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T20:28:58.847989Z",
     "start_time": "2024-09-08T20:28:58.816051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Processing and loading providing textual data\n",
    "stopwords = load_stop_words(mydir)\n",
    "positive_words = load_word_list(positivedictionaydir)\n",
    "negative_words = load_word_list(negativedictionaydir)"
   ],
   "id": "79184a48d9903012",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T20:29:05.071463Z",
     "start_time": "2024-09-08T20:28:58.911316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# implementing Sentimental_Analysis\n",
    "for index, row in dataset.iterrows():\n",
    "    # Extract the article text for the current row\n",
    "    text = row['article_text']\n",
    "    \n",
    "    # Remove stopwords from the text\n",
    "    stopwords_removed = remove_stopwords(text)\n",
    "    \n",
    "    # Recalculate word count for the current row\n",
    "    words_count = counts(stopwords_removed)\n",
    "    \n",
    "    # Calculate positive and negative scores\n",
    "    positive_score = float(positive_negative_score(stopwords_removed, positive_words, negative_words)[0])\n",
    "    negative_score = float(positive_negative_score(stopwords_removed, positive_words, negative_words)[1])\n",
    "    \n",
    "    # Calculate polarity score\n",
    "    Polarity_Score = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)\n",
    "    \n",
    "    # Calculate subjectivity score\n",
    "    Subjectivity_Score = (positive_score + negative_score) / (words_count + 0.000001)\n",
    "    \n",
    "    # Get the corresponding 'URL_ID'\n",
    "    url_id = row['URL_ID']\n",
    "    \n",
    "    # Update the output dataset with the results for the current 'URL_ID'\n",
    "    output_dataset.loc[output_dataset['URL_ID'] == url_id, 'POSITIVE SCORE'] = positive_score\n",
    "    output_dataset.loc[output_dataset['URL_ID'] == url_id, 'NEGATIVE SCORE'] = negative_score\n",
    "    output_dataset.loc[output_dataset['URL_ID'] == url_id, 'POLARITY SCORE'] = Polarity_Score\n",
    "    output_dataset.loc[output_dataset['URL_ID'] == url_id, 'SUBJECTIVITY SCORE'] = Subjectivity_Score\n",
    "\n",
    "# Convert columns to float after all updates\n",
    "output_dataset['POSITIVE SCORE'] = output_dataset['POSITIVE SCORE'].astype(float)\n",
    "output_dataset['NEGATIVE SCORE'] = output_dataset['NEGATIVE SCORE'].astype(float)\n",
    "output_dataset['POLARITY SCORE'] = output_dataset['POLARITY SCORE'].astype(float)\n",
    "output_dataset['SUBJECTIVITY SCORE'] = output_dataset['SUBJECTIVITY SCORE'].astype(float)"
   ],
   "id": "510858a0ef7a36b4",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T08:04:06.815760Z",
     "start_time": "2024-09-09T08:04:06.791580Z"
    }
   },
   "cell_type": "code",
   "source": "output_dataset.to_csv('/Users/mnu/Desktop/NLP_task/Sentimental_Analysis/Output_Data_Structure.csv', index=False)",
   "id": "e19ef6a56fb03580",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "86d0e63a89bca870"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
