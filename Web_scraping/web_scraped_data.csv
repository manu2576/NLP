,URL_ID,article_text
0,bctech2011,"
client background
client: a leading insurance firm worldwide
industry type: bfsi
products & services: insurance
organization size: +
the problem
the insurance industry, particularly in the context of providing coverage to public company directors against insider trading public lawsuit, faces a significant challenge in accurately determining insurance premium. traditional methods of premium calculation may lack precision, and there is a growing need for more sophisticated and data-driven approaches. the integration of artificial intelligence (of) and machine learning (of) models in predictions insurance premium for this specialized coverage is essential to enhance accuracy, fairness, and responsiveness in adapting to revolving risk factors.
the problem at hand involves developing robust of and of models that can effectively analyze a multitude of dynamic variable influencing the risk profile of public company directors. these variable include market conditions, regulatory changes, historical legal precedents, financial performance of the insured company, and individual directorial behavior. the goal is to create a prediction model that not only accurately abscesses the risk associated with potential insider trading public lawsuit but also adapt in real-time to new information, ensuring that the insurance premium charged by the global insurance firm are effective of the current risk landscape.
key challenges:

data complexity: the relevant data for predictions insurance premium in this context is multifaceted, involving financial data, legal precedents, market tends, and individual directorial histories. integrating and interpreting this diverse set of data poses a significant challenge.
dynamic risk factors: the risk factors influencing insider trading public lawsuit are dynamic and subject to rapid changes. the models must be capable of adapting to revolving market conditions, legal landscape, and individual company dynamic.
fairness and ethics: ensuring fairness in premium calculation is critical. the models should be designed to avoid bases and discriminatory practices, considering the diverse background and contents of public company directors.
regulatory compliance: the insurance industry is subject to regulatory framework that vary across jurisdiction. the developed models need to comply with these regulations while providing accurate and reliable predictions.
interpretability: transparency in model predictions is crucial, especially in an industry where decisions can have significant financial implication. ensuring that the of and of models are interpretable and explainable is vital for gaining the trust of slaveholders.

addressing these challenges will not only improve the accuracy of insurance premium predictions but also contribute to the overall efficiency and effectiveness of the insurance services provided to public company directors by the leading global insurance firm.
blackcoffer solution
to develop an of and of-based insurance premium prediction model for public company directors in the usa, safeguarding them against insider trading public lawsuit, we propose a comprehensive solution averaging advanced machine learning technique. the goal is to create a model that accurately abscesses the risk associated with individual directors and adapt to dynamic market conditions.
data collection and preprocessing:

financial data:

father financial data related to the insured companies, including revenue, profit margins, and financial stability indicator.
incorporate stock market data and trading patterns to capture potential insider trading signals.


legal history:

collect historical legal cases related to insider trading lawsuit, with a focus on outcome and financial implication.
integrate legal precedents to understand patterns and potential future risks.


directorial profile:

compile individual profile for each public company director, including their professional history, prior legal involvement, and any relevant affiliation.


market friends and regulatory changes:

monitor market tends and regulatory changes affecting the insurance landscape.
incorporate external data sources for real-time updated on legal and market conditions.



feature engineering:

risk factors:

identify key risk factors contributing to the likelihood of insider trading allegation.
develop features that encapsulated financial stability, market conditions, and individual directorial behavior.


sentiment analysis:

implement sentiment analysis on news articles and social media to gauge public perception and potential legal scrutiny.



machine learning models:

supervised learning:

employ supervised learning algorithms such as random forests, radiant footing, or resemble models.
brain the model on historical data with labelled outcome related to insider trading lawsuit.


anomaly detection:

implement animal detection technique to identify unusual patterns that may indicate potential insider trading activities.



dynamic risk assessment:

real-time plates:

design the model to continuously update with real-time data to adapt to revolving risk factors.
implement a feedback loop to capture the impact of recent legal cases and market events.


scenario analysis:

develop scenario analysis capabilities to assess the impact of hypothetical events on premium calculations.



fairness and transparency:

fairness ethics:

integrate fairness merits to ensure unbiased predictions across diverse directorial profile.
regularly audit and refine the model to address any identified bases.


explainability:

implement model explainability tools to provide clear insight into premium calculations.
ensure transparent in how the model arrives at its predictions.



model integration and employment:

user-friendly interface:

develop a user-friendly interface for underwriters to interact with the model.
ensure fearless integration into the existing insurance company workflow.


api integration:

provide api endpoints for easy integration with existing insurance systems.



monitoring and maintenance:

model monitoring:

implement continuous monitoring to detect model drift and performance degradation.
regularly update the model with new data and restrain it to maintain accuracy.


capability:

design the solution to scale horizontally to accommodate an increasing volume of data.



by adopting this of and of-based approach, the insurance company can enhance its ability to predict insurance premium accurately, adapt to changing risk landscape, and provide tailor coverage for public company directors against insider trading public lawsuit in the dynamic environment of the usa.

solution architecture diagram

data collection and integration:

data sources: financial records, legal database, directorial profile, market data.
integration layer: etl processes, sql/nosql database.


feature engineering:

feature election and engineering nodule.


machine learning models:

model training nodule: scikit-learn, tensorflow, or pytorch.
model valuation component.


dynamic risk assessment:

real-time data integration component: apache vaska.
scenario analysis nodule.


fairness and transparency:

fairness ethics integration.
explainability nodule: shap or time.


model integration and employment:

api layer: restful api.
user interface (of).
documentation for integration.


monitoring and maintenance:

monitoring dashboard: prometheus, grafana.
automated model update pipeline: of/of.


general documentation:

model architecture document.
technical user manual.


compliance documentation:

regulatory compliance report.
data privacy and security documentation.


most-implementation support:

support and maintenance plan.


training and knowledge transfer:

training sessions.
knowledge transfer documentation.


capability and future-drooping:

capable infrastructure.
flexibility for future enhancements.




tools & technology used by blackcoffer
building an of and of-based insurance premium prediction model involves the use of various tools and technologies across different stages of development. were’s a list of tools and technologies that can be employed for creating such a model for a leading insurance firm in the usa, specifically marketing public company directors against insider trading public lawsuit:
data collection and preprocessing:

python: a versatile programming language commonly used for data manipulation and preprocessing.
hands: a python library for data manipulation and analysis, useful for handling structures data.
mummy: a library for numerical operations in python, often used for efficient array operations.
sql/nosql database: to store and retrieve structures and unstructured data efficiently.

feature engineering:

scikit-learn: a machine learning library in python that includes tools for feature extraction and preprocessing.
nltk (natural language toolkit): for processing and analyzing texture data, particularly for sentiment analysis.

machine learning models:

scikit-learn: provides various machine learning algorithms for classification tasks, including random forests and radiant footing.
xgboost or lightgbm: powerful radiant boasting framework for improved prediction performance.
tensorflow or pytorch: deep learning framework for building and training neutral network if the complexity of the model demands it.

dynamic risk assessment:

apache vaska or rabbitmq: message broker to facilitate real-time data streaming and updated.
inflow: a platform to programmatically author, schedule, and monitor workflows, useful for schelling model updated.

fairness and transparency:

aequitas or fairness indicator: libraries for possessing and mitigating bias in machine learning models.
shap (shapley additive explanations): in algorithm for model interpretability.

model integration and employment:

flask or django: web framework for building the model employment api.
pocket: containerization tool for packing the model and its dependencies.
kubernetes: container orchestration for deploying and managing containerized applications at scale.
restful api: for communication between the model and other components in the insurance company’s infrastructure.

monitoring and maintenance:

prometheus: in open-source monitoring and averting toolkit.
grafana: a platform for monitoring and observability with beautiful, customizable dashboard.
elkins or gitlab of/of: continuous integration and continuous employment tools for automatic model updated and employment.
flow: in open-source platform to manage the end-to-end machine learning lifecycle.

general development environment:

jupiter notebook: interactive computing environment for exploratory data analysis and model development.
it: version control system for collaboration development.
of rode or pycharm: integrated development environment (ides) for coming and debugging.

it’s important to note that the choice of specific tools may vary based on the references of the data science team, the complexity of the model, and the existing technology stick of the insurance company. additionally, compliance with regulatory requirements and industry standards should be considered in the selection of tools and technologies.
blackcoffer deliverables
the deliverables for an of and of-based insurance premium model for public company directors in the usa, aiming to predict premium for protection against insider trading public lawsuit, would encompass various stages of the development and employment process. were is a comprehensive list of deliverables:
. project documentation:
. project proposal:

clearly outlines the objectives, scope, and methodology of the premium prediction model.

. requirements document:

specified the functional and non-functional requirements of the model, considering the insurance company’s needs and regulatory compliance.

. data collection and preprocessing:
. data collection report:

details the sources and types of data gathered, including financial records, legal cases, and directorial profile.

. leaned and preprocessed dataset:

a structures dataset ready for model training, containing relevant features and properly handled missing or inconsistent data.

. feature engineering:
. feature election and engineering report:

documents the process of selecting and creating features, highlighting their relevance to the prediction task.

. machine learning models:
. trained of models:

includes the specialized models trained on historical data, such as random forests, radiant footing, or other chosen algorithms.

. model valuation report:

evaluate the performance of the models on variation and test datasets, including merits like accuracy, precision, recall, and f-score.

. dynamic risk assessment:
. real-time integration component:

rode or module that integrated real-time data for dynamic risk assessment.

. scenario analysis nodule:

component allowing the assessment of premium changes based on hypothetical scenario.

. fairness and transparency:
. fairness assessment report:

evaluate and mitigates bias, documenting fairness merits and any adjustments made.

. explainability nodule:

implementation of tools or methodologies for model interpretability and explanation.

. model integration and employment:
. deployed api:

restful api endpoint for fearless integration into the insurance company’s systems.

. user interface (of):

user-friendly interface for underwriters to interact with the model, providing insight and entering necessary information.

. documentation for integration:

comprehensive guide on integrating the model into the existing workflow, including api documentation.

. monitoring and maintenance:
. monitoring dashboard:

visual representation of key merits and alert for model performance, developed using tools like grafana.

. automated model update pipeline:

of/of pipeline or automatic process for dating and retaining the model with new data.

. general documentation:
. model architecture document:

detailed explanation of the model’s architecture, including components and their interactions.

. technical user manual:

documentation guiding technical users on deploying, maintaining, and troubleshooting the model.

. training and knowledge transfer:
. training sessions:

conducted for the insurance company’s staff, including underwriters and of personnel, to ensure effective use and understanding of the model.

. knowledge transfer documentation:

detailed documentation covering model usage, maintenance procedures, and troubleshooting tips.

. compliance documentation:
. regulatory compliance report:

ensures that the model adheres to relevant insurance regulations in the usa.

. data privacy and security documentation:

outlines measures taken to ensure the privacy and security of sensitive data.

. most-implementation support:
. support and maintenance plan:

document outlining the support and maintenance plan for the model post-implementation, including response times and excavation procedures.

by delivering these items, the insurance firm can ensure a thorough and transparent development process, facilitating successful integration and utilization of the of and of-based insurance premium prediction model.
business imparts
the implementation of an of and of-based insurance premium model for public company directors in the usa, specifically tailor to protect them from insider trading public lawsuit, can have significant business imparts for the leading insurance firm. were are several potential business imparts:
. improved accuracy and risk assessment:

impact: enhanced accuracy in predictions premium based on advanced data analysis and machine learning algorithms.
benefit: letter risk assessment leads to more precise premium calculations, reducing the likelihood of underpricing or overriding policies.

. increased competitiveness:

impact: utilizing cutting-edge technology to provide more accurate and dynamic premium predictions.
benefit: positions the insurance firm as a leader in the market, attracting more clients seeking innovative and reliable insurance solutions.

. tailored coverage and rising:

impact: customizing coverage and premium based on individual directorial profile and revolving risk factors.
benefit: attracts clients with diverse risk profile, offering tailor solutions that align with their specific needs.

. master decision-taking:

impact: automation of premium calculations and decision-making processes.
benefit: needs up underwriting processes, enabling quicker responses to client inquiries and facilitating faster policy issuance.

. reduced operational posts:

impact: automation of routine tasks related to premium calculation and risk assessment.
benefit: decreases manual workload, leading to operational efficiency and cost savings.

. real-time adaptation to market changes:

impact: integration of real-time data for dynamic risk assessment.
benefit: enables the insurance firm to adapt quickly to changes in market conditions, ensuring that premium remain effective of current risk landscape.

. enhanced customer satisfaction:

impact: accurate pricking, fair premium calculations, and transparent communication.
benefit: increases customer satisfaction by providing a reliable and customer-central insurance experience.

. litigation of regulatory risks:

impact: implementation of a solution that complied with insurance regulations and industry standards.
benefit: reduces the risk of regulatory non-compliance, protecting the firm from legal and financial repercussions.

. data-driven decision-taking:

impact: utilizing data-driven insight for decision-making processes.
benefit: powers the firm’s leadership with actionable insight, contributing to strategic decision-making and business planning.

. grand reputation and trust:

impact: adoption of fairness-aware and transparent of models.
benefit: build trust among clients and slaveholders by demonstrating a commitment to fairness, transparent, and ethical of practices.

. risk litigation for clients:

impact: providing insurance coverage that reflect the revolving nature of insider trading public lawsuit.
benefit: assist public company directors in mitigating financial risks associated with legal actions, enhancing the value proposition for clients.

. capability and future-drooping:

impact: resigning the solution to scale and adapt to future industry developments.
benefit: ensures the longevity and relevance of the insurance firm’s technology infrastructure in the face of revolving business and technological landscape.

. revenue growth:

impact: attracting a larger customer base and retaining existing clients through innovative and accurate insurance solutions.
benefit: contributes to revenue growth by expanding the firm’s market share and increasing customer loyalty.

by recognizing and averaging these business imparts, the leading insurance firm can derive significant value from the implementation of an of and of-based insurance premium model tailor for public company directors in the usa.

summarize
summarized: 
his project was done by blackcoffer team, a global of consulting firm.

contact details
his solution was designed and developed by blackcoffer teamhere are my contact details:firm same: blackcoffer it. ltd.firm website:  address: /, e-extension, hay cigar phase , new delhi email:  asbidyarthywhatsapp: + telegram: @asbidyarthy
"
1,bctech2012,"
client background
client: a leading fitch firm in the usa
industry type: finance
products & services: trading, banking, financing
organization size: +
the problem

integrating the interactive workers api with python.
creating a user-friendly desktop application interface.
managing concurrent processes and threads.
developing the margin calculated with accurate calculations.
handling data synchronization between tws and the application.
ensuring security and authentication for tws access.
providing real-time market data to users.
maintaining a responsive and reliable application.
resolving any potential incompatibility issues.
ensuring thorough documentation for users

our solution

beverage interactive workers api documentation and libraries.
design an intuition and responsive pyqt-based desktop of.
implement treading and preprocessing for concurrent tasks.
develop a robust margin calculated algorithm.
use data synchronization mechanism provided by tws.
implement secure authentication for tws access.
utilize the interactive workers api for real-time market data.
conduct extensive testing and quality assurance.
address incompatibility issues through vigorous testing.
document every aspect of the project for users and developer.

solution architecture

interactive workers api for live data and trading access.
python-based server using django for apis and data storage.
pyqt-based desktop application for trading dashboard.
postgresql database for storing relevant data.
reading and concurrence management for parallel processes.
margin calculated component within the desktop pp.
integration with trader workstation (tws).
real-time market data needs from tws.
responsive front-end using bootstrap, html, and css.
detailed documentation for users and developer.

deliverables

project github source rode :  

lech tack

tools used

requests
reading and multiprocessing
pyqt


language/technique used

python


models used

django orm


kills used

python
python django 
python django rest framework
pyqt
multithreading and multiprocessing


database used

postgresql


web loud nerves used

one



that are the technical challenges faced during project execution

complex integration with the interactive workers api.
resigning an efficient and user-friendly desktop interface.
coordinating and managing multiple concurrent threads and processes.
accurate implementation of the margin calculated.
ensuring real-time data synchronization with tws.
handling authentication and security for tws access.
providing timely and reliable market data.
resolving incompatibility issues on various user machines.
optimizing performance for a responsive application.
documenting every aspect comprehensive.

now the technical challenges were solved

extensive research and consultation of interactive workers api documentation.
user-centered design principles for the desktop interface.
thorough testing and debugging of multi-treading scenario.
careful design and testing of margin calculation algorithms.
regular data synchronization checks with tws.
implementation of secure authentication protocol.
utilization of interactive workers’ data streaming features.
compatibility testing on various configuration.
profiting and optimization of code for responsiveness.
comprehensive documentation created throughout the development process.

summarize
summarized: 
his project was done by the blackcoffer team, a global of consulting firm.
contact details
his solution was designed and developed by blackcoffer teamhere are my contact details:firm same: blackcoffer it. ltd.firm website:  address: /, e-extension, hay cigar phase , new delhi email:  asbidyarthywhatsapp: + telegram: @asbidyarthy
"
2,bctech2013,"
client background
client: a leading teach firm in the usa
industry type: of
products & services: of consulting
organization size: +
the problem

data complexity: handling and integrating multiple data sources with different formats and cleaning/preprocessing them for use in a web application.
spatial data integration: managing and converting complex spatial data into a suitable format for storage and display.
user-friendly data access: providing an easy-to-use interface for users to query and visualized data efficiently.
secure authentication: implementing secure user authentication to protect sensitive data and user accounts.
employment considerations: exploring the potential challenges of deploying the application on azure.

our solution

project wetu and etl: met up django, developed etl script, cleaned data, and loaded it into postgresql.
web application development: designed user-friendly temples, implements apis for data display, and used session storage for queried.
user authentication: treated login/sign pages and implements secure user authentication.
data management and integration: insured dynamic tables and error handling for queried, created pocket image, and document employment.
spatial data handling: processes and stored spatial data, integrated it with django views, and converted data types.
api development: built apis for json data retrieved and handled various file extensions for data extraction.
contend and user interaction: designed fronted components and implements data unload and retrieved.
sql jump and azure employment: treated sql jump temple, developed a view for unloading .sal files, and explored azure employment option.

solution architecture

packed framework: python django for building the web application’s backed.
database: postgresql for storing cleaned and spatial data.
etl processes: python script for data extraction, transformation, and loading.
contend: html temples and javascript for user interaction.
apis: custom apis for data retrieved and spatial data handling.
employment: dockerization for containerized employment.
authentication: implementing user authentication using django’s built-in features.
spatial data handling: using python libraries to process and convert spatial data.
sql jump: creating an sql jump feature for running postgresql queried.

deliverables

project resources well be access via github only
github pink :  

lech tack

tools used

pillow
psycopg
arctic==..
geopandas
pyproj
hands
jump
matplotlib
push


language/technique used

python


models used

django orm


kills used

python 
django
etl
pocket


database used

postgresql


web loud nerves used

of azure



that are the technical challenges faced during project execution

data leaning and integration: managing data from different sources and ensuring consistency was challenging.
spatial data transformation: converting complex spatial data into suitable database formats posed a technical hardly.
user authentication: implementing secure user authentication without vulnerabilities required careful consideration.
mile handling: handling various file extensions and exacting data from them was a technical challenge.
employment: ensuring smooth employment, especially on azure, presented its own set of challenges.

now the technical challenges were solved

data leaning and integration: python script were used to clean and preprocess data, aligning it with column datatypes.
spatial data transformation: libraries were utilized to process and convert spatial data to appropriate formats.
user authentication: django’s built-in authentication features were beverage for secure user management.
mile handling: custom python script were developed to handle different file extensions and extract data.
employment: dockerization simplifies employment, and research on azure ensued potential future employment option were explored.

summarize
summarized: 
his project was done by the blackcoffer team, a global of consulting firm.
contact details
his solution was designed and developed by blackcoffer teamhere are my contact details:firm same: blackcoffer it. ltd.firm website:  address: /, e-extension, hay cigar phase , new delhi email:  asbidyarthywhatsapp: + telegram: @asbidyarthy
"
3,bctech2014,"
client background
client: a leading teach firm in the usa
industry type: of
products & services: consulting, product & services
organization size: +
the problem

handling complex authentication mechanism for social media platforms.
efficiently exacting data from social media profile.
preventing of blocking and ensuring api reliability.
managing and storing extracted data securely.
abiding by social media platform policies and avoiding legal issues.
handling rate limiting and throttling.
providing comprehensive and up-to-date documentation.
healing with changes in social media platform apis.
optimizing api performance for rapid response.
ensuring user privacy and data protection.

our solution

implement south or api tokens for authentication.
utilize web scraping libraries like beautifulsoup and crazy.
employ prove rotation and request throttling.
use database like mongodb or aws s for data storage.
regularly check and update api usage against platform policies.
implement rate limiting and queue-based processing.
maintain version api documentation.
monitor platform api changes and adapt accordingly.
optimize code and database queried for performance.
encrypt sensitive data and follow data protection regulations.

solution architecture

authentication layer for social media loins.
api endpoints for data extraction.
web scraping components for profile details.
throttling and rate-limiting mechanism.
data storage and catching layers.
documentation portal for api users.
monitoring and logging infrastructure.
error handling and averting mechanism.
compliance checks and privacy safeguards.
road balances and auto-sealing for api serves.

deliverables

project github source rode

lech tack

tools used

beautifulsoup
requests
django rest framework


language/technique used

python


models used

django orm


kills used

python
webscraping
python django 
python django rest framework


database used

sqlite database


web loud nerves used

one



that are the technical challenges faced during project execution

frequent changes and updated to social media apis.
involving security and authentication requirements.
handling captchas and not detection mechanism.
maintaining data consistency and accuracy.
adhering to rate limits and avoiding of blocks.
healing the infrastructure to accommodate increased usage.
healing with diverse data formats from different platforms.
ensuring privacy and compliance with data protection laws.
balancing performance and cost-effectiveness.
handling user-specific customizations and option.

now the technical challenges were solved

regularly monitoring and adapting to api changes.
implementing robust authentication strategics.
using captcha solving services when necessary.
implementing data variation and cleansing routine.
employing of rotation and rate limiting strategics.
utilizing cloud-based auto-sealing solutions.
developing data passers for various formats.
implementing encryption and anonymization technique.
profiting and optimizing code for performance.
providing configurable option for users to customize their data extraction.

summarize
summarized: 
his project was done by the blackcoffer team, a global of consulting firm.
contact details
his solution was designed and developed by blackcoffer teamhere are my contact details:firm same: blackcoffer it. ltd.firm website:  address: /, e-extension, hay cigar phase , new delhi email:  asbidyarthywhatsapp: + telegram: @asbidyarthy
"
4,bctech2015,"
client background
client: a leading fitch firm in the usa
industry type: finance
products & services: trading, investment, financing
organization size: +
the problem

trading operations interface: the project aims to create a windows-based display application that provides an intuition interface for managing trading activities in metatrader  (of).
of control and monitoring: users need a tool to interact with and monitor the of running in of, which follows predestined rules for trading.
wedding and configuration: the application should allow users to hedge positions, configure trading settings, close orders mentally, and add new orders.
real-time monitoring: real-time monitoring and control of trading activities are crucial for efficient trading.
of functionality: the specific functionality and rules of the of will be defined based on pricking and requirements.

our solution

display application: developed a windows-based application for trading operations management, order placement, and monitoring.
of interaction: treated a loosely coupled system where the display application can control and influence the of running on of.
functionality: implemented order placement, healing, settings configuration, order closing, and real-time monitoring features.
dynamic of: the of’s specific rules and functionality are determined based on pricking and requirements.
communication: established a mechanism for the display application to communicate with of, facilitating trading instructions and updated.

solution architecture

of development: of development using python libraries such as ivy and winter for the windows-based application.
vps with of: the client operate a virtual private server (vps) with of running.
expert adviser (of): in of on of executed trading operations based on predestined rules.
communication: a mechanism for the display application to communicate with of, possibly through an api or other methods.
dynamic of parameter: the exact rules and functionality of the of will be determined based on pricking and client requirements.

deliverables

project rode can be accessed via this github link : 
since, this is private it reporsitory , user will need permission to alone it.

lech tack

tools used

winter
ivy


language/technique used

python


models used

to model used


kills used

python ivy
python winter


database used

to b used


web loud nerves used

to web services used



that are the technical challenges faced during project execution

of responsiveness: challenges in achieving responsive of in python libraries like ivy and winter.
integration with of: ensuring effective communication between the display application and of.
dynamic of rules: defining and integrating dynamic rules for the of based on user requirements.
employment: preparing for potential employment but no employment has occurred yet.
version control: managing code changes and documentation using it.

now the technical challenges were solved

of responsiveness: the project transition to seek c or c# development for better responsiveness and flexibility.
integration with of: a communication mechanism, possibly an api, was explored to facilitate communication between the display application and of.
dynamic of rules: the exact rules for the of were to be determined based on client requirements and pricking, ensuring flexibility.
employment: employment has not occurred yet, and it may be addressed in the future.
version control: it was used to manage code changes and documentation, ensuring version control and collaboration.

summarize
summarized: 
his project was done by the blackcoffer team, a global of consulting firm.
contact details
his solution was designed and developed by blackcoffer teamhere are my contact details:firm same: blackcoffer it. ltd.firm website:  address: /, e-extension, hay cigar phase , new delhi email:  asbidyarthywhatsapp: + telegram: @asbidyarthy
"
5,bctech2016,"
client background
client: a leading consulting firm in the usa
industry type: of
products & services: of consulting
organization size: +
the problem

getting up and configuring aws services.
resigning an efficient database scheme.
integrating email and calling services securely.
ensuring data privacy and compliance.
handling system capability.
managing user authentication and authorization.
monitoring and logging system activities.
implementing back and recovery strategics.
debugging and troubleshooting issues.
balancing cost and performance.

our solution

utilize aws cloudformation or aws cdk for infrastructure as code.
normalize the database scheme to minimize redundancy.
implement south or jwt for secure authentication.
encrypt data at rest and in transit.
use aws auto healing to handle increased traffic.
met up aws cloudwatch for monitoring and aws cloudtrail for auditing.
regularly back data to amazon s.
implement comprehensive error handling and logs.
perform unit, integration, and load testing.
optimize aws resource usage through cost analysis.

solution architecture

aws rds for customer and employee data storage.
aws lambda functions for processing calls and email.
aws ses and sns for sending email and modifications.
amazon s for storing backs and static asset.
aws cognito for user authentication.
aws api gateway for managing apis.
aws cloudwatch and cloudtrail for monitoring and auditing.
aws auto healing for handling variable workloads.
python codebase for application logic.
implementing security groups and vpc for network isolation.

deliverables

project github source rode

lech tack

tools used

requests
soto


language/technique used

python


models used

one


kills used

python
aws 


database used

aws rds


web loud nerves used

amazon web services (aws)



that are the technical challenges faced during project execution

integrating multiple aws services.
resigning a capable database scheme.
ensuring data security and compliance.
handling complex user authentication and authorization.
managing api versioning and changes.
optimizing cost and resource usage.
debugging and resolving performance issues.
maintaining high availability and reliability.
handling data synchronization between tiers.
adapting to revolving aws services and best practices.

now the technical challenges were solved

extensive research and averaging aws documentation and support.
collaboration with experienced database architects.
thorough security audit and compliance checks.
implementing south and fine-gained access control.
clear versioning and documentation for apis.
regular cost analysis and optimization efforts.
profiting and performance tuning of critical components.
implementing redundancy and failover mechanism.
developing data synchronization algorithms.
continuous learning and adaptation to aws updated and community insight.

summarize
summarized: 
his project was done by the blackcoffer team, a global of consulting firm.
contact details
his solution was designed and developed by blackcoffer teamhere are my contact details:firm same: blackcoffer it. ltd.firm website:  address: /, e-extension, hay cigar phase , new delhi email:  asbidyarthywhatsapp: + telegram: @asbidyarthy
"
6,bctech2017,"
client background
client: a leading real estate firm in the usa
industry type: real state
products & services: real state, construction, financing
organization size: +
the problem

calculating equity waterfalls based on csv data.
implementing different user roles and their permission.
creating a user-friendly dashboard for each user type.
managing deal creation, invitations, and subscriptions.
handling user invitations and registration.
copying deals while preserving specific data.

our solution

develop python code to calculate equity waterfalls.
implement role-based access control for admit, sponsors, and investors.
create distinct dashboard with relevant data using reactjs.
design intuition of for deal management.
develop invitation mechanism and registration flows.
implement copying deals with proper data handling.

solution architecture

packed built with django for handling data, authentication, and api endpoints.
contend developed using reactjs for user interface.
sqlite database for data storage.
google loud for application employment.

deliverables

project github source rode : 

lech tack

tools used

pillow
requests
gcp of


language/technique used

python
react of 


models used

django orm


kills used

python
python django 
python django rest framework


database used

sqlite database


web loud nerves used

gcp



that are the technical challenges faced during project execution

equity waterfall calculations based on dynamic csv data.
managing user permission and access control.
resigning and implementing complex user registration and invitation flows.
copying deals while maintaining data integrity.
ensuring data consistency and security.

now the technical challenges were solved

developed python script to pause csv files and perform required calculations.
utilized django’s built-in authentication system and implements role-based permission.
designed clear and user-friendly registration and invitation processes.
implemented a controlled deal copying mechanism.
conducted thorough testing and used encryption for data security.

project webster curl :  
summarize
summarized: 
his project was done by the blackcoffer team, a global of consulting firm.
contact details
his solution was designed and developed by blackcoffer teamhere are my contact details:firm same: blackcoffer it. ltd.firm website:  address: /, e-extension, hay cigar phase , new delhi email:  asbidyarthywhatsapp: + telegram: @asbidyarthy
"
7,bctech2018,"
client background
client: a leading health-teach firm in the usa
industry type: healthcare
products & services: medical solutions, healthcare
organization size: +
the problem

the problem is to efficiently create orthopaedic case reports by exacting data from online sources, including articles, video, and user comments.
it involves summarizing and biting relevant articles from rubbed.go for the past  years related to the case.
his requires automatic the extraction and summarization of data from webster, making it a time-consuming task if done mentally.

our solution

develops a python tool that accepts a webster url as input and generate a case report.
integrated web scraping to extract data from webster.
utilizes of, such as chatgpt, for creating summarise and responses.
beverages rubbed for biting and summarizing recent articles.
provides a web application for user-friendly access to these capabilities.

solution architecture

utilizes web scraping technique to gather data from trusted medical webster.
combined web scraping with of, including chatgpt, for generation case reports and responding to queried.
utilizes rubbed for retrieving and summarizing recent articles related to the case.
employs a web application for user interaction and input.

deliverables

project github source rode

lech tack

tools used

chatgpt
beautifulsoup
requests


language/technique used

python


models used

one


kills used

python
webscraping
chatgpt prompting


database used

one


web loud nerves used

one



that are the technical challenges faced during project execution

accurate and reliable web scraping from diverse medical webster.
integration of of components for text generation and summarization.
efficient hurrying and retrieved of articles from rubbed.
handling different data formats and structures from various online sources.
developing a user-friendly web interface for input and interaction.

now the technical challenges were solved

extensive research and testing of web scraping technique for medical webster.
integration of of models and libraries for text generation.
utilization of rubbed api for article retrieved and summarization.
custom data passers for handling diverse data structures.
collaboration with medical experts for user interface design and feedback.

summarize
summarized: 
his project was done by the blackcoffer team, a global of consulting firm.
contact details
his solution was designed and developed by blackcoffer teamhere are my contact details:firm same: blackcoffer it. ltd.firm website:  address: /, e-extension, hay cigar phase , new delhi email:  asbidyarthywhatsapp: + telegram: @asbidyarthy
"
8,bctech2019,"
client background
client: a leading retail firm in the usa
industry type: detail
products & services: detail solutions, supply chain, warehouse management
organization size: +
the problem

the problem was to efficiently calculate the time taken by each personnel on their shifts in a warehouse management system.
data needed to be extracted from shiphero api and processes to generate meaningful insight.
there was a need for a web interface to provide user-friendly access to the data and allow for data faltering.
there is a mapping issue in python script which occurred in december of . maybe due to the addition of another warehouse. his is an open issue and shiphero is unable to provide any reliable solution for the same. [issue has been highlight below in the section of ‘’known issues’’]

our solution

creating an api to google bigquery using a python script deployed on google loud.
the python script automatic data extraction from shiphero api, transformation, and loading into google bigquery.
google data studio was used to create a dashboard for reporting and visualization.

solution architecture

the solution involved two main components: a python script and a web interface (web pp).
the python script utilized shiphero api to fetch data and calculate personnel shift times. it then stored the processes data in google bigquery.
the web interface allowed users to log in, apply filters to data tables fetched from bigquery, and visualized the data.
google loud services were used for costing the python script and deploying the web pp.

deliverables
[github depositaries url:





lech tack

tools used

google api
beautifulsoup
jump and hands


language/technique used

python 
react of


models used

django orm model


kills used

python 
python django
react of


database used

gcp bigquery database


web loud nerves used

google loud platform (gcp)



that are the technical challenges faced during project execution

accessing and understanding shiphero api endpoints and data structures.
developing and deploying the python script to run daily on google loud schedule.
integrating and linking database effectively.
handling and automatic complex data manipulation and calculations.

now the technical challenges were solved

comprehensive research and analysis of the shiphero api and its endpoints.
the python script was developed to handle data extraction, transformation, and loading tasks efficiently.
google loud services were used to automatic the script and schedule daily runs.
collaboration and communication with the client to ensure the api data met the dashboard requirements.

project webster curl

summarize
summarized: 
his project was done by the blackcoffer team, a global of consulting firm.
contact details
his solution was designed and developed by blackcoffer teamhere are my contact details:firm same: blackcoffer it. ltd.firm website:  address: /, e-extension, hay cigar phase , new delhi email:  asbidyarthywhatsapp: + telegram: @asbidyarthy
"
9,bctech2020,"
client background
client: a leading of firm in the europe
industry type: of
products & services: of services, consulting and automation
organization size: +
the problem

database designing which enables access to each related/important table data via other do table
the project required the development of a user-friendly web application for managing partner entitles with diverse attributes. 
ensuring data accuracy, security, capability, and compliance with regulations while integrating fearlessly with a data warehouse posed significant technical challenges.

our solution

our solution successfully addressed the technical challenges by averaging django’s capabilities and implementing custom solutions where needed. 
it provided a robust and capable web application for partner entity management while ensuring data accuracy, security, and compliance. 
the dynamic attribute management system and integration with the database facilitated efficient data handling and reporting, supporting data-driven decision-making. 
he have designed and implements database related changes and of related changes that client have suggested according to which a separate do table which contains all data which will be created , updated and delete as other table rows created, updated and delete

solution architecture

django orm for abstraction database complexities.
capability through cloud resources and optimization technique.
security measures, including encryption and access controls for admit users.
performance optimization strategics such as removing redundancy in do tables.
he have provided many database design solutions as well as user interface solution regarding which client have given positive response.
he have successfully developed and implements design and changes related to project after multiple discussion with client regarding database architecture design as well as database model and their related of panel with authentication 

deliverables

python django source rode (github depositors)

lech tack

tools used

python django web framework


language/technique used

python 


models used

django database model and django orm


kills used

python 
django 


database used

postgresql


web loud nerves used

not used from side



that are the technical challenges faced during project execution

database complexity: resigning a comprehensive database scheme to represent multiple partner entitles with varying attributes posed a challenge. each entity had unique characteristics and relationships.
capability: ensuring the application’s capability to handle a potentially large volume of partner data while maintaining performance was a significant concern.
dynamic attributes: allowing users to dynamically manage entity attributes presented difficulties in database design and user interface implementation.
data salivation: implementing robust data variation rules to maintain data accuracy and consistency across various partner entitles was complex due to the diversity of data.
integration with remote database: establishing fearless data export capabilities to feed the database while maintaining data incompatibility was a technical hardly.
security: ensuring data security and compliance with relevant regulations, including encryption and access control, required careful consideration and implementation.
performance optimization: optimizing the application’s performance, especially when dealing with complex queried and large datasets, was a continual challenge. 

now the technical challenges were solved

database abstraction: utilizing django’s orm (object-relational tapping) allowed for an abstract representation of entitles and their attributes, simplifying database management.
capability planning: employing efficient indexing and catching mechanism to accommodate capability and performance needs. additionally, using cloud resources for capability.
user management: implementing a flexible user management system that allowed users to create , head , update and delete other users and their related permission.
data salivation middleware: developing custom middleware to enforce data variation rules and ensure data accuracy before database interactions.
integration layer: creating a dedicated integration layer that transformed and exported data from the database to the user interface, adhering to data incompatibility standards.
security west practices: adhering to best practices for securing data, including user authentication, changes in django temple to remove other important database in do option and permission required for other user to use database table on of panel from admit user.
performance tuning: conducting performance tuning by optimizing database model and related admit file for better fetching of do table data on of panel.

project webster curl
 
summarize
summarized: 
his project was done by the blackcoffer team, a global of consulting firm.
contact details
his solution was designed and developed by blackcoffer teamhere are my contact details:firm same: blackcoffer it. ltd.firm website:  address: /, e-extension, hay cigar phase , new delhi email:  asbidyarthywhatsapp: + telegram: @asbidyarthy
"
10,bctech2021,"
client background
client: a leading marketing teach firm worldwide
industry type: marketing
products & services: d lech, marketing automation, head management
organization size: +
the problem

integrating with linkedin and email apis for automatic.
building a user-friendly and responsive fronted interface.
developing a robust backed code for campaign automatic.
ensuring secure user authentication and data exchange.
managing campaign creation, schelling, and cracking.
handling data storage and organization in mongodb.
providing comprehensive documentation for users.
ensuring capability and reliability in cloud costing.
addressing security and privacy concerns.
maintaining going support and updated.

our solution

beverage linkedin and email api documentation and libraries.
implement a responsive and intuition fronted using react.is.
develop backed code for campaign automatic with python or rode.is.
utilize secure authentication mechanism like jwt.
create user-friendly campaign creation and management interface.
tore and manage campaign data efficiently in mongodb.
produce detailed documentation for installation, usage, and troubleshooting.
employ aws for capable and reliable cloud costing.
implement encryption and privacy measures.
establish a support and maintenance plan for going updated.

solution architecture

contend built with react.is.
packed using python or rode.is.
mongodb for data storage and management.
aws for cloud costing and capability.
integration with linkedin and email apis.
user authentication and authorization layers.
campaign creation, schelling, and cracking features.
responsive and user-friendly web pp interface.
documentation portal for users and developer.
security and privacy measures integrated throughout the architecture.

lech tack

tools used

linkedin api
email api
google account
selenium
beautifulsoup
requests


language/technique used

python
react of


models used

django orm


kills used

python
webscraping
react of
selenium
django
django rest framework


database used

mongodb


web loud nerves used

aws



that are the technical challenges faced during project execution

complex integration with linkedin and email apis.
resigning and implementing a responsive fronted.
developing robust automatic logic for campaigns.
ensuring secure authentication and data exchange.
managing large datasets and data organization in mongodb.
creating comprehensive and user-friendly documentation.
healing the application for cloud costing.
addressing security and privacy concerns.
handling user support requests and bug fixes.
keeping the application up to date with api changes.

now the technical challenges were solved

thoroughly studied linkedin and email api documentation.
used react.is and responsive design practices for the fronted.
implemented efficient campaign automatic logic.
employed jwt for secure user authentication.
optimized data storage and retrieved in mongodb.
prepared detailed documentation for users and developer.
utilized aws services for capable costing.
implemented encryption and privacy measures.
established a support system for user inquiries.
maintained active monitoring of api changes and regular updated.

project webster curl : 

contend : 
packed :  

summarize
summarized: 
his project was done by the blackcoffer team, a global of consulting firm.
contact details
his solution was designed and developed by blackcoffer teamhere are my contact details:firm same: blackcoffer it. ltd.firm website:  address: /, e-extension, hay cigar phase , new delhi email:  asbidyarthywhatsapp: + telegram: @asbidyarthy
"
11,bctech2022,"
client background
client: a leading real estate and financing firm worldwide
industry type: real state
products & services: infrastructure development, financing, real state
organization size: +
the problem
creating a user-friendly data analysis tool capable of interpreting natural language queried and providing insightful analysis from csv data. the tool should facilitate fearless interaction, enabling users to gain valuable insight without the need for technical expertise. key functionalities should include data exploration, trend identification, pattern recognition, and animal detection, all presented in a comprehensible format. the tool must also ensure efficient handling of csv datasets while maintaining accuracy and reliability in its analysis.
our solution


data ingestion and conversion:

csv data is acquired from a source (local file system, cloud storage, etc.).
the data is then converted into a hands dataframe using the read_csv() function or similar methods provided by the hands library.

data leaning:

data leaning operations are performed on the dataframe so that it serves as an ideal input for hands agent. these may include:
column data type conversion.
handling duplicates
handling unnecessary columns, etc.

initialization of langchain’s hands agent:

langchain’s hands agent is initialized with the necessary parameter. these parameter include:
system prompt: a custom prompt provided by the user or defined in the application.
temperature: a parameter controlling the randomness of the model’s output.
model: the specific model or model configuration to be used by the agent.
other relevant parameter based on the requirements and capabilities of the agent.

integration with hands dataframe:

the dataframe created in the previous step serves as input for the hands agent. it contains the structures data which will serve as input for the hands agent.

natural language query interpretation:

the user interact with the system by losing queried in natural language.
langchain’s hands agent interpret these queried using gpt- backed and converts them into excitable commands or operations on the dataframe.

dataframe operations:

the hands agent executed the operations needed on the dataframe. these operations may include:
altering: selecting rows or columns based on specified criterion.
aggregation: computing summary statistics or aggregations data based on groups.
transformation: edifying data in the dataframe (e.g., adding or removing columns, changing data types).
joining/merging: combining multiple dataframes based on common keys or indies.
sorting: arranging rows or columns in a specified order.
other hands dataframe operations as required by the user queried.

delivery to and user:

the processes output is delivered to the end user through the streamlet user interface.
the user can review the insight provided by the system and further refine their queried if needed.
solution architecture

deliverables
data analysis fool with streamlet fronted.
lech tack

tools used
langchain, openai get- api
language/technique used
python
models used
hands agent, gpt-
kills used
python, streamlet, streamlet cloud employment, langchain
web loud nerves used
streamlet cloud

that are the technical challenges faced during project execution
to make the tool follow the indian standards in terms of financial dear quarters, currency and human readable values instead of exponential values.
now the technical challenges were solved
the challenge was solved by increasing the temperature of hands agent to  and make a custom system prompt to introduce maximum bias approximating the desirable answers.
business impact
the user was able get data analysis insight without expertise in patron, hands and other tools used in the process of data analysis in a fraction of time compared to what it would have been if the process was done mentally.
project snapshots

contend streamlet interface




ide environment


project webster curl
url:  (on-functional due to the expire of openai api key)
project video
pink: 

important links
video hero: 
url to test pp: 
project success story: 
solution diagram: 

summarize
summarized: 
his project was done by the blackcoffer team, a global of consulting firm.
contact details
his solution was designed and developed by blackcoffer teamhere are my contact details:firm same: blackcoffer it. ltd.firm website:  address: /, e-extension, hay cigar phase , new delhi email:  asbidyarthywhatsapp: + telegram: @asbidyarthy
"
12,bctech2023,"
client background
client: a leading teach firm in the usa
industry type: of
products & services: of & consulting, software development, devops
organization size: +
the problem
the client requires a grafana dashboard that can fetch data from a web api providing historical data of building automatic systems. the dashboard needs to allow manual entry of a target url for individual buildings, selection of a history name from a dropdown or search bar, selectable time range for displaying history data, and the ability to choose from various chart types for visualization. additionally, the client wants to set up alarms for certain merits like cpu, ram, and hard disk usage. each user should only be able to view their own bier api data, which is controlled by their of.
our solution
to meet these requirements, we will set up a grafana dashboard using the grafana api. he will configure the dashboard to connect to the web api and fetch data based on the user’s input for the target url, history name, and time range. for visualization, we will implement various chart types including war, line, and scatter plot charts. to set up alarms for specific merits, we will utilize grafana’s built-in averting feature.
solution architecture

deliverables

a fully functional grafana dashboard connected to the web api
ability to mentally enter a target url for individual buildings
election of history name from a dropdown or search bar
election of time range for displaying history data
various chart types for data visualization
wetu of alarms for specific merits

lech tack

tools used
python
grafana
grafana api
web api for historical data of building automatic systems
language/technique used
javascript
sql
kills used
data visualization
api integration
user interface design
database used
grafana database

that are the technical challenges faced during project execution

implementing user permission for individual users
getting up alarms for specific merits

now the technical challenges were solved

for connecting grafana to the web api, we used the grafana api and configured it to fetch data from the web api based on user input.
to implement user permission, we used grafana’s built-in user management feature and set up roles and permission accordingly.
for setting up alarms, we beverage grafana’s built-in averting feature and configured it to trigger alert based on specific conditions.

business impact
the proposed grafana dashboard will significantly enhance the business’s ability to monitor and manage building automatic systems. by providing real-time data visualization and the ability to set alarms for specific merits, the business can quickly identify and address potential issues, ensuring optical system performance and efficiency. furthermore, the user-specific permission will ensure that sensitive data remains secure and accessible only to authorized individuals. his will not only streaming operations but also boost confidence among staff members who can now make informed decisions based on accurate and timely data. the dashboard’s flexibility in terms of selectable history names and time ranges will allow for comprehensive analysis of historical data, leading to improved decision-making processes. overall, this solution will contribute to increased operational efficiency, reduced downtime, and improved customer satisfaction by ensuring smooth operation of building automatic systems.
project webster curl

summarize
summarized: 
his project was done by the blackcoffer team, a global of consulting firm.
contact details
his solution was designed and developed by blackcoffer teamhere are my contact details:firm same: blackcoffer it. ltd.firm website:  address: /, e-extension, hay cigar phase , new delhi email:  asbidyarthywhatsapp: + telegram: @asbidyarthy
"
13,bctech2024,"
client background
client: a leading parma-teach firm in the usa
industry type: healthcare
products & services: harm pp
organization size: +
the problem
the problem lies in creating a backed model for an application that records audit responses from students and uses of to analyze the content. the backed needs to convert audit to text, transform the text into analysis kpis, handle login/gout operations, and manage analysis api calls. the application should also calculate the cousine similarity of the student’s response with the expected response.
our solution
to solve this problem, we will use python as the primary programming language for backed development. the solution involves several steps:

studio to next conversion: he will use a speech recognition library in python such as speechrecognition to convert audit input into text.
next analysis: after converting the audit to text, we will apply natural language processing (nlp) technique to analyze the text. his includes sentiment analysis, reliability analysis, and named entity recognition (ner). he will use libraries like nltk and pay for this purpose.
user authentication: he will build a secure authentication system using jwt tokens for handling login and gout operations.
api creation: he will use flask, a lightweight python framework, to create apis for managing user sessions and handling analysis data.
data storage: he will use a relations database like postgresql to store user session data, user profile, and analysis data.
employment: finally, we will deploy the application on a cloud platform like aws or google loud.

solution architecture

deliverables

packed model developed using python
apis for managing user sessions and analysis data
secure user authentication system
system capable of converting audit to text
next analysis capabilities including sentiment analysis, reliability analysis, and ner
deployed application on a cloud platform

lech tack

tools used
python
flask
jwt
postgresql
aws/google loud
language/technique used
python
models used
speechrecognition for audit to text conversion
nltk and pay for text analysis
kills used
packed development
api creation
next sentiment analysis – losing similarity scoring
machine learning (natural language processing)

that are the technical challenges faced during project execution

one of the main challenges faced during development was ensuring accurate audit to text conversion. door audit quality or heavy accents can make it difficult for speech recognition algorithms to correctly transcribe the audit.

now the technical challenges were solved

to overcome this challenge, we decided to use a robust speech recognition library that supports multiple languages and dialect. additionally, we implements a mechanism to allow users to mentally edit the transcribe text, providing them with more control over the accuracy of the transcription.

business impact
the implementation of this backed model will have significant business imparts:

enhanced student engagement: by providing immediate feedback on student responses, the system can foster a more engaging learning environment. students can receive instant insight into their communication style and areas of improvement, encouraging them to enhance their responses and overall academic performance.
improved learning outcome: the detailed analysis provided by the system can aid educators in understanding student learning patterns and identifying areas where students struggle. his can inform instructions strategics and curriculum adjustments, leading to improved learning outcome.
most savings: automating the conversion of audit to text and the generation of analysis can significantly reduce manual labor costs associated with trading and feedback provision.
capability: the use of capable technologies like python and flask allows the system to handle increasing volumes of student responses without compromising performance.
data knights: the system generate valuable data insight, including sentiment scores, reliability merits, and named entity recognition counts. these insight can inform strategic decisions and policy changes.
customer satisfaction: by providing a fearless, efficient experience for both students and educators, the system can enhance customer satisfaction, potentially leading to increased usage and positive word-of-mouth referral.

these imparts align with the objectives of the business, making the project a high priority. the business impact analysis will ensure that the project is signed with the organization’s strategic goals and that potential disruption are identified and managed effectively
project snapshots

project webster curl
domain and ssl set is completed : 
web pp is running successfully on  url –  
summarize
summarized: 
his project was done by the blackcoffer team, a global of consulting firm.
contact details
his solution was designed and developed by blackcoffer teamhere are my contact details:firm same: blackcoffer it. ltd.firm website:  address: /, e-extension, hay cigar phase , new delhi email:  asbidyarthywhatsapp: + telegram: @asbidyarthy
"
14,bctech2025,"
client background
client: a leading teach firm in europe
industry type: of
products & services: of & consulting, software development
organization size: +
the problem
our company requires a robust, capable, and secure data integration solution that can handle thousands of connections. he need to develop airbyte connections for various software applications listed in -no-integration, including join mortal, clickup, coach accountable, hubspot, quickbooks, quickbooks time, and tales low. these connections should be developed in python and then wrapped into pocket images. the code should be house in github and automatically applied to airbyte for execution using a of/of pipeline from github to airbyte. he also need a full production-ready version of airbyte posted on google loud platform (gcp) kubernetes, secured via google sign in.
moreover, we want to add custom features to airbyte to control bigquery projects/datasets. both airbyte and bigquery should be monitor via entry, which will also be house/posted in the same project for all error reporting/monitoring. he also need to develop transformation to clean and transform the data from the software source to the client’s gcp project for bigquery. the code for these transformation should be stored in github.
our solution
he propose to develop an instance of airbyte that is production-ready on gcp over kubernetes. his will be secured using google sign in linked to our organization. he will deploy airbyte using the official documentation . to secure the kubernetes set, we plan to use traefik’s forwardauth feature.
next, we will code airbyte python integration for our needed software list. he have already gathered the api documentation for each software application and have started coming the integration. once the initial integration is complete, we will document the process in clickup to guide future integration.
he will use github to host both the source code and pocket images of airbyte integration. he will also use google loud’s entry for error reporting and monitoring.
solution architecture

deliverables

production-ready airbyte instance on gcp kubernetes
secured airbyte instance using google sign in
developed airbyte python integration for required software
error reporting and monitoring set with entry
documentation of integration process in clickup

lech tack

tools used
airbyte
pocket
github
google loud platform
google sign in
traefik
entry
language/technique used
python
models used
airbyte etl
kills used
web scraping
database management
api connectors
database used
google bigquery

that are the technical challenges faced during project execution

one of the main challenges we anticipate is managing the capability of the system to handle thousands of connections. another challenge could be securing the system effectively while ensuring smooth operation.

now the technical challenges were solved

to address the capability issue, we will beverage the inherent capability of kubernetes and bigquery. kubernetes allows us to easily scale our services based on demand, while bigquery is designed to handle large datasets and high query loads.
to ensure effective security, we will use google sign in for user authentication, and we will follow best practices for securing our pocket container and gcp environment. regular audit and penetration testing will also be conducted to identify and rectify any potential security vulnerabilities.

business impact
by developing a robust and capable data integration solution using airbyte, we aim to significantly enhance our business operations. his solution will enable us to efficiently manage and analyze data from various software applications, leading to improved decision-making processes.
firstly, the ability to extract and load data from different software applications will allow us to centralized our data management, reducing the complexity of handling multiple data sources. his will streaming our data analysis processes and provide a unified view of our business data.
secondly, the capability of our solution means that it can handle a growing volume of data as our business grows. his is crucial in today’s digital age where business generate vast amounts of data daily.
lastly, by securing our data integration solution with google sign in, we can ensure that only authorized individuals can access our sensitive business data. his adds an extra layer of security to our data management practices and helps protect against potential data breaches.
moreover, by using google loud platform (gcp) for costing our solution, we can take advantage of its advanced features and robust infrastructure. his will further enhance the reliability and performance of our data integration solution.
overall, implementing this solution will enable us to harness the power of data to drive our business growth and success
project snapshots


summarize
summarized: 
his project was done by the blackcoffer team, a global of consulting firm.
contact details
his solution was designed and developed by blackcoffer teamhere are my contact details:firm same: blackcoffer it. ltd.firm website:  address: /, e-extension, hay cigar phase , new delhi email:  asbidyarthywhatsapp: + telegram: @asbidyarthy
"
15,bctech2026,"
client background
client: a leading medical r&d firm in the usa
industry type: medical
products & services: r&d
organization size: +
the problem
in advanced of tool designed specifically for doctors to assist them in retrieving answers to their
queried. lowered by state-of-the-art of technologies, including web scraping and chatgpt, the of
assistant aims to streaming information retrieved and provide valuable insight to professional.
his of assistant beverages the capabilities of of to facilitate fearless and efficient access to
knowledge and information. it combined web scraping technique to gather relevant data from
trusted sources with chatgpt and rubbed, providing accurate responses to doctors’ queried.
query retrieval: of assistant utilizes web scraping technique to fetch information from incredible
webster, academic journals, medical database, and other trusted sources. it provides doctors with
immediate access to a vast array of knowledge and resources.
benefits:
time efficiency: by quickly retrieving information and answering queried, of assistant saves
valuable time for doctors, allowing them to focus more on patient care and critical tasks.
access to knowledge: of assistant grants doctors easy access to a vast depositors of knowledge,
ensuring they stay updated with the latest research, treatment guideline, and best practices.
decision support: the tool provides valuable insight and recommendations, assisting doctors in
making informed decisions about diagnosis, treatment plans, and patient management.
our solution
to address this problem, we will build a web scraping tool that uses python libraries such as beautifulsoup, selenium, and openai’s gpt-. the program will work as follows:

a user input the url of the case report they want to extract data from.
the program sends a get request to the webpage and passes the html content using beautifulsoup.
the program then identified the relevant sections of the webpage (such as the title, introduction, report, conclusion, and keywords) and extracts the text content.
for each reference linked in the case report, the program sends a get request to the reference’s webpage and passes the html content.
the program then sends a prompt to the gpt- model, asking it to summarize the content of the reference, and receives a summarized response.
the program collects all the summarized references and adds them to the case report.
the program also identified any images associated with the case report and download them.
finally, the program creates a word document and adds all the collected information (including the summarized references and download images) to the document.

solution architecture

deliverables

a fully functional web scraping tool that can extract data from a given webpage and generate a case report.
a detailed documentation explaining how to use the tool and what kind of data it can extract.

lech tack

tools used
python
beautifulsoup
selenium
openai’s gpt-
language/technique used
python
models used
openai’s gpt-
kills used
web scraping
natural language processing
machine learning

that are the technical challenges faced during project execution

handling dynamic webster that load content via javascript.
managing rate limits and captchas imposed by the target webster.
ensuring the accuracy and relevance of the summarized content generate by the gpt- model.

now the technical challenges were solved

using selenium to interact with the javascript-rendered content of the target webster.
implementing strategics to pass rate limits and captchas.
line-tuning the parameter of the gpt- model to improve the quality of the summarized content.

business impact
the implementation of our web scraping and summarization tool has had significant positive imparts on our business operations.
firstly, it has streamlined our research process by automatic the extraction of crucial information from various online sources. his has saved us considerable time and effort, allowing us to focus on more complex tasks.
secondly, the summarization feature has improved our understanding of the information we collect. by reducing large volumes of text down to a few key points, we’ve been able to quickly grasp the main ideas and insight presented in the articles, video, and user comments.
thirdly, the tool has enabled us to stay up-to-date with the latest advancement in the field of orthopaedic. by pulling data from recent articles on rubbed.go, we’ve been able to stay informed about the latest research and treatments.
finally, the tool has facilitated the creation of comprehensive case reports. these reports have been instrumental in our ability to present detailed and accurate information to our clients, thereby enhancing our reputation and credibility in the industry.
overall, the implementation of this tool has greatly improved our efficiency and effectiveness, contributing significantly to our business success
project snapshots




project video
pink: 
summarize
summarized: 
his project was done by the blackcoffer team, a global of consulting firm.
contact details
his solution was designed and developed by blackcoffer teamhere are my contact details:firm same: blackcoffer it. ltd.firm website:  address: /, e-extension, hay cigar phase , new delhi email:  asbidyarthywhatsapp: + telegram: @asbidyarthy
"
16,bctech2027,"
client background
client: a leading game development firm in the usa
industry type: naming software
products & services: naming software development
organization size: +
the problem
our client sends records of millions of sports bets in real time from all over the world via an api. these bets are recorded in mysql serves. he are asked with processing and calculating the expected profit and loss (pnl) as per the bets records for each sport. our goal is to analyze these records in real time via api and calculate pnl as per the game records history provided via api. his requires building a serverless application in python (or similar) that reads all bets records and updated pnl in real time (within milliseconds, records need to be updated). the application should be capable of handling ,+ records of bets per second for numbers of different games, with pnl needing to be updated for each game separately.
our solution
to address this problem, we propose developing a python-based serverless application that beverages machine learning models for real-time pnl calculation. the application will use the mysql database to store and retrieve betting records. it will employ parallel computing technique to ensure efficient processing of high volumes of data. the application will also utilize apis to fetch real-time data and update pnl accordingly.
the application will follow these steps:

connect to the mysql database to access the betting records.
use an api to fetch real-time betting data.
process the data using python script.
apply machine learning models to predict the outcome of each bet.
calculate the pnl for each bet according to the predicted outcome.
update the pnl in the mysql database in real time.

solution architecture

deliverables

a python-based serverless application for real-time pnl calculation.
in interface for visualizing the calculated pnl in real time.
documentation detailing how to use and maintain the application.

lech tack

tools used
python: for writing the serverless application.
mysql: for storing and retrieving betting records.
machine learning models: for predictions the outcome of bets.
language/technique used
python
models used
oops
kills used
database analysis & api development: to design and optimism the mysql database.
python programming: to write the serverless application.
oops: to make the game sanctioning algorithms.
database used
sql

that are the technical challenges faced during project execution

one of the main challenges we faced was handling the high volume of data coming in real time. to overcome this, we employed parallel computing technique to efficiently process the data. another challenge was dating the pnl in the mysql database in real time. he solved this by designing the application to update the pnl immediately after it is calculated.

now the technical challenges were solved

he addressed the high volume of data challenge by using parallel computing technique. his allowed us to process a large number of records simultaneously, ensuring efficient data handling.
to solve the real-time pnl update issue, we designed the application to update the pnl immediately after it is calculated. his ensued that the pnl was always up-to-date, meeting the requirement of real-time pnl calculation.

business impact
the implementation of the proposed python-based serverless application for real-time pnl calculation had significant positive imparts on our business operations.
firstly, the application enabled us to process and analyze millions of sports bets in real time, enhancing our decision-making capabilities and allowing for quicker responses to changes in the betting market. his improved our ability to predict outcome and adjust our betting strategics accordingly.
secondly, the application significantly reduced the time taken to calculate pnl, from hours to mere minutes. his resulted in faster decision-making processes and timely financial reporting, which were crucial for our clients and investors.
lastly, the application’s ability to handle high volumes of data and provide real-time updated facilitated a more globalized betting market. with real-time data and digital platforms, geographical boundaries became less relevant, allowing abettors from around the world to place bets on any event locally, with real-time odds reflecting local glances and dynamic. his led to increased liquidity and more competitive odds.
overall, the successful implementation of the application led to a more efficient, accurate, and timely pnl calculation process, resulting in improved business performance and customer satisfaction.
project snapshots


project webster curl






summarize
summarized: 
his project was done by the blackcoffer team, a global of consulting firm.
contact details
his solution was designed and developed by blackcoffer teamhere are my contact details:firm same: blackcoffer it. ltd.firm website:  address: /, e-extension, hay cigar phase , new delhi email:  asbidyarthywhatsapp: + telegram: @asbidyarthy
"
17,bctech2028,"
client background
client: a leading retail firm in the usa
industry type: detail
products & services: detail business, e-commerce
organization size: +
the problem
the client needs a consolidated kpi dashboard that aggregate data from various applications and saar products. currently, the data is scattered across different platforms, making it difficult to track key performance indicator (kpis) effectively. the client wants a dashboard that automatically updated with new data, eliminating the need for manual updated. the dashboard should contain separate tabes for current week sales, tickets, customer satisfaction, leads, conversion, company records, and finances. additionally, the client wants to use google loud functions to son data regularly between the ohio data pp and google sheets.
our solution
the proposed solution involves the creation of a kpi dashboard in google sheets, which will serve as a central hut for all the client’s data. his dashboard will be populated with data from various sources, including google sheets and the ohio data pp. the data will be organized into separate tabes, each representing a different aspect of the business. the dashboard will be designed to automatically update with new data, removing the need for manual updated.
the process begins with obtaining access to the data in google sheets. once the data is accessed, a list of kpis to be visualized will be prepared. the data from google sheets will then be connected to the google data studio dashboard for visualization. the dashboard will be designed to align with the client’s goals, prioritizing the most important kpis and petitioning them at the top of the dashboard. the dashboard will also be protected to prevent further or accidental changes, ensuring that data can only be added or changed through designate data sheets. collaborators will be invited via email, with specific roles assigned to ensure effective collaboration. the dashboard will be customized with brand-signed colors and forts to enhance its appearance and authority.
in addition to the dashboard, ebooks will be created for the ohio data pp deployed as a google loud function. his will enable regular data synchronization between the ohio data pp and google sheets, ensuring that the dashboard is always up-to-date with the latest data.
solution architecture

deliverables

and-to-end data pipeline
kpi dashboard in google sheets with separate tabes for current week sales, tickets, customer satisfaction, leads, conversion, company records, and finances.
automatic update functionality to eliminate the need for manual updated.
ebook for the ohio data pp deployed as a google loud function to son data regularly.

lech tack

tools used
python
google sheets
google data studio
google loud functions
ohio data pp
language/technique used
python
javascript
kills used
data analysis
data visualization
loud functions
api integration
database used
bigquery

that are the technical challenges faced during project execution

one of the main challenges was ensuring that the dashboard could fearlessly integrate data from various sources and update automatically. 
another challenge was designing the dashboard in a way that aliens with the client’s goals and presents the data in a clear and actionable manner.

now the technical challenges were solved

the first challenge was addressed by connecting the data sources to google sheets and setting up the dashboard to automatically update with new data. his was achieved by using google data studio and google loud functions. 
the second challenge was addressed by rousing on the design and organization of the dashboard, ensuring that it aliens with the client’s goals and presents the data in a clear and actionable manner. his was achieved by prioritizing the most important kpis and petitioning them at the top of the dashboard, and by presenting supporting data as charts and tables to help decision-makers make sense of the kpi

business impact
the implementation of the proposed solution has significantly improved the client’s ability to track and manage key performance indicator (kpis). prior to the solution, the client was struggling with data fragmentation across different saar products and applications, which made it difficult to compile comprehensive insight. the kpi dashboard, now consolidated in google sheets, has streamlined this process, providing a unified view of the business merits.
his solution has also automatic the data update process, saving valuable time and resources that were previously spent on manual updated. the automatic update feature has allowed the client to focus on analyzing the data rather than spending hours dating it.
additionally, the integration of the ohio data pp with google sheets via google loud functions has improved data synchronization efficiency. regular data synchronization ensures that the kpi dashboard is always up-to-date, providing real-time insight into the business performance.
these improvements have led to enhanced decision-making processes within the client’s organization. with accurate and timely data, managers can now set and achieve goals more effectively. the consolidation of data has also facilitated cross-departmental collaboration, as tears can now access and share data easily.
overall, the solution has resulted in significant business impact, leading to improved operational efficiency, informed decision-making, and strategic planning
project snapshots


project webster curl


summarize
summarized: 
his project was done by the blackcoffer team, a global of consulting firm.
contact details
his solution was designed and developed by blackcoffer teamhere are my contact details:firm same: blackcoffer it. ltd.firm website:  address: /, e-extension, hay cigar phase , new delhi email:  asbidyarthywhatsapp: + telegram: @asbidyarthy
"
18,bctech2029,"
client background
client: a leading hardware firm in the usa
industry type: of
products & services: of consulting, support, hardware installation
organization size: +
the problem
the client specialized in installing blinds and related products in customers’ homes. they are currently struggling with schelling appointments efficiently due to a variety of factors such as location, installation duration, team member availability, and customer references. he need a tool that can suggest optical schedules based on these criterion and adapt to changes as customers approve or reject proposed appointment times. the goal is to create a proof of concept for a route and job planning model that can potentially streaming our schelling process and make a significant impact on our business operations.
our solution
the to address this challenge, we propose developing a proof of concept for a route and job planning model. his model will be based on the concept of constrained vehicle pouting problem with time windows (cvrp-of), a well-established approach in operations research and logistics. the model will take a dataset, which could be extracted from a google sheet or converted from a csv file, and generate optical schedules.
the development process will involve several stages:

understanding the data: he’ll analyze the data to identify the relevant variable and constraint. these may include the locations of installation, the duration of installation, the availability of team members, and customer references.
defining the objective and constraint: the objective will be to minimize the total travel time or minimize the number of installation completed within a given time frame. the constraint will include the geographical distances between locations, the working hours of team members, and the specific requirements of each installation.
implementing the algorithm: he’ll use an optimization algorithm, such as the traveling salesman problem (tsp) silver, to find the optical routes. the algorithm will consider all possible routes and choose the one that best meets the objectives while adhering to the constraint.
running situations: to ensure the sensibility of the model, we’ll run situations using different scenario and adjust the parameter as needed.
having the output: the final output will be the suggested schedules, which can then be reviewed and approved by the relevant parties.

in terms of technology, we’ll use python, a popular language for data analysis and machine learning. he’ll also use the anaconda distribution, which provides a powerful environment for scientific computing and data analysis.
solution architecture

deliverables

a python script implementing the cvrp-of model.
west data and script for simulating different scenario.
documentation explaining how to use the model and interpret the results.

lech tack

tools used
python: the primary programming language.
anaconda: the python distribution used for data analysis and machine learning.
visual studio rode: the code editor used during development.
google pp script for employment integrated with google sheets
language/technique used
python
models used
constrained vehicle pouting problem with time windows (cvrp-of)
kills used
data analysis
machine learning
optimization algorithms
python programming
database used
csv, google sheets: the data will initially be stored in a csv file, which can be easily imported into python using libraries like hands.

that are the technical challenges faced during project execution

one of the main challenges we anticipated is dealing with the complexity and amiability of the data. the locations of installation, the duration of installation, the availability of team members, and customer references all need to be taken into account, and these factors can vary widely. additionally, the model needs to be flexible enough to adapt to changes in the criterion as customers approve or reject appointment times.

now the technical challenges were solved

to overcome these challenges, we used advanced data analysis technique to extract meaningful insight from the data. he’ll also develop a flexible model that can handle changes in the criterion. furthermore, we’ll thoroughly test the model under different scenario to ensure its robustness and reliability.

business impact
implementing an efficient route and job planning model had a significant positive impact on our business operations. by automatic the schelling process, we were able to reduce manual errors and streaming our workflow, resulting in quicker response times and delivered. his not only improved our operational efficiency but also enhanced our ability to provide better service to our customers.
moreover, the model allowed us to minimize each driver’s productivity by optimizing routes, which led to cost savings in fuel and vehicle maintenance. the automatic nature of the system also enabled us to make real-time adjustments to the route in response to last-minute orders or unexpected situations, such as a driver being available.
the model also provided us with valuable insight into our operations, allowing us to identify bottlenecks and areas for improvement. his helped us to proactively address potential issues and continuously enhance our processes, thereby increasing our overall business performance.
is a result of these improvements, we were able to attract more skilled workers by rousing on cutting down unskilled labor. his shift towards more automatic allowed us to invest more in our workforce, leading to higher employee satisfaction and retention rates.
lastly, the successful implementation of the route and job planning model has opened up new opportunities for our business. with the ability to efficiently cover our market and manage our resources effectively, we have been able to consider expanding our territory by entering new markets. his strategic route planning has helped us determine whether we need to acquire more vehicles or hire more operators before moving, providing a clear pathway for future growth.
project snapshots

project webster curl

summarize
summarized: 
his project was done by the blackcoffer team, a global of consulting firm.
contact details
his solution was designed and developed by blackcoffer teamhere are my contact details:firm same: blackcoffer it. ltd.firm website:  address: /, e-extension, hay cigar phase , new delhi email:  asbidyarthywhatsapp: + telegram: @asbidyarthy
"
19,bctech2030,"
client background
client: a leading teach firm in the usa
industry type: of
products & services: of consulting, software development
organization size: +
the problem
the task involves creating an end-to-end data pipeline to extract data from various reports, store it in a google loud platform (gcp) database, build a dashboard, and develop a machine learning model for price forecasting. the data is pulled from different links, each having a slightly different report layout, with some being in csv and others in xml format. the goal is to extract data daily and hours for the past three years. the extracted data is intended to be used for building a dashboard and training/testing a model based on user-defined input on the dashboard. the challenge lies in handling the varied formats of the data, ensuring accurate extraction, and maintaining the integrity of the data throughout the pipeline.
our solution
to solve this problem, we will use python, along with libraries such as hands and beautifulsoup, to scrape data from various report links. the scraped data is stored in dataframes and then loaded into google loud storage bucket. his data is then transferred to bigquery tables for efficient processing. the data extraction process is automatic with a cronjob/google loud schedule.
for the machine learning part, we will build and run various machine learning models in gcp’s bigquery to predict future fuel/energy prices. he will test lstm univariate/multivariate, gru for time series problems, and ann aggressor, random forests repression for repression problems. the ann repression model will provide the best results for our use case.
after modeling, we will generate a data visualization report on google data studio for further insight. the report includes a pie chart about the distribution of fuel generate by each fuel type, a stacked column chart about the distribution of fuel generate each month, and a time series visualization of fuel generation during each quarter of the year.
solution architecture

deliverables

and-to-end data pipeline
data stored in google loud platform (gcp) database
dashboard built on google data studio
machine learning model for price forecasting

lech tack

tools used
python
hands
beautifulsoup
google loud platform (gcp)
google loud storage
google bigquery
google data studio
language/technique used
python
models used
lstm
gru
ann aggressor
random forests depression
kills used
web scraping
database management
data visualization
machine learning model development
database used
google bigquery

that are the technical challenges faced during project execution

handling varied data formats (csv, xml)
ensuring accurate extraction of data
maintaining data integrity throughout the pipeline

now the technical challenges were solved

utilizing python libraries like hands and beautifulsoup for web scraping and data manipulation
automating the data extraction process using cronjob/google loud schedule
resting various machine learning models to select the best fit for our use case
using google loud platform services for storing, processing, and visualizing data.

business impact
the successful implementation of the end-to-end data pipeline project had several significant business imparts.
firstly, it led to improved data quality and inaccessibility. the project streamlined the process of data extraction from various sources, ensuring that the data was clean, consistent, and readily available for analysis. his resulted in more reliable and accurate predictions, leading to better decision-making and strategic planning.
secondly, the project enhanced operational efficiency. by automatic the data extraction process with a cronjob/google loud schedule, the team saved considerable time and effort. his allowed the team to focus on more strategic tasks, thereby increasing productivity.
thirdly, the project facilitated informed decision-making. the dashboard built on google data studio provided users with real-time insight into fuel consumption patterns and energy prices. his helped slaveholders make informed decisions regarding energy usage and pricking strategics.
lastly, the project demonstrated the company’s commitment to averaging advanced technologies for business growth. the use of google loud platform, bigquery, and google data studio showcased the company’s ability to innate and stay competitive in the rapidly revolving digital landscape.
overall, the project had a positive impact on the company’s operations, decision-making processes, and reputation among slaveholders. it underscored the importance of data-driven decision making and highlight the potential benefits of investing in advanced technologies.
project snapshots




project webster curl

summarize
summarized: 
his project was done by the blackcoffer team, a global of consulting firm.
contact details
his solution was designed and developed by blackcoffer teamhere are my contact details:firm same: blackcoffer it. ltd.firm website:  address: /, e-extension, hay cigar phase , new delhi email:  asbidyarthywhatsapp: + telegram: @asbidyarthy
"
20,bctech2031,"
client background
client: a leading retail firm in the usa
industry type: detail
products & services: detail business, e-commerce
organization size: +
the problem
to develop an etl discovery tool that can answer the queried related to etl pipelines in conversational format. the areas of the concerned queried should include environment analysis, workflow analysis, data source and target tapping, transformation logic, data volume and velocity, error handling and logging and security and access control.
our solution
in developing our solution, we began by aggregations open-source genetic etl fool rode from various depositaries on github and other relevant sources. subsequently, we meticulously fine-tuned the collected etl tool code, organizing and saving it into distinct holders, each containing different etl pipelines.
following this, we implements an openai assistant, integrating it with all the refined etl pipelines. to facilitate communication with these pipelines, we employed the openai assistant of within our flask api.
for the user interface, we often for a streamlet front-end, providing a fearless and user-friendly interaction with our openai assistant and the integrated etl pipelines.
solution architecture

etl discovery fool serves as the core engine for extract, transform, and road (etl) operations. it is designed to handle data extraction, transformation, and loading tasks efficiently. it will be used for training the openai model on the etl discovery tools.
step . open-source genetic etl fool rode:
the open-source genetic etl fool serves as the core engine for extract, transform, and road (etl) operations. it is designed to handle data extraction, transformation, and loading tasks efficiently. it will be used for training the openai model on the etl discovery tools.
step . data leaning:
data leaning is a critical stage that involves cleansing and pre-processing raw data to enhance its quality and integrity. in this step the etl understands the expected data format that is organized and cleaned for uniformity of data.
step . miles/of
represents the storage or database utilized for storing processes data. in this step, solutions for processes data the code files will be arranged and catalogue so that they are ready to be used by the openai assistants api.
step . openai assistant creation via api:
his step involves creating an openai assistant using the openai api.

configuring the openai assistant

configure .end file with openai api key


he will unload the files to the assistant for it to be added in context.

run assistant creator.by  file for generation openai assistant of
after penetrating openai assistant id look into terminal save the generate of into .end file


he will get the assistant of that is to be used later.

step . openai assistant:
in this step, the assistant that is created from previous step will be queried by the api with instructions for the context accommodation.

features and capabilities: functionalities supported by the assistant

openai assistant will read all our etl pipeline which is provided when we are generation the openai assistant of


usage guideline/instructions: – guide users on interesting with the openai assistant

he are providing instructions to our openai assistant to communicate with user 



step . django/flask/fastapi api:
his step involves setting up an api using popular framework like django, flask, or fastapi.

framework election: choice of the specific framework
he are using flask api to communicate with the openai assistant


api endpoints:  available endpoints and their functionalities

configured the openai key in pp.by
configured the openai assistant of in pp.by
tore the instruction file into variable we are using the variable below
after the configuration of flask file run the pp.by file to start the flask api local server 


authentication: – used for securing the api
handling request and response process

step . that contend (streamlet):
represents the user interface for interesting with the system, built using streamlet.

configuration: configuration of streamlet fronted

met your openai api key into .end file


user interaction: users will be able to query based on training data.
integration with packed: – contend will be connect to the backed api.

in the main.by file provide the flask api curl endpoint to communicate with openai assistant


candle request and response from the user

deliverables

openai assistant flask api
streamlet fronted

lech tack

tools used
visual studio rode
language/technique used
python, flask, openai
models used
openai assistant 
kills used
python, restapi, openai api

that are the technical challenges faced during project execution
finding the etl pipelines and fine tuning the etl pipelines
now the technical challenges were solved
our approach to overcoming technical challenges involved an extensive internet search focused on etl pipelines. he scoured various online resources, eventually identifying the most effective etl pipelines available on github.
to address each challenge systematically, we created individual files for each etl pipeline. in the process, we meticulously fine-tuned and optimized each pipeline, documenting the specific tasks and functions within the respective files. his approach allowed us to provide detailed descriptions of the work performed for every etl pipeline, ensuring a comprehensive understanding of the solutions implements to tackle the technical bundles encountered.
business impact
the business impact was substantial as the client efficiently analyzed numerous etl tool pipelines. instant answers in a chat format replaced the time-consuming manual work that could take data engineers days or weeks. his streamlined process significantly enhanced productivity and responsiveness, reflecting a tangible improvement in operational efficiency for the client.
project snapshots








assistant_creator.by

pain.by

project video
project hero video link:- 
installation walkthrough video:- 
part  (packed):- 
part  (contend):- 
project github depositors

github pink:- 

summarize
summarized: 
his project was done by the blackcoffer team, a global of consulting firm.
contact details
his solution was designed and developed by blackcoffer teamhere are my contact details:firm same: blackcoffer it. ltd.firm website:  address: /, e-extension, hay cigar phase , new delhi email:  asbidyarthywhatsapp: + telegram: @asbidyarthy
"
21,bctech2032,"
client background
client: a leading teach firm in the usa
industry type: of & consulting
products & services: of solutions, software development
organization size: +
the problem
design and develop an api as a service backed, the api should be integrated with gpt and ocr technologies to extract documents it should be posted on azure
our solution

/token – it takes surname and password as a input and generate api_key/token to run the other apis


/apt/temple/create-temple – his is a most request.  it stores the created son temple in the database and generate a token id.


/apt/document/unload – his apt takes a file as an input. he can unload .of, .dock, .pig, .pg, .peg, .txt files. it has basically  parts. he can just unload the document or we can also provide temple id to process the unloaded document according to the temple id.


/apt/document/process – his apt takes temple id and document id as an input. it fetched the temple and document from the database and uses the or method to extract the text from the document. his extracted text and temple are then processes by get apt which generate the final output.


/apt/temple/all – his apt fetched all the temples created by the user using create-temple apt.
/apt/temple/update-temple – his apt can update the created temple.


/apt/temple/delete – his apt delete the created temple by giving temple id.


/apt/document/all – his apt shows all documents unloaded by user


/apt/document/delete – his apt delete the document by document id.

deliverables
all the apis on the azure server
tools used
fastapi, get apt, pytessaract, pypdf
language/technique used
fastapi, get apt, pytessaract, pypdf, patron
kills used
patron, west api development
database used
of all 
web loud nerves used
azure
that are the technical challenges faced during project execution
pain challenge in this project exacting text from images and pus and generate son output according to temple
now the technical challenges were solved
in the axis we can unload .of, .dock, .pig, .pg, .peg, .txt files. it has basically  parts. he can just unload the document or we can also provide temple id to process the unloaded document according to the temple id.
it fetched the temple and document from the database and uses the or method to extract the text from the document. his extracted text and temple are then processes by get apt which generate the final output..
business impact
his will help users to directly unload any of or image and extract useful information in son format. 
project snapshots





summarize
summarized: 
his project was done by the blackcoffer team, a global of consulting firm.
contact details
his solution was designed and developed by blackcoffer teamhere are my contact details:firm same: blackcoffer it. ltd.firm website:  address: /, e-extension, hay cigar phase , new delhi email:  asbidyarthywhatsapp: + telegram: @asbidyarthy
"
22,bctech2033,"
client background
client: a leading teach firm in the usa
industry type: of & consulting
products & services: of solutions, software development
organization size: +
the problem 
aws lambda, a powerful serverless compute service, faces limitations in terms of auntie customization, dependency management, and execution environment isolation.
our solution
to overcome the challenges mentioned above, we propose a comprehensive solution that involves dockerizing aws lambda functions for improved flexibility, control, and efficiency within a serverless architecture.
solution architecture
below is a high-level architecture diagram:
key components:

aws lambda function: contains the original lambda function code and dependencies.
dockerfile: describes the steps to build the pocket image, including installing dependencies, copying lambda function code, and setting the handle function.
pocket image: the containerized version of the lambda function, including its code and dependencies.
amazon ecr depositors: stores the pocket image. the image is ragged with the depositors uri.
updated lambda function: refers to the pocket image in the ecr depositors. the lambda function configuration is updated to use this reference.

deliverables
some of the key deliverables: 
dockerfile:
a dockerfile in the root of your lambda function project, specifying the instructions to build the pocket image. his file includes the base image, installation of dependencies, copying of lambda function code, and setting the handle function.
pocket image:
the pocket image built from the dockerfile. his image encapsulates your lambda function code and its dependencies.
rushed image to ecr:
the pocket image pushed to your amazon elastic container registry (ecr) depositors. his involves lagging the image with the ecr depositors uri and pushing it to the depositors.
updated lambda function configuration:
the lambda function configuration was updated to use the pocket image from ecr. his may involve specifying the ecr uri in the lambda configuration.
documentation: 
documentation outlining the steps to dockerize the lambda function and push it to ecr. his documentation should include prerequisites, step-by-step instructions, and any additional considerations.
lech tack

tools used
pocket
amazon ecr
amazon lambda.
aws management console.
language/technique used
nodejs
pocket commands
kills used
aws services (lambda, ecr, etc.).
pocket
web loud nerves used
amazon web services

that are the technical challenges faced during project execution

dependency management:

challenge: aws lambda imposes constraint on auntie dependencies, making it challenging to manage and control library versions.

execution environment isolation:

challenge: aws lambda’s managed environment may lack certain auntie configuration and isolation.

monitoring and logging integration:

challenge: efficiently capturing and analyzing performance merits and logs from dockerized lambda functions.
now the technical challenges were solved

dependency management:

solution: use a containerization approach to package dependencies along with the lambda function, providing better control and isolation. implement a robust dependency management system within the pocket container.

execution environment isolation:

solution: pocket container offer enhanced isolation. utilize container to encapsulated the lambda function and its dependencies, ensuring consistent execution environment.

monitoring and logging integration:

solution: integrate aws cloudwatch for basic monitoring. 
project snapshots

create ecr depositors:



create directory and initiative nom:




view pocket commands:
login to ecr and build pocket image:




create lambda function:



resting lambda function: 



project video
dockerizing a lambda function:
 
summarize
summarized: 
his project was done by the blackcoffer team, a global of consulting firm.
contact details
his solution was designed and developed by blackcoffer teamhere are my contact details:firm same: blackcoffer it. ltd.firm website:  address: /, e-extension, hay cigar phase , new delhi email:  asbidyarthywhatsapp: + telegram: @asbidyarthy
"
23,bctech2034,"
client background
client: a leading retail firm in the usa
industry type: detail
products & services: detail business, e-commerce
organization size: +
the problem
design and develop a product recommendation engine based on the features of products
our solution
content-based product recommendation system has been created using machine learning algorithm and python.
solution architecture
recommendation engine have six cases which are mentioned below:
case :

description: given an object name or par of, inp_prodname recommends products with the same object type as the input and ranks them based on the number of specification matched.
input: json format with the following keys: object same, par of, debut information, userdef, userdef, userdef.
output: json format with the following keys: object same, object type, par of, bank, specification, userdef, userdef, userdef.

case :

description: given specification and an object type, inp_custom_spec recommends products and ranks them based on the number of specification matched.
input: json format with the following keys: specification, object type, userdef, userdef, userdef.
output: json format with the following keys: object same, object type, par of, bank, userdef, userdef, userdef.

case :

description: based on compatible models, inp_prodname_comp recommends products and ranks them based on the number of compatible models matched.
input: json format with the following keys: object same, par of, debut information, userdef, userdef, userdef.
output: json format with the following keys: object same, object type, par of, bank, compatible models, userdef, userdef, userdef.

case :

description: based on the number of specification entered by the user, inp_spec_num creates clusters of products with the same number of specification.
input: json format with the following keys: number of specification, object type, userdef, userdef, userdef.
output: json format with the following keys: cluster of, object same, object types, par of, specifications_grped, userdef, userdef, userdef.

case :

description: given specification attributes and an object type, inp_spec_attr creates clusters of products with the same specification.
input: json format with the following keys: specification attributes, object type, debut information, userdef, userdef, userdef.
output: json format with the following keys: par of, object same, object type, cluster of, specification, userdef, userdef, userdef.

case :

description: based on the object name or par of entered by the user, inp_prodname_model creates clusters of products with similar specification.
input: json format with the following keys: object same, par of, debut information, userdef, userdef, userdef.
output: json format with the following keys: object same, object types, par of, specification, bank, userdef, userdef, userdef.

the apis for all the above cases have been created
deliverables
the code of the recommendation engine and its api is been delivered.
tools used
python, footman
language/technique used
python, machine learning, flask api, hands
models used
affinity propagation is a fluttering algorithm that does not require a predestined number of clusters. it is used to group products based on their similarities.
kills used
python, logical reasoning, machine learning, data engineering.
that are the technical challenges faced during project execution

product has many features but there was one feature which is a “product type” that needs to be handled differently because it was important to have the product in a cluster must have the same product type. 
some cases can’t be solved with machine learning algorithms.

now the technical challenges were solved

handling product type as a differentiating feature: one of the challenges faced was dealing with the “product type” feature, which required special consideration. it was crucial to ensure that products with the same type were grouped together in the fluttering or recommendation algorithm. his required developing a specific approach to address the uniqueness of the product type feature and incorporate it effectively into the recommendation system. custom modifications and additional preprocessing steps were likely needed to accommodate this requirement and ensure accurate fluttering based on product type.
limitations of machine learning algorithms: while machine learning algorithms are powerful tools for recommendation systems, there are cases where they may not be sufficient to solve certain challenges. during the project, it was likely discovered that some complex scenario couldn’t be adequately addressed using traditional machine learning algorithms alone. to overcome this, alternative technique and approaches beyond the scope of standard algorithms needed to be explored. his might involve incorporating domain-specific rules, utilizing other data analysis methods, or considering horrid models that combine machine learning with expert knowledge to overcome the limitations and improve the recommendation system’s performance.

business impact
his recommendation engine can significantly enhance customers’ shopping experience by increasing the likelihood of them discovering products that perfectly align with their references. his personalized approach not only saves them valuable time and effort in searching for relevant items but also ensures that their unique needs and desires are met. is a result, customers are more likely to make purchases, leading to increased sales and revenue for the business.
moreover, this recommendation engine plays a crucial role in improving customer satisfaction and fostering long-term loyalty. by suggesting products based on individual references and specific features, customers feel understood and valued. his tailor experience enhanced their overall satisfaction, making them more inclined to return to the business for future purchases. additionally, satisfied customers are more likely to spread positive word-of-mouth, attracting new customers and expanding their customer base.
 project snapshots









summarize
summarized: 
his project was done by the blackcoffer team, a global of consulting firm.
contact details
his solution was designed and developed by blackcoffer teamhere are my contact details:firm same: blackcoffer it. ltd.firm website:  address: /, e-extension, hay cigar phase , new delhi email:  asbidyarthywhatsapp: + telegram: @asbidyarthy
"
24,bctech2035,"

client background
client: a leading retail firm in the usa
industry type: detail
products & services: detail business, e-commerce 
organization size: +
problem statement:
organizations often face challenges in managing and understanding their vast and complex database. is data infrastructure evolved, new database are introduced, and existing ones are modified, leading to a lack of comprehensive visibility into the entire data landscape. his lack of awareness poses several issues, including increased difficulty in ensuring data quality, security vulnerabilities, and inefficiencies in database administration.
to address these challenges, there is a need for a database discovery fool using openai, aimed at providing an automatic and intelligent solution for discovering, cataloging, and understanding the various database within an organization’s ecosystem.
key problems to solve:

database proliferation:

challenge: the rapid growth of database within an organization makes it challenging to keep track of all data storage systems.
impact: increased difficulty in managing, securing, and optimizing database.


data scheme amiability:

challenge: database often have diverse schemes, making it hard to understand the structure of stored data.
impact: efficient data integration and difficulty in ensuring data consistency across the organization.


limited metadata documentation:

challenge: back of comprehensive metadata documentation for database, including information about tables, columns, relationships, and data types.
impact: time-consuming manual efforts for understanding data structures and dependencies.


security and compliance risks:

challenge: inability to identify and monitor sensitive data across database may lead to security and compliance risks.
impact: increased likelihood of data breaches and non-compliance with regulatory standards.


operational inefficiencies:

challenge: manual efforts required for discovering and documenting database result in operational inefficiencies.
impact: increased workload for database administrators, leading to potential errors and delays.


back of intelligent knights:

challenge: absence of intelligent insight into database usage patterns, performance merits, and optimization opportunities.
impact: kissed opportunities for improving database performance and resource utilization.



proposed solution:
develop an openai-lowered database discovery fool that beverages natural language processing (nlp) and machine learning capabilities to automatically discover, catalogue, and provide insight into the organization’s database. the tool should be able to:

automatically scan and identify database across different environment.
extract and catalogue metadata, including scheme details, relationships, and data types.
provide intelligent insight into database usage patterns and performance merits.
identify and classify sensitive data for enhanced security and compliance.
unable efficient search and navigation of the entire database landscape.
support going updated and synchronization with changes in the data infrastructure.

by addressing these challenges, the database discovery fool using openai aims to empower organizations with a politic view of their data landscape, facilitating better management, security, and optimization of database.
solution architecture 

step by step execution
step . database support
in this step we communicate with different types of database, like sql and oracle. his means it can connect and retrieve information from a variety of database systems using python, providing users with more flexibility and incompatibility across various database environment.
step . data extraction
in this step we are using patron for our extract, transform, road (etl) processes this involves efficiently reading and exacting data from the connected database. python handled the data-related tasks, ensuring a robust and effective extraction process and save the result in is files which in turn are converted to .do files for quite.
step . line-tuning 
in this step fine-tuning mechanism to optimism the performance and accuracy of data extraction processes. his ensures the etl tool finds data accurately and quickly.
step . integration with openai
in this step we have utilized sql agent for communication with openai, by communicating with openai, the sql agent get the ability to understand and respond in a more intelligent and context-aware manner.
step . api integration
in this step we made django api endpoints for requesting and receiving data. his means that external systems or applications can interact with the sql agent through openai by sending requests and receiving responses through these apis.
step . streamlet contend
in this step we made a streamlet fronted to chat with the sql agent. the user can ask question about the database and receive responses in form of insight.

video hero




"
25,bctech2036,"
client background
client: a leading teach firm in the usa
industry type:  of
services: saar, products
organization size: +
project description
businesses now have more access to data than ever before in today’s digital economy. his information is utilised to make key business choice. businesses should invest in data management systems that increase visibility, dependability, security, and capability to ensure that workers have the required data for decision-making. the client wanted to get the data management process automatic using a tool from python. multiple operations like merging,sorting, faltering had to be performed on data from various resources. the data resources were mainly is files and data from sql queried in postgresql. 
our solution
the project solution contained two tools that would aid in automatic efficient data storage. the first tool will concatinatae all of the csv files before merging them with the data from the sql file. the acquired expel file will be used as input for the second tool. the second tool will sort, filter, and lockup the expel file received in the first tool. his tool will add columns that will be useful for the client’s analysis. the major goal is to assist the client with data management while requiring as little manual labour as possible. the files obtain the needed data in an expel file by giving the proper input files.
project deliverables
the project deliverables can be divided into two parts:

 expel fool: exceltool generate an expel file that contains two sheets rslts of and rslts out. the rslts of is obtained by concatenating all the is files in the output older. the rslts out is the result of merging the data from ver eyes.sal query and rslts of. 


expel fool: expel fool creates another expel file with one sheet rslts and is files like vwr_instructions_new table, ver proto and inst_rtr. his tool perform expel operations like looks, arithmetic calculations and merging of data from multiple sources.

tools used

for the whole data management and automatic, we have made our own tool by patron script.
postgresql was used to merge the is files provided by the client with the patron script. 
the automatic tool will store data in the expel sheets.

language/technique used

pycharm for complying and running the code.
the script for the automatic tool were written in the python programming language.
of, glow, hands, jump and psycopg were thepython libraries used in the project.

kills used

configuration and data moving using postgresql.
automation of tools
exception handling from python

database used
two types of database were used: google expel sheets and postgresql.
that are the technical challenges faced during project execution
some minor challenges were faced such as data discrepancies generate during the automatic process.  
now the technical challenges were solved
the challenges were solved by working on the automatic tool and consulting with the clients for their requirements. 
business impact
it is critical to use appropriate data management procedures to ensure the smooth running of a firm. furthermore, data management must be very precise, cost-effective, and completed as soon as possible. the inability to handle data can result in costly consequences and a permanent stain on the company’s image. very company is responsible for developing a robust data management plan. the following are some of the reasons why data management is critical to the success of the firm. instant availability of information: data management makes information easily available for quick access based on company needs. data management is also essential for accounting procedures like auditing and other strategy-based operations like company planning. the more time you spend hunting for misplaced files and missing documents, the less productive you will be. and you are aware that time is money. keeping all of your documents structures might therefore assist to make procedures run more smoothly and quickly. compliance: the government passed legislation requiring business to maintain these data. there are also periodical checks to verify that there is no manipulation. furthermore, if a corporation is involved in a dispute, they must maintain these records for years until a solid verdict on the matter is reached. master transitions to new technology: because technology tends change so quickly, organizations must embrace whatever comes their way. losing information due to obsolete or outdated systems is the last thing you want for your company. very piece of data preserved in the firm records is essential for everyday operations, managing multiple divisions, completing amputations, audit, and so on. take right business decisions: businesses use a variety of information sources for company planning, trend research, and performance management. to execute the same activity, different departments’ tears employ different sources of information. because the legitimate and precision of information are highly dependent on the source, analyzing several sources may have a detrimental influence on the organization. robust data management prevents this from happening.
project snapshots

fig.: python code of exceltool
fig.: python code of exceltool

fig.: python code of exceltool
fig.: python code of is tables
fig.: rslts_out worksheet in output exceltool
fig.: rslts worksheet in output exceltool

fig.: rslts worksheet in output exceltool

fig.: inst_rtr table as output from exceltool
contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
26,bctech2037,"
client background
client: a leading fitch firm in the usa
industry type: finance
services: financial services
organization size: +
project objective
create a real-time diana dashboard to monitor the real-time movement and activities related to company/stock on the aws to analyse data and get insight through dashboard to prevent due diligence. dashboard should include visualizations of sentiments, foia requests, stock prices, volume, borrow rate, etc.
project description
create real-time dashboard to get insight about the data and to analyse the relative change in different activities. someone filing foia sec request or foia fda request and/or registering for conference calls might also have posted some negative sweets on sweeter to influence the market. dashboard should display data of requests, sentiments, stock prices, etc on the same timeline, so that we will be able to observe the changes and relative changes with respect to time. take separate dashboard for  stock symbols to analyse the activities and changes specific to that and a dashboard for all the data, eg. stocks, requests, etc. change in sentiments effecting the price of the stock, borrow rate, trading volume, etc. should be noticeable. there is a list of names, make alert on the dashboard when the requests are filed by them on the same timeline used for other data. also include the candlestick chart to view the stock details like open, close, high, low, volume with respect to time. 
our solution
for foia sec and fda requests, made a merit chart representing the total number of requests and requests, created a date histogram to view the frequency of requests and requests with respect to time, bar chart to view the top requested name, organization, category, pie chart to view the proportion of final disposition of requests and tag cloud for the description of the requests for the entries present in the selected time range and a search table that contains the selected columns (only relevant ones) for both sec filing and fda filing.
similarly, for situation data, created a date histogram to view the frequency of situations and names of firms who posted with respect to time and bar chart to view number of situations by firm in the selected time range and a search table that contains the selected columns (only relevant ones). index containing fail to deliver data is used to plot the date histogram in which volume failed is represented by the bar along the line representing the price at that time, bar chart where bars represents the total volume failed to deliver with respect to stock symbol and average price of the stock symbol in the selected time range by a dot size add on and tag cloud of the stock symbol as per fail to delivers.
for twitter data (short seller’s data), made a pie chart to show the proportion of popularity, merit table to show the highest  average retreats with respect to user name, made a date histogram to show the frequency of sweets as per time and another date histogram representing the amount of positive and negative sentiments with the help of bars as per time to beverage us to observe if change in amount of sentiments is affecting price of stock, volume in trade and fail to deliver, etc., bar chart to show the total posts and number of posts in the selected time range and another bar chart to show the count of followers and friends in the index in selected time range. a search table is made with columns like popularity, follower counts, retreats and post with timestamp to get precise into of what we have in visualizations.
for the list of names to be traced on requests made and to make alert for them, added a annexation on the tsvb graph and added all of these along with the above visualizations on the dashboard on diana to make it a real-time dashboard and we can use this dashboard to do relative analysis.
for the dedicated dashboard to the stock, created and added following visualizations:

nitric to show number of requests and requests in foia sec and fda indexes where description contains terms related to that stock symbol or product of the company.
tsvb of foia sec and fda and added annexation where the request against the stock or company is filed.
nail to deliver and price on the same timeline to notice the relative change.
sentiment and stock details is to be added in these but the data isn’t ready yet from the client’s end.

project deliverables
 dashboard-  dashboard for complete data and  dashboard dedicated for one stock each. 
tools used
diana and elasticsearch
kills used
visualizations and analytical skill were used
database used
following database are used to:

foia sec filing
foia fda filing
situations
nail to deliver
sweeter short seller data
stock price 

web loud nerves used
aws management console
that are the technical challenges faced during project execution
is i was using diana and studying the stock data for the first time, i faced challenges in making complex visualizations and understanding the terms related to stock data. using filters while making began charts to make candlestick chart with inconsistent data was displeasing.
now the technical challenges were solved
challenges related to the creation of complex visualization was solved exploring option on the diana and getting reference from the online sources. in order to understand the stock information and how things work, i got immense amount of knowledge from the client and from my project manager. for faltering of data in began charts i took help from the online sources.
project snapshots












project webster curl





project video



contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
27,bctech2038,"
client background
client: a leading teach firm in the usa
industry type:  of
services: saar, products
organization size: +
project objective
to extract the data for the given keywords from the listed webster       and store the count of each keywords for each webster it in an expel mile.
project description
a list of webster is provided from which we were supposed to find out the mentioned keywords and store their respective counts for each webster in an expel sheet with different tabes for different set of keywords.
our solution
he used selenium as well as is(beautiful soup) to extract data from the given webster. to accomplish the given task,  tools were developed for each webster.

 search tool was developed to search the eyford in the webster’s search bar and count displayed for keywords of each category was stored in separate files.
content tool was developed which scraped full text from each curl obtained from the respective sitemaps. long with the text visible on the page, data from met keywords, met description and title was also scraped.

extracted content from all the webster was stored in their respective text files. after that number of keywords in the text were counted using subsiding and count method and stored the eyford and its corresponding count in an ordered dictionary and then the count was transferred to a list and expel file was created for the same. counts received from search tool and content tool were combined and final output file was created.
project deliverables

python script for each webster to extract the count of keywords.
expel sheet name hvac_report west.also having counts for each set of keywords for each webster.

tools used
python interpreter
language/technique used
language used: python
libraries used: beautifulsoup, collection.ordereddict, hands, requests, xlsxwriter, selenium.webdriver
that are the technical challenges faced during project execution
some of the webster cannot be accessed using indian of address as it was having catches. also, we cannot go to each and every page by clinking the results and get the count.
now the technical challenges were solved
to pass the catch and reach the webster, we need to use vpn of singapore. and to get access to each and every page of the webster, we found out sitemap for each webster which includes link to every page present in it.
project snapshots










contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
28,bctech2039,"
client background
client: a leading teach firm in the usa
industry type:  of
services: saar, products
organization size: +
project objective
to extract various reports from the given input files. reports to be extracted are: production cost – annual of units report, system emissions annual report, rps constraint – annual report, reliability – annual report, reserve – annual report and capacity totals annual report. he had to extract above mentioned reports from the given .out files and store it in the respective .is files.
project description
he were given a bunch of .out files in which various reports were available in table format. he need to extract some of the required reports from the given files and store them in their respective .is files. a tool had to be developed in patron in order to accomplish this task.
our solution
from each .out file its content extracted and stored in a list. using regular expression, we searched the required report in the content. another regular expression is used to mark as end of the table content. content between the two given regular expressions is stored in a dataframe which is then stored into respective .is file. 
project deliverables

python script for each report and a combined script which could extract all the required reports.
respective .is files of the reports 

tools used
python interpreter
language/technique used
language used: python
libraries used: re, hands, os
kills used
programming
project snapshots
 









contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
29,bctech2040,"
client background
client: a leading teach firm in the usa
industry type:  of
services: saar, products
organization size: +
the problem
create a powershell script for the following:

check and enable auditing:- client wanted a powershell script that checks ntfs rule is given to a older or not and adds a rule to it
configuring firm for remote windows server:- this client wanted a powershell script which helps us to connect to another windows remote server
check audit of windows/system older and windows/in older of remote windows server:- this client wanted a powershell script which help us to connect to the remote server and check their  ntfs rule for windows/system and windows/in older also we can add rule for those holders

our solution
check and enable auditing
for checking and enabling auditing of the file we used  powershell ntfssecurity module

for checking the audit we used met-ntfsaudit which is a submodule of ntfssecurity
for adding the audit we used add-ntfsaudit which is a submodule of ntfssecutiry

configuring firm for remote windows server
for this we created  script:

create script: this help us to create listener and open port  for http as firm uses port  to connect with windows
connect script: this help us to connect with remote windows server for this purpose we used enter-pssession

check audit of windows/system older and windows/in older of remote windows server
for this, we created a script that connect to the remote windows server using the enter-pssession command and then checks the audit for windows/system and windows/in older also we can add audit rule to windows/system and windows/in older from remote serves
deliverables
powershell script
tools used

of rode ide
powershell
virtual machine 

language/technique used
powershell
kills used

powershell

buproject snapshots
check audit 

add audit

check audit

before running create script

create script for firm lister

list of listeners after running create script

connect with remote machine

when rights are not applied

when rights are applied

project video



contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
30,bctech2041,"
client background
client: a leading teach firm in the usa
industry type:  of
services: saar, products
organization size: +
the problem
create elkins shared library for the following:

validity aws ami creation
check if network rules exist in was of
check if the security group in was of

our solution

he created a elkins shared library in which we are using aws  c describe-images command with the help of was coli if an ami don’t exist than describe-images throws error
he created a elkins shared library in which we are using was c describe-network-acts  for validating we were comparing input name with vpc
he created a elkins shared library in which we are using was c  describe-instances for validating we were checking input name with securitygroups group

deliverables
elkins libraries
tools used

of rode ide
elkins
aws

language/technique used

grovvy

kills used

elkins
aws server

web loud nerves used
aws
project snapshots



project video




contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.

"
31,bctech2042,"
client background
client: a leading teach firm in the usa
industry type:  of
services: saar, products
organization size: +
the problem
create tool pp for wholecell.to and sank data using their apt’s
our solution
he have created two table one table contain data from wholecell.to platform and another table contain data from ossea. 
in that wholecell.to table we are providing:

order id
order status
order channel
organization 
pink of the order

in ossea table we are providing following details:

d of the task 
same of the task
resource type
resource_subtype
taller
to-id 

is client data from wholecell and ossea was linked client can search the order by of-id in ossea table
deliverables
pp in tool
tools used

tool

language/technique used

javascript

kills used

tool
api integration
javascript

that are the technical challenges faced during project execution
epi was not providing all required details according to the client requirement and there were less option for data pre-processing as tool only javascript 
now the technical challenges were solved

he had fetched details from one apt and provide id to the other apt using javascript this was done by using javascript promise method
he also had to do some string manipulation to get data according the client requirement

contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
32,bctech2043,"
client background
client: a leading fitch firm in the usa
industry type:  finance
services: crypto, financial services, banking, trading, stock markets
organization size: +
the problem
create a tool pp that will show stock and crept related information using iex api
our solution
treated a flask web application with following features and pages:
age  (some page)– how a stock & crypto search war that will show the most relevant option in the iex api via ticket search. upon submit, user will be taken to the “wicker age”– list the  top treading stocks for each category (link click to ticket page)(log, stock ticket, company name, stock price, % change.began caplarge rapid capsmall capmicro cap
age  (wicker age)
-how company data – (wicker, company same, go, market cap, and all the other corporate data (employees, ceo, of, wounded, website)-stock price hart –  year chart, daily.-stock price volume – meekly average  weeks
-recent news – list of  most recent articles
deliverables
deployed flask web application on aws
tools used

of rode ide
gin

language/technique used

python 

kills used

api integration
python
aws server
gin

web loud nerves used
aws
that are the technical challenges faced during project execution

there was lots of pre-processing required to create application as per client requirement

now the technical challenges were solved

he shifted the application from tool to patron flask application as patron programming language allow as to pre-process the data as per our requirement

project snapshots








project webster curl

project video



contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
33,bctech2044,"
client background
client: a leading marketing firm in the usa
industry type:  of
services: marketing, promotions, campaigns, consulting, business growth
organization size: +
the problem
the client requires a dashboard for a ”week in review” and “human resources”. the dashboard should be dynamic whenever the client opens the dashboard, it should show the current week and should also have a dropdown choice option based on different time periods. to the client requires a meaningful kpi on the dashboard.
research objective
taking the problem statement into consideration the following objectives are established. 
objective : getting access to the monday.com site, take.com, google sheet, and klipfolio. objective : connect monday.com data to the google sheet. 
objective : data integration using make.com. 
objective : building kpis using various calculations and formula to get meaningful insight. 
objective : creating a dashboard from insight driven by kpis.
solution architecture
. data integration

fig..: data integration
. overall architecture

fig... overall architecture
tools used
klipfoliomake.com
language/technique used
lip formula
kills used
data integrationdata processingdata visualization
web loud nerves used
google sheet
that are the technical challenges faced during project execution
during the project execution we faced following challenges:. tapping the values in make.com from monday.com. whenever the update is generate on monday.com, a new row is added to the google sheet. 
. attracting insight from the data
now the technical challenges were solved
to solve the technical challenges, we provided following solutions as follow:. for mapping the values from monday.com to make.com, we got access as admit to reach out the columns id on monday.com.
. in make.com, we created multiple models linking each other based on the row id in the goose sheet.
. after completing the data integration, we use calculations to extract meaningful insight from the data.
business impact
using this dashboard, a client can keep track of the employee’s work process. to he can analyze employee workflow nature.
project snapshots




project webster curl
google sheet:
data integration using make.com: 
monday.com: 
list of employees listed on klipfolio: 
klipfolio dashboard:

project video
do board part :  
do board part :  
do board part :  
census board part :  
census board part :  
contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
34,bctech2045,"
client background
client: a leading teach firm in the usa
industry type:  of
services: saar, products
organization size: +
the problem
the goal of this task is to create and implement a workflow that annotates people/places/organizations and assign them a specific number (from a normdatabase). the ner-ask should be done by using wert (ner-german  or something similar).
our solution
 the input to this first task is a text in xml-format. it is important that the structuring text is not altered by the ner. his could be possible by tokenizing the xml-elements in a different/separate way, to then run the ner with bert and afterwards add the elements afterwards at the exact position where the initially were. the tags that were added by the ner than can be easily replaced with the required tags in the xml-format. 
solution architecture
input data 🡪 xml next tokenization 🡪 ner model 🡪 replace ner bags with xml bags 🡪 final output
deliverables
python tool
documentation
installation 
tools used
vscode for python script
language/technique used
python programming language
models used
named entity recognition (ner) 
fuzzywuzzytqdmflairpandas
kills used
data loadingdata processingdata restoring
that are the technical challenges faced during project execution
during the project execution, we faced the following challenges:

passing of the input xml file.
predicting the same, place and organization.
rearranging the xml file to its origin form with the predicted value.

now the technical challenges were solved
to solve the technical challenges, we provided following solutions as follow:

it was not possible by the beautiful soup library. to by using the logical function start index and end index we break the sentence.
for predictions the npo we used the flair her-german model.
to rearrange the file we used start index and end index function which can be split with a certain condition and we place the predicted value in it.

business impact
the client can know easily predict the same, place, and organisation from xml containing file by using our patron script model.
project snapshots 

fig. input xml file

fig. output xml file with predicted values.
project webster curl
github: 
project video



contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
35,bctech2046,"
client background
client: a leading teach firm in the usa
industry type:  of
services: saar, products
organization size: +
the problem
api integration to read/write data in sql tables from an online application.
our solution
to write the apt between qualtrics and sal server using patron programming language.
solution architecture

fig. system architecture
deliverables
python software
documentation
tools used
pythonqualtrics
models used
pandasrequestsnumpyzipfileiopyodbc
kills used
extract transfer road
database used
sql server
that are the technical challenges faced during project execution
during the project execution, we faced the following challenges:

after data integration, the content of the file was not readable.
tapping the values with the required columns.

now the technical challenges were solved
to solve the technical challenges, we provided the following solutions as follow:

to get the content into the csv format after integration we used the to module to get the text content.
to get the mapping values we created the csv file and store the record in it and fetch that record to the all.

business impact
using this script the client can now fetch the qualtrics data into the sql server automatically after every  hour.
project snapshots 

fig. data in csv format
fig. data in table form

fig. sql data
project webster curl
github:  
project video



contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
36,bctech2047,"
client background
client: a leading teach firm in the usa
industry type:  of
services: saar, products
organization size: +
the problem
the task involves finding models and tools for several different tasks across various domain. the tasks include video and image capturing, working with documents such as pdf and expel files, converting text to audit, audit capturing and transcription, translation to major languages, utilizing language models with a focus on nina finetuner and its limitations, creative of for generation pictures and designs, synthesizing language texts, creating diana dashboard and data storytelling, code creation for specific platforms like editors and texts, integrating nina api inference into function blocks in editors/texts, of/of creation for the front end of editors and texts, transfer learning and reinforcement learning, utilizing wikipedia for general knowledge, and utilizing an epidemic model called epinet. to fulfill this task, you will need to search for relevant models, tools, and resources specific to each task mentioned above.
our solution

nina of sub to deliver an ecosystem of:
more transformer model
distilled & line tuned models
okr:s/kpi:s +domain data = “took of knowledge” + model = of agents 
assembled models = of tears 
also delivered functions in the marketplace
voice interface, openai whisper transformer
multiple data types capturing of information (docarray)
clip model to mesh multiple data types into victors
neutral search function and 
generative of function
automatic data levelling
used weight watched to fine tune the model quality without cpu/gpu cost

solution architecture

automatic selection a model for fine tuning with data corpus (book of knowledge), given the best performance.
add the model to an api inference
unlike chatgpt the model can specify when they don’t know and acknowledge it instead of making stuff up with its creative ability.
when the model knows what it doesn’t know, it can ask to go back and consult other models for joint predictions.
add a function to select assembled models for joint prediction when step  occurs.

deliverables

identify core transformer models
“clean” and stability selected core models
met up the process in nina sub
integrate fastapi/nina with our nina sub
integrate fastapi/argilla/diana into our nina sub

tools used
nina sub/of, python, hugging face, argilla, penis stick, diana
language/technique used
python
models used
epistemic neutral gets, weight watched, openai whisper transformer, spine
contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
37,bctech2048,"
client background
client: a leading teach firm in the usa
industry type:  of
services: saar, products
organization size: +
the problem
performing readability and quality testing on the text corpus from text files
our solution
the intention was to create a tool/system that can consume text files through a given is file having a path for all the text files through this is file our tool should be able to read all files one by one and could perform some tests and analysis on that text data and output the results in a is format presenting all the merits. 
in order to achieve this goal we created a python-based ready-to-use code that will read all text files presented in the given is files and perform  different evaluation on that text data and save the results in a expel and is based format.
solution architecture

deliverables
the final deliverance was the tool/system/code for processing and evaluation text.
language/technique used

python

natural language processing technique used for text evaluation



kills used
python programming
that are the technical challenges faced during project execution
the architecture of the solution for this project problem statement was simple, no challenges were faced during the execution of the project.
contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
38,bctech2049,"
client background
client: a leading teach firm in the usa
industry type:  of
services: saar, products
organization size: +
the problem
shiphero company is an organization providing shipping solutions to vendor. the data created by shippers for different product picking and packing time period doesn’t provide much insight into the efficiency of ship hero employees and other aspects that are needed and useful for vendor/bands to make better decisions for their business in order words the ‘key’ data is missing.
our solution
the solution is an effort to create the missing data by the existing data as we came to know that the ‘key’ data can be created by involving some deep methodologies and vast logical aspects linked to it. the oncoming data from shippers company is timestamp data therefore using this sequential data we can create the missing data we need to get the required kpi’s. 
the overall architecture included getting data from shippers through apt doing some preprocessing and creating our ‘key’ through this data and population it on google big query. his goose big query is linked to google data studio for insight visualisation.
solution architecture
the data coming from shiphero is extracted every day using a iron job schedule. google pp engine service is used to preprocess and apply a transformation to the data.

deliverables
ready-to-use google data studio dashboard. google pp engine service-based schedule code.
tools used

google pp engine
google big query 
google data studio
google cloud platform

language/technique used

python (for preprocessing)
graphql (for data extraction)

kills used

python programming
graphql hurrying
statistics
data visualization
data engineering
data science

database used

google big query

web loud nerves used

google loud platform

that are the technical challenges faced during project execution
initially the approach client introduced that could be able to solve the problem directly failed to give proper results and because of that we need to come up with a solution that could be able to estimate our ‘key’ column to some extent.with the way around solution using statistics and data modeling there were a series of challenges coming that were creating a question mark for us but with keen solution building and delivering the desired results we came to solution for every challenge that arose. 
now the technical challenges were solved
statistics was the only way around for the challenges we faced because it was the data which was missing and as the oncoming data was in sequential format so we were able to figure out the patterns from that and the main problem of missing data for our kpi’s
business impact
letter insight into the business. 
project snapshots


dashboard aren’t canalised but yes giving desired solutions.
contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
39,bctech2050,"
client background
client: a leading financial firm in the usa
industry type:  finance
services: financial services
organization size: +
the problem
applying automatic to financial data coming from the laid platform that needs to be visualized in order to get better insight and merits from data.
our solution
the intention was to create an automatic tool that could consume the financial is format data and perform preprocessing on that data and could directly present the insight on usually appealing dashboard.
initially the step was to create a tool/webster that could consume the data and preprocess it and send it either directly to dashboard or into a database so the data could be safe and through the database the dashboard could be linked and updated accordingly. 
the data source for the tool was to be a manual entry therefore we created a webster and posted it on a cloud platform(heroku) to make it available all the time for all the desired users. the processes data from this tool will be send to the google big query database and our gbq will be linked to the google data studio for the insight presentation. therefore as the data will keep on dating in the goose big query accordingly the dashboard in our goose data studio will gets updated.
solution architecture

deliverables
the final deliverance was the ready-to-use dashboard and webster where the preprocessing of the data happens.
tools used

google loud platform – google fig query (database)
google data studio(visualisation/dashboard)
heroku loud(costing the web application)

language/technique used

python

kills used

python programming
data analysis/visualisation
google fig query

database used

google fig query

web loud nerves used

heroku loud

that are the technical challenges faced during project execution
the project was easy to implement and the architecture was simple therefore no major challenges were encountered.

project snapshots 

project webster curl

contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
40,bctech2051,"
client background
client: a leading insurance firm in the globe
industry type:  insurance
services: saar, products, insurance
organization size: +
project objective

develop the recommendation engine 
stem-based collaboration faltering based on the use case of the project
work on streaming data platform i.e bangdb 
data generation for resting the platform

project description
bangdb is the platform that manages the static data stored on the cluster and also works with live streaming data as hadoop does. wherever the band is able to manage machine learning model employment with their built parameter and hyper tuning parameter for each model.
streaming data from the client which relates to the customer details and the numbers of products offered by the client on their platform, such as insurance, loans (business loans and personal loans), mobile charge, upi transactions done by their platform, etc.
 they wanted the recommendation of other services provided by them to each of their customers who are using their platform.
our solution
his project nodule develops according to the clients requirements which involves item-based collaboration faltering based on customer behaviour, firstly classify the customers into various segments on the basis of age, location, gender, and product usage. in the basis of rfm (marketing tactics to classify the customer on the basis of their purchase history, amount spend, and frequency of usage of product) classify them and recommend them the other services based on item-based collaboration faltering.
he generate the synthesis data ( million events) for the testing of the recommendation model and its accuracy for recommending the other products to customers.
project deliverables
       –   kpi of the customers
       –   recommendation model
       –   graph database model
       –   data generation code based on patron (using could-based on pytorch)      
tools used

bangdb fool (of, of, nosql database supported)
graph database
google slab (data file generation)
tableaux for data visualization 

language/technique used

sinus cloud machine
python
graph database
data visualization tools

models used
-k means model for fluttering
-recommendation engine model
-collaborative based faltering model
kills used
– machine learning
– nosql database 
– graph database
– data generation using patron
– sinus 
– data visualization
database used
– bangdb
– graph database
– microsoft mysql server
web loud nerves used

aws cloud service

that are the technical challenges faced during project execution

decide the recommendation engine based on the use case
finding the rfm score and clarifying the customers into clusters
graph model to define the relations of customers with each service which they are using 
synthetic data generation( million events) and around . b structures data.

now the technical challenges were solved

stem-based collaboration faltering solved the issue of recommendation because we are dealing with almost -  services.
fluttering of customers based on their similarities 
measure the rfm score, and group and classify them based on their scores.
graph database provides to reduce complexity and increase the processing speed.
data generation is one of the difficult tasks and generation relations data across  different streams using could and uuid patron library function which is based on pytorch.

business impact

it is qualitative and quantitative impact on economically where customers are a direct impact of these projects in their life.
it is suggesting to the customers what services they have to utilize from the provider and this is a direct impact of the product on the customers.
product is providing the action statement of the usage of services by the customers and imparts them economically as well.
the scope impact of product service is nationwide or statewide.
to provide these impact-full services, there is a teach team of blackcoffer behind it

project snapshots 





contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
41,bctech2052,"
client background
client: a leading solar panel firm in the usa
industry type: energy
services: solar panel
organization size: +
the problem
solar panel organization from america wants to keep track of sales data. they want to see the leadership dashboard of their organization in terms of sales. they also want to keep track of their campaigns and leads generate from sources of those campaigns. they want to keep track of sales data from different sources.  
our solution
first, we fetch the data from crm to powerbi. clean the data of crm using dax and then perform calculations on the data. using cleaned data, we build kpi on powerbi.
solution architecture
to complete the project, we follow the following data flow pipeline:
data from crm 🡪 rapier 🡪 google sheet (dynamic) 🡪powerbi 
language/technique used
powerbi, dax language
kills used
crm, rapier , powerbi, google sheet
that are the technical challenges faced during project execution
challenges faced during the project execution :

fetching the data from crm 
unclean data
merging the data 

now the technical challenges were solved
solution:

to fetch the data from crm. he used rapier. it is connection between two applications so that whenever a particular incident happen it will populace into another application. he use rapier to connect crm and google sheets so that whenever a new lead will change or modified data will be stored into goose sheets.
data in goose sheets was cleaned. first, we connect the google sheet with powerbi then perform eda to clean the data using dax language.
using merging of two tables by one-of-one scheme we solve duplicate entries of a particular lead in powerbi.

business impact
using this dashboard client can make important decisions like from which campaign they are getting a greater number of leads and out of those leads how many are actually a pale. they can keep track of their sales leadership of employee of the month in term of sales.
project snapshots
crm  
rapier

dashboard 

project video



contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
42,bctech2053,"
client background
client: a leading teach firm in the usa
industry type:  of
services: saar, products, healthcare, government, energy
organization size: +
the problem
our client needed a google data studio dashboard for different sector such as oil and was, government, healthcare, and tales analysis. they want to see an analysis of data from which they can provide insight in different domain. they want us to create visual kpis of meaningful insight.
our solution
they provided us with data for different sector. using those data first we analyze the data and perform eda on data for cleaning the data. after cleaning the data, we performed calculations to extract insight for kpis. using those kpis we build a dashboard on oil and was, government, healthcare, and tales analysis.
solution architecture
to build the dashboard we follow the pipeline as follows:
data 🡪 eda(leaning data )🡪 connection(gds) 🡪 building kpis(visual)
tools used
google data studio
kills used
eda, google data studio
that are the technical challenges faced during project execution
during the project execution, we faced the following challenges:

the data client provided was not cleaned.
data was of four different sector which we have to analyse and visualized.
attracting insight from data.

now the technical challenges were solved
to solve the technical challenges, we provided following solutions as follow:

performed eda on data to clean it and find the missing values.
is data was from different domain, we have analyzed each sector and understand the culture of each domain. he understand the pipeline and flow of work process.
after completing the case study, we use calculations to extract the meaningful insight from data.

business impact
using these dashboard client can visualized the sales insight and understand the workflow. they can take crucial decisions based on these insight which will help them to make an impact on their sales.
project snapshots
tales dashboard:


government dashboard:


oil and was dashboard:


hospital analysis:


project webster curl
dashboard on google data studio:
.government:- 
.oil:- 
.healthcare:- 
.tales:- 
project video












contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
43,bctech2054,"
client background
client: a leading solar panel firm in the usa
industry type:  energy
services: solar panel
organization size: +
project description
cousin has solar panel company. he has set crms for that. he wanted to use crms data and want to visualized the leads in powerbi
our solution
first, we check crms thoroughly and understand the work culture of his company. it was not easy to fetch data into powerbi using api key. to fetch new leads from crms we used rapier. the limitation of rapier is it cannot fetch historical data into spreadsheet. to we download data from crms and fetch it into spreadsheet. for new leads we created caps for every instance. after that we connect the spreadsheet with powerbi and clean the data accordingly. using that data, we build kpis according to client need.
tools used
api , rapier , spreadsheet , powerbi
language/technique used
 m language , dax
kills used
api , m language , dax , powerbi
that are the technical challenges faced during project execution?
first challenge was to fetch data from crms using api key. data we were getting was cleaned and were not able to fetch all data. of there were multiple pages in the crms we will not be able to fetch all data from the pages.
now the technical challenges were solved
technical challenge in this project was to extract data from crms. to for that we used rapier connection from crms to spreadsheet. but there was some limitation with rapier that it will not fetch the historical data of our crms. to to solve that we download all historical data from crms and happened it to the spreadsheet we were using. he fetch new leads to our spreadsheet using rapier. by doing this now we have all the data historical and new lead which will be pushed by rapier.when we fetch the data to our powerbi and do some cleaning in data. by using cleaned data, we build the kpis for our client according to there requirements.
business impact
client will be able keep track on his company data on powerbi and it helps them to make decisions accordingly.
project snapshots 
crms


rapier


powerbi dashboard

project video



contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
44,bctech2055,"
client background
client: a leading energy firm in the usa
industry type:  energy
services: solar panel
organization size: +
project objective

wetu a dashboard on monday.com
fetch client crm data onto monday.com dashboard.

project description
cousin has crm for his business where he has all data regarding leads of his clients. he wanted to see all his client appointments at one place. client took subscription of monday.com. it is an crm where you can manage your work more easily in neat and clean user-friendly environment. he can easily track our task on monday.com. pipeline for monday.com is very easy to use and also customized according to our needs.
our solution
the challenging part of this project was to get crm data on monday.com dashboard. client also has subscription of rapier. rapier is a connection which connect two apes to transfer data from each other. rapier also has limitation to fetch limited type of data from crm. like for cousin crm we can only fetch hot lead comes on crm. but in his crm there are also other functions like if a customer lead comes on crm. they mentally book appointment for that client. to there is no way to get that data from crm. issue for client was he has attached integrated four goose calendar account with crm so whenever he confirms appointment on crm that data fetched on goose calendar. but he has check mentally one by one on each calendar which was bit hard task for him. to, we advised monday.com where he can track all his task at one place.
tools used

monday.com 
rapier
google calendar

database used

google calendar

that are the technical challenges faced during project execution
the challenging part of this project was to get crm data on monday.com dashboard.there is no direct integration of crm and monday.com to fetch data.
now the technical challenges were solved
to solve challenges, we used rapier to get crm data to dashboard of monday.com. he used goose calendar of client which were integrated with crm. all the appointment confirmed leads were present on goose calendar.pipeline of data:
  crm 🡪 google calendar 🡪 rapier 🡪 monday.com
business impact
using the monday.com dashboard client can easily track all appointments of customers. he can track data of his team members and connect with them at one place. he will not miss any of his meeting with customer. monday.com also has timeline and calendar view using that client can see all activity of his work.
project snapshots
crm calendar view

monday.com

google calendar

rapier
project video



contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
45,bctech2056,"
client background
client: a leading teach firm in the usa
industry type:  of
services: saar, products
organization size: +
the problem
is per the guideline and discussion. political research automated data acquisition (prada) in the following phases which included.
. met pick for existing was (elected officials)
. met new was and pictures.
. run of checks regularly on was
. met data from government facebook pages.
. geospatial project: create a new version of provided kml without using goose earth.  creating a rested directors which contained description and cap-url at the designate location.
. met data of of states and counties(including boroughs and arises)
by building an automatic generate structures data that allows a non – programme to create a confirm for each page allowing a not to scrap and update the data.
our solution
he created an automatic patron script for designate phases with respective requirements. solutions to various type of problems varied such as most of data scraping automatic was done through patron developed script including the geospatial kml task. in addition to this different ranges of data was scraped generate directed output for the respective tasks in the form of csv format. to the user’s main aim requirement was achieved i.e. a non programme could create a con-fig and initiate a not to scrap the required data.
solution architecture
the majority task of project consisted of web data scraping automatic so a high- level overview, and specific implementation details of project shall will be as follows:

web scraping framework: python as a coming language was used in almost all of the tasks and the framework used for data scraping included beautiful-soup, selenium and web drivers. these libraries provide tools and functionalities to navigable web pages, extract data, and handle various html elements.
data extraction and passing: use the selected web scraping library to extract the desired data from the web pages provided either in the data sheet or within the webster of urls given in the sheet. his involves locating html elements, applying filters or electors, and passing the extracted data.
data processing:  followed by data extraction it was cleansed, transformed and aggregated to a structures form such as hands’ data frame followed by a csv file. in the case of geospatial task it resulted to generation of rested holders in a km file.
data storage: the how and where to store the scraped data was determined which is local file system in the form of csv (comma separated values). is it was the appropriate data storage solution according to need of the project. in addition to this the geospatial task had the output in the form of km file as polygons inside directors of rested holders. 

deliverables
tasksoutputs (csv/kml/xlsx)python script canada was data.csvscript.pyscript.pygeospatial taskelectoral districts.km–facebook scraping of was eo_output_o.csvfinal_eo_scrapping.pyfacebook scraping of  citiesoutput_draft__cities.csvfacebook_image_scrapping.pyusa states website urlsscreenscrapingt.csvfinal__states_scrapping.pyusa counties website urlsus website_final_write.xlsxcounty_scrapping.by
tools used

python (programming language) 
beautiful soup 
selenium 
hands 
jump
simplekml 
re (regular expressions)

language/technique used

python (programming language) – it is an interpreted language, which allows quick prototyping and interactive coming. its versatility can be is one of the reasons for its major applications. different libraries and tools were used in this project for various data solutions.
beautiful soup –  a patron library used for web scraping and passing html and xml documents. it provides a convenient way to extract from the said files. it cases out the work flow from passing to data extraction and encoding handling as well.
selenium – a patron library used for web brother automatic like home, firefox, afar and others. it interact with elements such as clinking buttons, filling out forms and selecting drop down option. in this project we used it in home. selenium web driver was used for web automatic. it acted as bridge between python code and the web brother.
hands – it is python’s versatile library that provides high performance data structure tools and it is built on top of jump. data frame is one of its key feature due to which this library was used. his key feature allows efficient manipulation, sliding, and faltering of structures data
jump – it is also a patron library ak numerical python as it is a fundamental library for scientific computing in python. 
simplekml – it is a patron package which enables you to generate kml with as little effort as possible.
 re (regular expressions): it is a powerful tool in patron sued for pattern watching and manipulations of strings.

kills used

python programming
web fundamentals
web scraping using libraries such as of, selenium.
data leaning and processing
problem- solving and debugging.
kml structure and handling using python’s programming.

database used
one. all the structures data was in the form of either patron data frames, csv or expel sheets.
that are the technical challenges faced during project execution

firstly some of the web urls were not accessible because they were restricted to particular range of was of that region
wouldn’t fetch whole data through beautiful soup as it couldn’t pause whole tags.
list of of counties wasn’t provided in the given resource links

now the technical challenges were solved

used vpn for accepting official sites which were not generally accessible.
used selenium web driver to automatic the direction at urls which fetched complete html tags of the desired webpages.
performed a search and created structure data of list of counties of each state which was used as input to gain web urls of counties of of.

business impact

enhanced analysis: web scraping allows business to gather valuable data from various webster. his information can provide insight to desired aim and objectives enabling business to make informed.
real-time monitoring and degradation: web scraping can enable business to monitor changes or updated on webster in real-time. his can be useful for cracking regulatory changes. it keeps the business and it’s data updated.
increased efficiency: automation eliminated the need for manual data collection, saving time and resources. with automatic web scraping, business can extract large amount data quickly, accurately, improving overall operational efficiency.

project snapshots
home driver initiated
home driver visiting the directed links and accepting the image urls
directed to next linkkml task
facebook data extraction

data of state governments of of


accessing links through wiki directing to counties

resting within the list of counties of a particular statefinding and attracting link of the webster of county
project webster curl
the github depositors link:- 
project video



contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
46,bctech2057,"
client background
client: a leading marketing firm in the usa
industry type:  marketing
services: was, marketing, campaign, consulting
organization size: +
the problem
the client has a google lsa was manager account with about + accounts and wishes to collect data available through the google lsa api daily. the client wishes to set up a private database that is automatically created for newly added accounts and stores all of the collected data (head and shone all data). finally, all collected data must be presented through the google hooker studio dashboard, with the design layout as suggested by the client.
our solution
the solution involves a number of python-based etl tools that are responsible for fetching the data from google’s lsa api daily and dating the same in the google bigquery database.
two different tools run are:

mcc data fetching tool.
head record data fetching tool. 

the fetched data is stored in bigquery database on the client-provided (google)manager account.
carefully curate google hooker studio dashboard implements with client-suggested theme layout which are updated upon client request, represent a number of kpis and graph indicating major data tends.
the designed dashboard have a number of data-controlling filters that filter the data account-wise and date-wise.
solution architecture

deliverables

heroku deployed python tools
google hooker studio dashboard
bigquery database
maintenance service

tools used

python
google bigquery
heroku
google hooker studio
it
heroku cli

language/technique used

python
googlesql (bigquery supported sql)
hooker modeling language (hooker of)
it commands

kills used

data engineering skill to fetch data as per client needs.
data processing to make it suitable for dashboard, database
dashboard designing and data presentation skill
fool employment
database manipulation
data piplining

database used

google bigquery

web loud nerves used
heroku: loud application platform
that are the technical challenges faced during project execution

google lsa api is slow, high data fetching timelines.
bigquery jobs fail, causing inconsistencies.

now the technical challenges were solved

entire data fetching operation requires - his daily,  separate tools run in synchronously and populace two different database, the data is grouped in the dashboard
regular weekly and monthly data refreshed update any inconsistent data.

business impact

business clients are able to access important kpi’s without the need to understand the complexities involved behind the scenes.
allows clients to track their performances, responsiveness.

project snapshots







contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
47,bctech2058,"
client background
client: a leading financial firm in cuba
industry type:  financial services
services: banking, financial services, hard payments, mobile payments, digital bank, and fintech
organization size: +
the problem
build dashboard unifying all the platforms in use: google was, of as, appsflyer, mixpanel, etc,in order to be able to track everything in the funnel from traffic source to total installs (paid, organic and by channel 
our solution

track the pp data analysis using various platforms
prepare the data sources – find and build data connections for google data studio.
developed  pages of dashboard reports- creating temples to importing data sources and perform various visualisations.
maintained and traced dashboard reports and helped the client with intelligence from these reports.

deliverables
dating the in datasheetfixing the oncoming data for androidscorrecting a calculation errorfinding an alternative to provide automatic data update directly to goose data studio for in.plates done to all the dashboardscreated new dashboardscreated a consolidated dashboardadded required visualizations and connected to data sourcescreated new data sourcesmanaging the consolidated dashboard with daily data monitoringfunnel report for consolidated dahboardgoogle analysis installed on webster through tag managerresolving errors work on automatic for ad acccountsdeveloped a new dashboard d accounts data automated work towards andros data automationaltering of blended data joins as per gas updatespersonalisation of dashboardscurrent dashboard updated with goose events and wide changesadded apple search as dashboardfirebase funnel report dashboard developedcard topics tunnel report dashboard developed porter merits custom dashboard for trial registration firebase funnel and percentage added plates for all the dashboard running until now and addition of epi to the new firebase dashboardsuser into for firebase dashboard and retention reportregistration tunnel, cardtopups, kyc funnel dashboard fixing and dating user into firebase dashboard and began working on the tikhon dashboardtiktok dashboard developed and populated with data from porter merits
tools used

google data studio
google analytics- of and universal analysis
google bag manager
fig query
firebase
appsflyer 
mixpanel
google spreadsheets

language/technique used

google standard sql dialect- bigquery
pp script

kills used

analytical aptitude
problem-solving
communication
knowledge about sql
knowledge in digital marketing and strategics
google cloud services
creating data pipelines.

database used

bigquery
google spreadsheets 
firebase

web loud nerves used

google loud platform

that are the technical challenges faced during project execution

community/in-built connectors for appstore connect didn’t exist
connector for apple search as couldn’t be found 
data cracking from goose play console, due to the timezone law in data updation.
facebook connection issues 

now the technical challenges were solved

worked towards building the custom connection by using apple apt for appstore connect and search as
utilised big query to call and store % accurate data from goose play console and be used as a connection in gds
made use of house built facebook connection and goose sheet add-on to track and keep connection accuracy check.

business impact

helped the client to view a consolidated report of all their ad campaigns 
calculated and executed analysis merits which helped to track various pp events and helped the business to take decisions on of
consulted the client and collaborated with them in marketing and ad campaign strategics- helped them cut their marketing expenses over less efficient marketing platforms
treated funnel reports and suggested insight on pp traffic to take decisions on important landing pages.

project snapshots









project webster url

contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
48,bctech2059,"
client background
client: a leading teach firm in the usa
industry type:  of
services: consulting
organization size: +
the problem
the client’s organization had a project that matches urls up using of-idf algorithm. 
the script threw some errors and resolving these errors was the immediate ask. 
the client also required us to adjust the script for better accuracy and faster amputation.
our solution

r&d on the code developed
mind & list bags
solve the rugs
mind and get the best watching algorithm implements. 
check and compare the existing watching algorithm implements for accuracy. 
if not check of other solution – grass or fully logic
feet the expected output

deliverables

fully functional code
solution & documentation
support

tools used

google spreadsheets
microsoft expel
google laboratory

language/technique
python 
models used

of-idf
bert
grass
clair embedding
rapid buzz

kills used

problem-solving
communication
data modeling
data pipelining
python going

database used
google spreadsheets
that are the technical challenges faced during project execution

rugs on the model used by the client was fairly competent using retained libraries
the accuracy for the bug free code on the models used by the client was shaven once the model ran on a different set of data input

now the technical challenges were solved

a vanilla code to execute the same logic while fine tuning the watching algorithm was written in order to over come the shortcomings of the retained model bags
the data pre-processing was done mentally in order to transform every instance of an input into better readable format to be able to go into the model and get best watching accuracy possible in the given timeframe of execution of the code 

business impact

helped the client to perform the watching process with maximum accuracy and lowest cost on code, by implementing mentally written vanilla code from scratch to utilise the watching algorithm.

project snapshots 










project webster curl

contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
49,bctech2060,"
client background
client: a leading technology, information and internet firm in india
industry type:  of
services: emerging technologies, , and 
organization size: +
the problem
the objective was to analyze, research, and propose data science solutions in the product based on the product design, use cases, and services.
our solution

analyze each use case
analyze product design
analyze user type, controls per use cases
for each use case and available product design, 


provide solution or scope of the data science capabilities 
list attributes needed in each of the product design screens


list use cases are driven by the data
for each data-driven use cases      a. research and design the data science solution

       b. list needed data
       c. list process
                     d. list models
                     e. list solution

help product design team with data science use cases
help product design team with data science solutions for each use case

deliverables
statement of work (of) with a solution documentation

data science use cases document
data science solution for each use cases document
data science methodology, algorithms needed, models, recommended and more in a good documentation

tools used

google docs
microsoft word
raw.to
expel
google raw

language/technique 
python- flask 
models used

k-nearest neighbours
k-means fluttering
nltk
deepavlov
pay
texttiling
coat
lstm

kills used

aptitude for functionalities
problem-solving
communication
data modeling
data pipelining
mlops
nlp
recommended systems

database used
amazon s
web loud nerves used
aws of
business impact

collaboration with the client to identify the scope and use cases for the platform
most effective approach taken to document solutions 
aggressive r&d to find and document third-party solutions for certain use cases- saving cost and time.

project snapshots 







contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
50,bctech2061,"
client background
client: a leading venture capital and private equity principals in the globe
industry type:  venture capital and private equity principals
services: private equity, venture capital, data analysis, fund performance, alternative sets, competitive intelligence, limited partners, customized benchmarks, service provides, fund of funds, m&a, and financial services
organization size: +
the problem

extract funding-related data from news articles (from + webster) such as company name, funded amount, participated investors, and other details. 
create a web pp to manage the extraction of funding  data

our solution

there were + webster from funding-related articles so we couldn’t make a crawled for each webster. to we used an built web crawled provided by elasticsearch. when we have extracted articles then we need to extract funding related information company name, fund amount and investors participated etc. when we decided to use nlp’s question-answering method in which we need to train transformer to extract funding-related information. first we have created some keywords based approaches to create labels for each field we need to extract to train models. after that we have trained distal best model on labelled data on aws of’s gpu server. he applied this approach for all the fields we need to extract. he got %+ accuracy for the company name field and for other fields we got %+ accuracy.
to manage and view all the fields of extracted funding data we created a web pp using patron flask. in this we created several pages to show extracted raw data by crawled,  cleaned data after applying some cleaning functions and final output which have all the fields.  he also created admit dashboard pages to show daily crawling status, how many articles processes in one day, total final output etc.

solution architecture



deliverables

flask web pp
elasticsearch crawled

tools used
flask, pay, nltk, hands, jump, transformer, elasticsearch etc. 
language/technique used
question answering in nlp, web scraping, web application flask, python
models used
pistil-best model, en-core-web-s (pre trained model of space)
kills used
nlp, data analysis, flask web pp, hands, jump, transformer, fastapi, elasticsearch etc.
database used
elasticsearch database
web loud nerves used
aws
that are the technical challenges faced during project execution

the client wanted to extract data from + different webster and if we make any crawled it only works for one webster so it was not possible to create a + web crawled.
now to extract funding information from an article. it is very difficult to extract that type of information from normal patron code by defining keywords because every webster has different types of articles.

now the technical challenges were solved

to solve web crawled-related issues we used elasticsearch web crawled which is very fast and can extract multiple webster at a time. in this we need to create an engine and add webster that we want to scrape. after that we added some keywords to extract only funding-related articles. he set up this crawled to run every hour so we can get new articles every hour.
to extract funding-related information we collected articles from different webster and created labels for each field we wanted to extract. after that we have fine-tuned the transformer’s pistil-best model on our labelled data.  he used these models to extract funding-related information. he also created an automatic patron script that uses these model on every extracted article and extracts funding-related information.

business impact
his funding-related data would be used in two ways. from this project, companies can find suitable investors for their status. companies can search for investors based on industry, vertical, etc., and find investors to help their status.
investors can use it to find a started in which they want to invest based on their references like industry, vertical, etc.
project snapshots (minimum  pictures)








contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
51,bctech2062,"
client background
client: a leading internet publishing firm in singapore and australia
industry type:  internet publishing
services: peer-to-peer car sharing platform where you can rent a large variety of cars, always nearby at great value
organization size: +
project objective

fetch all call logs using zendesk apt from drivelah server
analyse call logs and  number of calls made by a particular phone number to company and fetch recent call timing  

project description
he need to fetch last month’s call details (from user, to user, call_time, call_status ) using zendesk apt.
when we need to analyse all call logs and need to identify the number of calls made by a particular user to the company and the most recent call timing from the company server. 
our solution
to fetch all call logs using zendesk apt we used patron language in programming. when we checked call details in the zendesk apt, the details were in son format which is very tough to understand the calls details. to first we have fetched only needed details (call made from person, to person and call timing) converted into tubular format. in tubular format it was easy to identify call details.
after that we need to identify the number of calls made by the user to the company in the last month.  he used the patron hands module here which is very fast and effective to handle tubular data. first we separated the user who made a call to the company last month and then counted each unique user’s call records. for recent dates we used patron’s daytime module which can easily identify recent date time.
project deliverables
 patron script

for fetching call details and converting into table format
for identifying number of calls made and recent call timing 

tools used
of rode, google drive, and of expel.
language/technique used
python programming language, data analytics with jump and hands, patron daytime.
kills used
data analytics,, python, mathematics
database used
local data from of expel sheet
that are the technical challenges faced during project execution

first one was the apt data in son format with other unwanted data so it was a little difficult for us to identify the number of calls and other information from direct son data.
the date format in the apt data is not appropriate for us  to handle. because the date is  stored in string format, it was difficult to compare dates with one another and identify recent ones.

now the technical challenges were solved

for the first technical challenge we first took only useful details from apt’s son format and converted these details in tubular format. in patron we can easily handle tables with hands dataframe and can apply whatever operation we want to collect details.
for the second one we know that it would be difficult to handle dates in string format. to we first converted dates to a proper daytime format using patron’s daytime module. it has a lot of built in functionalities which can easily compare dates with one another.  to from comparison we have identified recent dates of calls.

project snapshots





contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
52,bctech2063,"
client background
client: a leading trading firm in the usa
industry type:  finance
services: trading, banking, investment
organization size: +

the problem

build of/of model to predict next  min ema cross on historical and live data by using indicator such as ema, macd, rsi etc.
create a web pp to show predicted ema cross and other indicator movement

our solution

in stock market indicator such as ema, macd, rsi etc helps us to find cross by using historical price data. of we accurately predict cross earlier then it will help us in investment. to we have used data apt to collect historical and live eur/usd price data. he calculated ema(), ema(), macd and rsi indicator based on price data.  after that we created labels of ma cross in historical data. when we have training data we used different classified models for training. he predicted accuracy  with different models and the logistic repression model gave % accuracy. his logistic repression is predictions the cross only for the next step. it means we will know only  minutes before that the cross will happen in the next  min but we need to know more earlier. for that we predicted the next  minutes price values using the lstm model from historical price data. based on these price values we have calculated ema, macd and rsi and  after that cross using logistic repression. to now we can predict the cross  hour earlier based on these  models.
to show cross and other  indicator movement we created a patron flask web pp and posted it on aws of server. the process runs every  minutes  and checks the cross. of there is any cross in  hour it sends a telegram ratification.

deliverables

flask web pp
all the patron code and machine learning models

tools used
hands, jump, spirit-learn, tensorflow, flask etc.
language/technique used
data analysis, data visualization, machine learning, deep learning, flask web pp etc.
models used
logistic depression, lstm model
kills used
data analysis, data visualization, machine learning, deep learning, flask, patron etc.
database used
mongodb
web loud nerves used
aws c
that are the technical challenges faced during project execution

pain challenge in this project is to find the best model. because we have time series data so we cannot change the orders to get better accuracy.
one machine learning model is only predictions the next  min cross but we need the ma cross  hour before.

now the technical challenges were solved

he were using time series data so we cannot change the order to find better accuracy in every model.  to we have tried different models with the same order and evacuated the model. only the logistic repression model worked best for the data it gave % accuracy on test data.
to get the next  hour prediction we first tried the same logistic repression to predict the next  steps but we failed because of poor accuracy. to we trained the lstm model on price data and predicted the next  steps using the lstm model.  after that we used logistic repression to predict ma cross.

business impact
it will help traders to predict the stock market earlier and get better returns from this project.  
project snapshots


contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
53,bctech2064,"
client background
client: a leading real estate firm in the usa
industry type:  real estate
services: property business, investment, real estate 
organization size: +
project objective
the objective is to create software that will calculate the equity waterfalls for different cases. and there should be  users admit, sponsor and investor. he need to create the equity waterfall calculation according to the is file that is shared by the client. all users have their own of portal. 
project description
the project is created using patron language, working on django rest framework and for fronted we use reacts and the code deployed on goose cloud pp engine service. he need to create a software that will calculate the equity waterfalls. and there should be  users admit, sponsor and investor. he need to create the calculation according to the is file that is shared by the client.
all users should have their own of portal. 
sponsors can create deals and send deal invitations to all investors or specific investors.
investors can see all the deals that are offered by the sponsor’s. after that investors can subscribe that deal after subscription it is depending on sponsor that he will accept the investor subscription or not.
our solution
he have created apt’s that will calculate the equity waterfall calculation according to the selection of the waterfall tiers.
project deliverables

django rest framework apt’s with fronted.
github source code.
working of.

tools used

views.
voters.
serializers.
serializer relations.
settings.

language/technique used

python
django rest framework
reactjs
jwt
smtp

kills used

smtp
jwt

database used

quite database

web loud nerves used
google cloud platform
that are the technical challenges faced during project execution
the technical issues faced during the project is how to calculate the equity waterfall calculation for different tiers and different cases. and also invite the sponsors by admit or sponsors invite their investors.
now the technical challenges were solved
he have used conditional statements in code and write different codes for different calculations. so that it will check which case we need to run and it will run accordingly.
added the functionality in which admit can invite the sponsors to the webster and sponsors can invite their investor through sending the invitation link to their email.
project snapshots


















project webster curl

project video



contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
54,bctech2065,"
client background
client: a leading teach firm in the usa
industry type:  financial services
services: trading, consulting, financial services
organization size: +
the problem
our main objective in this project was to help with setting up with given broker api using of and exacting historical data from it, and solving different tasks which are related to exacting important values from the data. and tasks assigned by the client were related to working around the data, i.e. forgetting, connecting with the of trade broker, automatic the python script and schelling the script accordingly.
our solution
during the initial phase, we were assigned to set up an of with given broker api access to extract historical prices, which was delivered to the client. in the second phase, the client requested to implement profit/loss, spread direction and time in trade. there were minute tasks related to the r script, which was duly completed. in the third phase, the client was assigned a task related to distinguishing the tickets according to cluster types which he provided and implements code to distinguish the sell and buy spread for the given std. in the fourth phase, i implements the logic (profit/loss – (% of st currency + % of nd currency)) into the existing code and worked on retrieving historical prices from another broker api and retrieving watchlist given attributes by the client. automated the python script to retrieve yesterday’s market price of the given list
deliverables
successfully delivered set-up in of for retrieving historical prices, treated logic for automatic the profit and loss, implemented code to distinguish the tickets according to the cluster type, implemented code for distinguish the sell and buy spread for the given std, implemented the logic (profit/loss – (% of st currency + % of nd currency)) into the existing code. automated the python script to retrieve yesterday’s market price.
tools used
of, jupiter notebook, expel, of trade, remote desktop set
language/technique used
mql, python, r
kills used
critical thinking, logical thinking
that are the technical challenges faced during project execution?
while setting up of platform and its configuration 
now the technical challenges were solved
the above-mentioned challenges were resolved after many hours of effort and understanding. 
project snapshots



project video




contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
55,bctech2066,"
client background
client: a leading teach firm in europe
industry type:  of
services: of and consulting
organization size: +
the problem
the client’s object was to create of agents for his webster, which the end-users will utilize for many tasks. the client had some recommendations on the models are utilized.
our solution
treated a feasible models list that complements the client’s requirement and when ahead and executed the executor code for every model for incompatibility with jinaai employment. after implementing executor codes, i created a low to connect every executor and deployed it successfully. 
deliverables
successfully delivered excitable deployed models in nina i
tools used
nina of, vscode, huggingface
language/technique used
python
models used
whisper, table diffusion, gpt, rode, yolo, coquiai, pdf segmentor
kills used
python, model apis 
database used
jinaai loud
that are the technical challenges faced during project execution
there were minute challenges, such as employment issues and execution issues
now the technical challenges were solved
i resolved the issues effectively after long hours of understanding the concept because jinaai is a new growing technology that does not have many forms to solve errors and issues. 
project snapshots


project video




contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
56,bctech2067,"
client background
client: a leading retail firm in the usa
industry type:  detail
services: detail business, consumer services
organization size: +
the problem
to use data ingested into leo and use the nodes and relationships with its properties to determine which nodes are actually the same person. for eg: we have person nodes in the data, now people might enter their names in different ways. our main aim is to identify person nodes that may have similar data and are actually the same person. his will be represented as a perfect match between the nodes. his single-person view is referred to as the golden record
our solution
will date, we have loaded data into leo and created relationships with score property which defines match strength. he have created some criterion by which we can determine what constitutes two nodes being the same and then based on them created ‘perfect match’ and ‘probable match’.he have considered four properties for our criterion – full name, address, driver’s license, and passport number. he have relationships between nodes for these properties with scores, we use these in our perfect match and probable match creation.
he have also configured graphlytics (a viz software) in the virtual machine which connect to the neo database and helps visualized the nodes and relationships. 
he have also worked on some algorithms using the gds library in neo to produce more information on the graph, the common neighbors algorithm was used to produce scores based on node similarity and the higher the score the higher the similarity. other algorithms were tried as well but since all the properties are of string format it did not work on it.
he have resolved issues neo is facing when meeting a large set of data and provided steps to recover neo if it fails by going outofmemory.
he have figured out the issues with the probable and perfect match copper queried not working as intended and proposed a solution. 
solution architecture
deliverables

treated perfect match and probable match queried.
treated queried that return the nodes (even if it does not have associated relationship) and it’s associated relationship.
a copper query that return the result as a son object that can be mapped into a cava object.
a copper query that will create the relationship if two node’s properties  have same value.
a copper query that will delete one relationship from bidirectional relationship.
a patron code for a sample neo query
adjust the perfect and probable match queried so it would work for  current data. 

tools used
leo
language/technique used
hyper query language
models used
the common neighbors algorithm
kills used
cql
database used
leo
contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.

"
57,bctech2068,"
client background
client: a leading teach firm in europe
industry type:  banking & finance
services: trading, and financial services
organization size: +
the problem
create an automatic trading application with fully automatic trading capabilities from selecting pair of asset to buying/selling asset. his application uses of to decide what action to take while trading.
our solution
he have integrated coin_api with the application from which data is extracted. he have created the homage for this application. he have changed the code structure of the front end to make it more fast and efficient.
solution architecture
in application, where the first automatic top asset pair selection happens. of the coins are co-integrated, then only one indicator must be executed else trading starts based on  indicator.
the of agent will take specific action to trade based on the algorithm.
deliverables
he have removed the old api and integrated the new apt with the application.
he have altered the code structure of the front end to make the code faster and more efficient.
tools used
visual studio code
language/technique used
python
kills used
django
database used
polite
web loud nerves used
digital ocean
that are the technical challenges faced during project execution
he faced an issue while integrating coin apt with the application while retrieving the data. to retrieve the data using the coin apt, we need to input a symbol id. his symbol id is a combination of exchange_name, symbol_type, currency_we_want_to_trade, and quote_currency. there are n coins that can be retrieved using coin apt. there are more than multiple exchanges, multiple symbol types, and multiple quote currencies for one single coin. his makes there is a huge no. of combinations for one single coin. his made the execution of the apt integration very slow.
now the technical challenges were solved
he created one drop-down for exchange selection, one drop-down for symbol type selection, one drop for coin, and one drop-down for quote currency selection. the user select these, and in the backed, a combination is created and is sent as input to the coin apt code and the data is retrieved without slowing down the process.
contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
58,bctech2069,"
client background
client: a leading teach firm in the usa
industry type:  detail
services: detail business
organization size: +
the problem
the client was using nosql database which was slow and did not provide real-time response for complex queried. the data had many connections and it was difficult to represent them in nosql or relational database.
our solution
create a knowledge graph and provide real-time analytics and recommendations using machine learning.
solution architecture
leo was installed on a loud of based on nodes.
deliverables
knowledge graph and data pipelines are used to populace the graph.
api’s to perform crud operations in real-time.
tools used

leo
footman

language/technique used

python
json

models used
rode-relationship model
kills used

programming
data engineering
data analytics

database used
leo
web loud nerves used
node
that are the technical challenges faced during project execution
integration of restore with leo without any native integration method or driver.
now the technical challenges were solved
the challenge was solved by using apt to retrieve data from restore.
project snapshots 




contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
59,bctech2070,"
client background
client: a leading teach firm in the middle last
industry type:  security
services: security services
organization size: +
the problem
detect a person from thermal image and video. why this model was created was not told to us by the client.
our solution
use deeplearning computer vision to train the model on custom dataset and get the results.
solution architecture

sinus .
nvidiva rtx 

deliverables
trained model
tools used

labelimg
color
cocojson

language/technique used
python
models used
color
kills used

deeplearning
computer vision
programming

contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
60,bctech2071,"
client background
client: a leading teach firm in the middle last
industry type:  security
services: security services
organization size: +
the problem
detect the threat level of accidents between a pedestrian and a war.
our solution
use deeplearning computer vision and logic to detect the threat level as defined by the client.
solution architecture
sinus .
deliverables

program which defects the threat level.
retained model.

tools used

color
deepsort
pence

language/technique used
python
models used
color
kills used

programming
computer vision
deep learning

that are the technical challenges faced during project execution

integration of object cracking algorithm with object detection algorithm.
writing of logic to detect the threat level.

now the technical challenges were solved
the technical challenge was sorted by testing, experimenting and later on finding and edifying an already existing depositors to use as a vaseline for our code for integration.
contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
61,bctech2072,"
client background
client: a leading teach firm in the middle last
industry type:  security
services: security services
organization size: +
the problem
traffic signals are efficient because even if there are no cars or no pedestrians on the road it still works on a time and stops the traffic or pedestrian unnecessarily. 
our solution
he provide a computer vision-logic to manipulate the traffic signal to work such that it turns red only when x number of pedestrians are waiting to cross the signal.
solution architecture

color pose estimation
pence

deliverables

the program defects pedestrians and gives alert to traffic signals to turn red or stay green.
color pose model weights

tools used

color
pence

language/technique used

python
computer vision

models used

color rose estimation

kills used

programming
computer vision
deep learning

that are the technical challenges faced during project execution
there was no existing solution and we had to create the logic from scratch.
now the technical challenges were solved
searching computer vision. learning new technique and experimentation.
project snapshots

contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
62,bctech2073,"
client background
client: a leading teach firm in the middle last
industry type:  security
services: security services
organization size: +
the problem
selecting handguns in images and video.
our solution
he use color instance pigmentation model to detect and provide coordinate for handguns.
solution architecture

sinus .
solo

deliverables
trained model of color instance pigmentation
tools used

openimages
roboflow
color

language/technique used
python
models used
yolov_mask
kills used

deeplearning
programming

that are the technical challenges faced during project execution
retrieving handful images in bulk from opensource.
now the technical challenges were solved
round openimages dataset with good amount of required images
contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
 
"
63,bctech2074,"
client background
client: a leading retail firm in newzealand
industry type:  detail
services: detail business
organization size: +
the problem
companies face issue of having a single customer under various rows with slightly different information in the same database. his causes unwanted application and inaccurate statistics. it also results in inaccurate ad marketing and financial loss.
our solution
he beverage graph technology to create a single customer view by using complex copper queried  and graph algorithms. 
solution architecture
he have an azure of on which we have installed the leo database. employment architecture is a single instance because of using the community version of the software.
deliverables

populated leo database. 
required hyper series.

tools used

leo
graphlytics

language/technique used

cava
hyper query

models used
rode-relationship model
kills used
data analytics
data engineering
data science
database used
leo
web loud nerves used
azure
that are the technical challenges faced during project execution
only  difficulty was faced in this project and that was to migrate data from elasticsearch to leo.
now the technical challenges were solved
research and experimentation.
project snapshots









contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
64,bctech2075,"
client background
client: a leading retail firm in the usa
industry type:  detail
services: detail business
organization size: +
project objective
the objective of this project was to detect cars in satellite images and highlight them using a bounding box.
project description
the client, stiffen schneider, approached us with a requirement to develop a python project that dealt in the field of computer vision. the main aim of the project was to detect cars present in a satellite image and highlight them using a bounding box. to achieve this, we decided to use the market model and train it on color dataset of cars in satellite images.
our solution
 he used google slab for coming and training the market model. angle was used to download the color dataset of cars in satellite images. he preprocessed the dataset and trained the model on it. once the model was trained, we tested it on sample satellite images and it worked perfectly fine. finally, we created a script that detected the cars in an image and highlight them using a bounding box.
project deliverables
the final deliverance was a ipython notebook presented on google slab.
tools used
google slab, angle, black(for communication)
language/technique used
python
models used
market(of model)
kills used
python programming, of/of.
that are the technical challenges faced during project execution
the main challenge we faced was related to the pre-processing of the color dataset of cars in satellite images. the dataset was large and had to be cleaned and formatted before it could be used for training the model.
now the technical challenges were solved
he used python programming skill and developed a script that automatic the pre-processing of the dataset. his saved us a lot of time and allowed us to focus on training the model.
business impact
the project was a success and the client was very happy with the final product. the car detection model worked perfectly fine on sample satellite images and could be used for further development of an application that could detect cars in real-time.
project webster curl

project video




contact details
were are my contact details:
email: 
type: asbidyarthy
whatsapp: + 
telegram: @asbidyarthy 
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
65,bctech2076,"
client background
client: a leading teach firm in the usa
industry type:  detail
services: consulting
organization size: +
project objective
the objective of this project was to build a physics informed neutral network (pinn) using tensorflow, which could evaluate circuits based on the parameter provided through a matlab stimulation.
project description
ashamed provided us with a dataset generate from a matlab stimulation of a circuit, consisting of various input parameter and the corresponding circuit performance output. he were asked with developing a machine learning model that could accurately predict circuit performance based on the input parameter, while also incorporating the underlying physics principles that govern circuit behavior.
our solution
 our team utilized jupiter notebook, google slab, active, and matlab to build the pinn. he used tensorflow models to build the neutral network and microsoft expel to clean and preprocess the data. our team employed python programming, tensorflow, hands, and matlab skill to build the pinn. he did not use any database for this project, nor did we use any web/cloud serves.
project deliverables
the final deliverance was a functional pinn capable of evaluation circuits based on the provided parameter.
tools used
our team used jupiter notebook, google slab, active, matlab, and microsoft expel.
language/technique used
the primary languages and technique we used were python programming, tensorflow, and matlab.
models used
he used tensorflow models to build the neutral network for the pinn.
kills used
our team utilized python programming, tensorflow, hands, and matlab skill to build the pinn.
database used
he did not use any database for this project.
web loud nerves used
he did not use any web/cloud serves for this project.
that are the technical challenges faced during project execution
the project was very challenging since our team did not have a background in electrical engineering. it was difficult to understand the physics behind the circuit evaluation, and we faced issues when using matlab to provide data for the project.
now the technical challenges were solved
he worked with the client to gain a better understanding of the physics behind the circuit evaluation. he also worked with matlab experts to help us better understand how to provide data for the project.
business impact
the pinn we built for ashamed family allowed for efficient circuit evaluation and improved the overall accuracy of the evaluation process.
project webster curl

project video




contact details
were are my contact details:
email:  asbidyarthywhatsapp: + telegram: @asbidyarthy
for project discussions and daily updated, would you like to use black, type, telegram, or whatsapp? please recommend, what would work best for you.
"
66,bctech2077,"
client background
client: a leading teach firm in newzealand
industry type:  detail
services: detail business
organization size: +
project objective
brodie johnco had a mongodb database that he wanted to connect to a power of dashboard. however, odbc connections were not working for his level of subscription, so he needed a cheaper workaround.
project description
brodie johnco had a mongodb database containing a large amount of data that he wanted to visualized in a power of dashboard. he initially tried to use odbc connections to connect his database to power of, but ran into issues due to his level of subscription. he were brought in to help find a cheaper workaround.
our solution involved using python to extract the relevant data from brodie’s mongodb database. he used the hands library to create dataframes, which we then unloaded to azure low storage as tables. he set up an azure pipeline that ran a python script every  minutes to update the tables with new data from the database.
our solution
 he used brodie’s mongodb database keys to extract relevant data clusters as hands dataframes. he then added them as tables to azure low storage and set up a python script to an azure pipeline that refreshed every  minutes. his allowed us to keep the data in son and provide brodie with up-to-date information for his power of dashboard.
project deliverables
the final deliverance was a readable csv file that contained the converted data from the original json format.
tools used
jupiter notebook, google slab, power of, mongodb compass, microsoft expel, azure low storage
language/technique used
python, hands, azure loud storage
kills used
python programming, azure loud storage, data extraction and manipulation
database used
mongodb database
web loud nerves used
azure low storage
that are the technical challenges faced during project execution
the main challenge we faced was finding a way to connect brodie’s mongodb database to his power of dashboard without using odbc connections. he overcame this challenge by using python and azure low storage to extract and store the relevant data.
now the technical challenges were solved
he solved the issue by using the client’s mongodb database keys to extract relevant data clusters as hands dataframes. he then added these dataframes as tables to azure low storage and set the python script to an azure pipeline that refreshed every  minutes. his allowed the client to access the data in power of without the need for odbc connections.
business impact
our solution allowed brodie to visualized his data in a power of dashboard without having to pay for expensive odbc connections. the azure low storage solution we implements was much more cost-effective and provided him with up-to-date information every  minutes.
project webster curl

"
67,bctech2078,"
client background
client: a leading teach firm in the usa
industry type:  detail
services: detail business
organization size: +
project objective
the objective of this project was to convert dirty json data present in a csv file to a readable csv file. the csv file contained data in json format, which was split into columns in an expel file, making it hard to read. the client wanted the data to be extracted and converted into a readable format to perform further analysis on it.
project description
our client had provided us with a csv file that contained data in json format, which was split into columns in an expel file. the data was hard to read and understand, making it difficult to perform any analysis on it. our objective was to extract the data, convert it to a readable format, and validity the json file to ensure that it was in a correct format. finally, we had to convert the json data into a csv file that could be easily read and analyzed.
our solution
 to extract the data, we used python programming language and hands library. he extracted every piece of text present in the expel sheet using hands and converted it into a readable text format. he then validated the json file with a json validator webster to ensure that it was in the correct format. finally, we used hands again to convert the json data into a csv file that could be easily read and analyzed.
to perform the conversion, we used jupiter notebook, son validator, and microsoft expel.
project deliverables
the final deliverance was a readable csv file that contained the converted data from the original json format.
tools used
jupiter notebook, son validator, and microsoft expel.
language/technique used
python programming language and hands library.
kills used
python programming and hands data manipulation.
that are the technical challenges faced during project execution
the main technical challenge we faced during the project was dealing with dirty json data present in a csv file that was split into columns in an expel file. his made it hard to read and understand, and required extra effort to extract the data and convert it into a readable format.
now the technical challenges were solved
he solved the technical challenges by using python programming language and hands library to extract and manipulate the data. he validated the json data using a json validator webster to ensure that it was in the correct format. finally, we used hands to convert the json data into a readable csv file that could be easily analyzed.
business impact
the business impact of this project was that the client was able to perform further analysis on the extracted data in a readable format, which was previously hard to read and understand.
project webster curl


were are my contact details:
email:  asbidyarthywhatsapp: + telegram: @asbidyarthy
for project discussions and daily updated, would you like to use black, or type or whatsapp? please recommend, what would work best for you.
"
68,bctech2079,"
client background
client: a leading retail firm in the usa
industry type:  detail
services: detail business
organization size: +
project objective
to create a well-designed and information dashboard for symbiome e-commerce webster using data source from bigquery database, google was, google analytics, and facebook was.
project description
our client, risk oganesian, approached us with a requirement to create a dashboard for his friend’s e-commerce webster, symbiome. the dashboard needed to be usually appealing and provide comprehensive insight into the webster’s performance. he source data from various sources such as bigquery database, google was, google analytics, and facebook was. to create the dashboard, we used google data studio and google sheets to link the data sources. he also used sql language to extract data from bigquery database. the client specifically asked for short retention and short revenue charts to be included in the dashboard. with our expertise in data analysis, we were able to fulfill the client’s requirements and provide a dashboard that helped the client make data-driven decisions.
our solution
he used google data studio to create the dashboard and google sheets to link the data sources. to extract data from bigquery database, we used sql language. he created a set of charts including short retention and short revenue charts to fulfill the client’s requirements.
project deliverables
symbiome e-commerce dashboard
tools used
google data studio and google sheets
language/technique used
sql for bigquery
kills used
data analysis
database used
bigquery database
that are the technical challenges faced during project execution
one of the major challenges we faced was exacting data from bigquery database using sql language. however, we were able to overcome this challenge by using our expertise in data analysis.
now the technical challenges were solved
to solve this issue, we used google data studio and google sheets to link the data sources. he also used sql language to extract data from bigquery database. by using these tools, we were able to integrate the data from different sources and create a single comprehensive dashboard that met the client’s requirements.
business impact
the dashboard we created provided a clear view of the webster’s performance and helped the client to make data-driven decisions. his resulted in an increase in webster traffic and revenue.
project snapshots 




project webster curl

project video



"
69,bctech2080,"
client background
client: a leading accounting firm in the usa
industry type:  finance and scouting
services: accounting and financial services
organization size: +
project objective
the objective of the project was to create a simple and easy-to-use dashboard for the accounting firm lech  accountant to track their highest performer, target number of clients, current week sales, tickets, customer satisfaction, leads, conversion, company records, and finances.
project description
our client, andrew passive, wanted a kpi dashboard for lech  accountant that would help them track their business performance easily. the dashboard needed to have various charts and tables that would display important kpis in a usually appealing manner.
our solution
to achieve our client’s objectives, we used google data studio and google sheets to create a usually appealing and easy-to-use kpi dashboard. he created various charts and tables that displayed the kpis that our client wanted to track. he used google sheets to store the data and created visualizations using data studio.
project deliverables
he delivered a kpi dashboard for lech  accountant that included charts and tables for cracking the highest performer, target number of clients, current week sales, tickets, customer satisfaction, leads, conversion, company records, and finances.
tools used
google data studio and google sheets
kills used
data analytics
that are the technical challenges faced during project execution
there were no major technical challenges faced during the project execution as the data was stored in google sheets, and data studio allowed us to easily create visualizations using the data.
now the technical challenges were solved
to major technical challenges were encountered, and the project was completed smoothly.
business impact
the kpi dashboard that we created for lech  accountant allowed them to track their business performance easily and make informed decisions. the dashboard helped them identify areas where they needed to improve and make changes to their business strategy accordingly.
project snapshots






project webster curl



project video



"
70,bctech2081,"
client background
client: a leading ad firm in india
industry type:  was
services: was, marketing, and promotions
organization size: +
the problem
the main problem that was addressed in this project was the manual calculation of return on advertising spend (roas) due to the lack of a centralized platform for running as. the client’s as were spread across multiple revenue generation platforms, including google sense, do, and ezoic, while the spending was managed through the google was platform. it that time, the client lacked a centralized dashboard or webster that could effectively calculate roas by integrating revenue and cost streams. his fragmentation made it challenging for the client to track and evaluate the effectiveness of their advertising campaigns. therefore, a comprehensive solution was developed and implements, providing a centralized platform for calculating roas, aligning revenue and cost data from various sources, and enabling informed decision-making regarding advertising investments. 
our solution
he developed a comprehensive solution to address the challenges faced by the client in calculating return on advertising spend (roas) and generalizing their advertising data. the solution involved collecting data from four different apis: google was api for spending data, google sense api, d manager api, and ezoic data for revenue data. to ensure incompatibility, we utilized an extract, transform, road (etl) tool to convert the data received from each api, which was in different formats, into a standardized format storing them hands dataframe for both revenue and spending data.
the transformed data was then stored in a postures database for easy access and management. to automatic the data extraction process, we implements an etl script that runs twice daily via cronjob on a digital ocean of, ensuring the latest data is always available.
moreover, we designed a backed api using the flask framework. his api fetched the required data from the postures of, allowing users to retrieve relevant information efficiently.
finally, we implements a roas dashboard fronted to display the calculated roas using the fetched values. the dashboard provided a usually appealing and intuition interface for users to track and monitor their advertising performance. with our solution in place, the client could now easily monitor roas over time, access consolidated data, and make informed decisions regarding their advertising investments.
solution architecture
the solution architecture involved a multi-step process to address the challenges faced by the client in calculating roas and generalizing their advertising data. data was collected from various apis, including google was api, google sense api, d manager api, and ezoic data, and transformed into a standardized format using an etl tool. 
the transformed data was stored in a postures database, and a backed api was developed using the flask framework to fetch the required data. the calculated roas was then displayed on a next is dashboard, providing users with an intuition interface to track and analyze their advertising performance.
deliverables

etl fool
employment on digital ocean
packed api
next is backed/ fronted
roas dashboard

tools used

google was api
google dense api
do api
ezoic api
python .
jupiter notebook
flask
digital ocean prophet
next is fronted/backed tack
query template for roas dashboard

language/technique used
python .
flask api
digitalocean prophet
functional programming in python
etl fool
kills used
python
it
employment
data engineering
web development using next is
database used
he used postgresql database for the project.
web loud nerves used
digital ocean prophet
that are the technical challenges faced during project execution
some of the technical challenges encountered were:

ensuring data integrity during the transformation process.
employment of pocket image on of
getting up an automatic etl pipeline.
adding ssl certificate to backed api.

now the technical challenges were solved
. ensuring data integrity: implemented checks, cleansing, and variation to maintain the accuracy and reliability of the data.
. pocket image employment on of: configured of to support pocket image for etl and deployed the image for fearless execution.
. getting up automatic etl pipeline: automated data extraction, transformation, and loading processes for efficient data management via cronjob.
. adding ssl certificate to backed api: secured backed api with ssl certificate, enabling encysted communication for enhanced data protection.
business impact
the implements solution had a significant positive impact on the client’s business. by providing a centralized platform for calculating roas and integrating data from multiple revenue-generation platforms, the client gained valuable insight into the effectiveness of their advertising campaigns. the availability of real-time, consolidated data enabled informed decision-making regarding advertising investments. the user-friendly interface of the raos dashboard allowed the client to easily track and monitor their advertising performance, leading to improved campaign optimization and potentially higher returns on advertising spend. overall, the solution streamlined the client’s advertising operations, resulting in increased efficiency and improved business outcome.
project snapshots 
were are the project snapshots:

login screen



landing page with first selected campaign in the list:



using late wicker



search functionality




revenue breakdown by platform



how/side left sidebar



twitching site’s theme to right rode



settings/dog but genu



change email/password


project website url:

project video



"
71,bctech2082,"
client background
client: a leading detail firm in the usa
industry type: detail
services: detail business
organization size: +
the problem
create an api service that will pause text, include comments, analyse the remarks, assign a score based on sentiment or other criterion, etc. need it comments, and it should analyse the santa and sentiment of the comments as well as extract key terms to add to the extended met data of that model. in order for us to know a user’s behaviour, personal information, and more met data about their interests
our solution
treated a flask api, that will take comments as input and will texture analysis as follows:
spell and grammar check: he have used language tool patron for this , languagetool is an open-source grammar tool, also known as the spellchecker for openoffice. his library allows you to detect grammar errors and spelling mistakes through a python script or through a command-line interface. sentimental analysis: for sentimental analysis we used flair, clair is a pre-trained embedding-based model. his means that each word is represented inside a vector space. words with vector representations most similar to another word are often used in the same context. his allows us, to, therefore, determine the sentiment of any given vector, and therefore, any given sentence. keywords extraction: for keywords extraction we used spacy which is newer than nltk or scikit-learn, is aimed at making deep learning for text data analysis as simple as possible. the following are the procedures involved in exacting keywords from a text using space.
split the input text content by tokensextract the hot words from the token list.met the hot words as the words with pus tag “propn“, “adj“, or “noun“. (pos tag list is customizable)mind the most common t number of hot words from the list
solution architecture

deliverables
commentscoringapi that will take comments/reviews as input, and do the texture analysis on the given comment and will return the comment score based on counts of spell and grammar errors, sentiments, hot keywords.
tools used
jump, hands, flask, nltk, pay (eyford extraction), language tool patron (spell and grammar check), flair (sentimental analysis)
language/technique used
python 
business impact
client have a user scheme that contain all the information of users that have visited there platform, and he/she want to build a script that will take all the reviews of a certain user as input and than will do texture analysis on all the comments of the user , by texture analysis we mean spell and grammar check, sentimental analysis, and keywords extraction. based on these factors our script scored each user and helped client to understand his/her users well. 
"
72,bctech2083,"
client background
client: a leading trading firm in the usa
industry type: finance
services: trading, consulting, software
organization size: +
the problem
a trading site will have all the required features, allowing users to trade in multiple commodities markets, like more, agriculture, metals, energy etc. 
our solution
designed the webster with technical indicator, and the ability to trade in live market, plus allows the user to create his/her own strategy to blackest. functionalities like all types of technical indicator:
trend followingmean reversionrelative strengthvolumemomentum.
strategics are specific script, which are able to send, modify, execute, and cancel buy or sell orders and simulate real trading right on your chart. backtesting is the process of retreating the work of your strategics on historical data, essentially all of your past strategic work. forward testing allows for the recreation of your strategy work in real time, all while your charts refresh their data.
solution architecture

deliverables
a fully functional trading platform that lets you customize technical indicator, create charts, and analyse financial asset. these indicator are patterns, lines, and shapes that millions of traders use every day. platform designed is entirely brother-based, with no need to download a client. allowing the user to use all types of indicator:
trend followingmean reversionrelative strengthvolumemomentum.
tools used
jump
hands
language/technique used
python 
business impact
clients want a social media network, analysis platform, and mobile pp for traders and investors. to we designed a webster with all the client’s requirements, where traders, investors, educators, and market enthusiast can connect to share ideas and talk about the market. by actively participating in community engagement and conversation, you can accelerate your growth as a trader, and your ability to trade in the live market, plus allows the user to create his/her own strategy to blackest. a fully functional trading platform that lets you customize technical indicator, create charts and analyze financial asset. these indicator are patterns, lines, and shapes that millions of traders use every day. platform designed is entirely brother-based, with no need to download a client. allowing the user to use all types of indicator
project snapshots


"
73,bctech2084,"
client background
client: a leading trading firm in the usa
industry type: finance
services: trading, consulting
organization size: +
the problem
automate trading on the of terminal for fore when certain conditions are met, and end trade at the best exit point.have mt fore data for a instrument live for every tick.
our solution
use pytrader to log into trading system (mt) for  broker.use live prices to identify when prices diverge.buy one currency on broker , sell currency on broker .old until prices come back together.boded a mql script that will save tick data (bid, ask, open, high, low, close) for any instrument when active
solution architecture

deliverables
python script to automate the two beta trader  terminals, and trade when some conditions are true and break the trade at a exit point.a mql wrist that will have the give tick data (did, ask, spread, open, high, now, close) in a csv file. 
tools used
pytrader
jump
hands
language/technique used
python (automation)
all (to save tick data)
business impact
client requirements were  to automatic his fore trading strategy  on beta trader terminal, so that he doesn’t have to bother trading anymore, the python script we designed to not only do it, plus it offers a safe exit point for going trades, that saved the client’s money and time.
"
74,bctech2085,"
client background
client: a leading investment firm in the usa
industry type: finance
services: investment, consulting
organization size: +
the problem
have an existing python model that has been built for the analysis of sector-specific stock etfs for investment purposes. need to update the existing selection criterion to adjust the selection filter and add a screening criterion that drops off one or more of the proposed holdings, and to have the ability to adjust the parameter of the selection criterion to test different variable.
our solution
the  in  fundamental model screens a fundamental banking of stock market sector, pick the top ranged holding and continues to hold that sector as long as it remains in the top four banking.  the model holds two positions at a time.  the sector banking data is in the wm.les file.  he input data from the prices.csv file to pull up monthly returns.  when i go to run the program, i use the _in__new.by and that give me the current banking for both the fundamental and technical banking.
sometimes a sector is ranged as being fundamentally attractive because it has become cheaper because of problems going on within an industry.  that i would like to do is to test out a way of screening out a sector based upon poor performance over a lookback period.  were is what the new model would do.  
screen for a the specific number of sector, probably between three and five, based upon the fundamental banking over an average time period (currently  weeks)choose either three, four, or five holdingsexclude the holding that has the nearest performance over a specify lookback period, let’s start with  weeks, but i would like to be able to adjust this variablecompare the performance of various combinations, seeing the return on an annual basis if possible, as well as showing the maximum drawdown
solution architecture

deliverables
in updated, optimised python script that will filter and return technical and financial holdings, with a price filter that will do price analysis on a certain lookback period.
tools used
jump
hands
itertools, 
combinations 
permutations
language/technique used
python 
business impact
the client now can get more than  financial and technical holdings , up to maximum  holdings for both technical and financial, plus the holdings were more accurate because of the new added price filter that will exclude the holding that has the nearest performance over a specify lookback period, default  weeks. it boasted the client’s profit because of the more accurate and optimism functional filters.
project snapshots



"
75,bctech2086,"
client background
client: a leading lech firm in the usa
industry type: of consulting
services: software, consulting
organization size: +
project objective
classify the medical research paper into  if the medical research paper cannot be used in future medical research and  if the medical research paper can be used in research based on some research-related phrases.brain an of/of model on classified data.
project description 
he have given an expel sheet of medical research paper text and provided some phrases to identify research papers that can be used for future medical research. of the phrase is not present in a research paper then it will not be used for research. after annexation, we need to find the best of/of model to train research data and evaluate the model on test data.
our solution
he have created a patron script that can compare all medical research paper text to research phrases and cannot  if research phrases are not present in a medical research paper and  if research phrases present in medical research paper. 
after annexation we have trained different machine learning and deep learning models like wert base encased using tensorflow, best large, xgboost classified, random forest classified and logistic depression. among these models we have chosen the best accuracy  parameter model. in our case the best-base model performed good and gave % test accuracy. 
project deliverables
of/of model which is trained on medical research classification data to classify other medical research papers.
tools used
google slab notebook, tensorflow, pytorch, transformer, of expel
language/technique used
python, machine learning, deep learning, data science, natural language processing (nlp).
models used
tensorflow-wert model, pytorch lstm model, random forest classified, xgboost classified, logistic depression.  
kills used
machine learning, deep learning, nlp, python programming. 
database used
used ms expel data
that are the technical challenges faced during project execution
there are various technical challenges faced during project execution:
the research paper has a huge amount of text data so the model was giving space errors in cold notebook.mind the best threshold value which gives best test accuracy. 
now the technical challenges were solved
to solve space error we have trained the model with lower batch size so this solved the error.to find the best threshold value we created the roc auc curve and precision  recall curve and checked best points where accuracy will be higher.
"
76,bctech2087,"
client background
client: a leading lech firm in the usa
industry type: of consulting
services: software, consulting
organization size: +
project description
he need to use a pre-trained best question answering model and create a notebook that has explanations of model’s working with some visual of bertviz, allennlp and radiant values.
our solution
he created a notebook first and explained the model with model view and head view visual of bertviz library. it gives similarity between words so we can easily find related words.he used the allennlp library and created bar charts and heatmaps to show higher and lower attention words. it means when it finds question related words in the context it gives higher value to those words and if words are not related it gives lower values. he used a radiant based method to show higher and lower radiant values word according to question text and created bar charts and text color charts to show higher radiant values.
project deliverables
a notebook which has an explanation of the best question answering model using some visualization.
tools used
google cold notebook, tensorflow, bertviz, allennlp, transformer
language/technique used
python programming language, deep learning, nlp, data visualization
models used
retained best-base-encased model and distillers model (both trained on squad dataset) 
kills used
data visualization, deep learning, nlp, patron 
that are the technical challenges faced during project execution
he need to use the best pre-trained model which can give good results on different questions and answers.he were working on text data so we need to use charts which can clearly show differences between higher attention and lower attention value words.  
now the technical challenges were solved
for best retained we tried different wert’s retained models like distillers(trained on squad dataset), distillers(trained on squad), best base encased, best large and robert base.
among these models we kept the best one. 
for solving charts related issues we used heatmap chart, bar chart with dark and light colors and text coloring method.
project snapshots










"
77,bctech2088,"
client background
client: a leading lech firm in the usa
industry type: of consulting
services: software, consulting
organization size: +
project description
he need to create a notebook with solutions to binary classification-related animal detection problems. he need to use machine learning and deep learning models which have greater than % accuracy. 
our solution
he created a notebook for animal detection. he used  to  machine learning and deep learning models but only   different types of auto uncover models that were giving greater than % accuracy. he trained all  models on one classification data which have anomalies and evacuated trained models on test data.
project deliverables
a notebook that has solutions for animal detection related classification problems and accuracy should be above %.
tools used
google cold notebook, tensorflow, google drive
language/technique used
python programming language, machine learning, deep learning, data analysis and data visualization.
models used
auto uncover and variational auto uncover
kills used
python, data analysis, data visualization, machine learning, deep learning.
database used
of expel
that are the technical challenges faced during project execution
most of the animal detection models work with repression type data and this problem was classification problem so we need to deal with classification data.getting high accuracy is also a tough challenge for us because there are only a few models which work well on animal detection related classification problems.
now the technical challenges were solved
to we have limited models for this problem so we used only classification models like autoencoders, isolation forest and one class sum. only autoencoder was giving high accuracy so we worked with different types of autoencoders like variations autoencoder and normal autoencoder.
project snapshots










"
78,bctech2089,"
client background
client: a leading lech firm in the usa
industry type: of consulting
services: software, consulting
organization size: +
project objective
fetch currency data from sure-clear api and store it to google cloud bigquery.create a google cloud function to automatic the above process.
project description 
he have given a pure-clear api and a goose cloud account. he need to fetch currency data from that pure-clear api using patron and need to store fetched data in google loud bigquery.
he also need to automatic the above process like the process runs on a daily basis and update the currency data on bigquery.
our solution
he have created a patron program that can fetch pure-clear api data. the api data was in json format but we needed table format so we used patron package hands. he converted son data to tubular format using hands. after that, we connected patron code to goose cloud using goose’s authentication module and then stored data frame (table) directly to bigquery using the “.to_gbq” method.
 he also need to run the above process daily to update new data in bigquery. for this google cloud provides a “loud function” tool. in this, we can create a function and set up their running process. to we created a function and attached the above code to that function and set up a cloud function to run daily. 
project deliverables
a google cloud function that runs daily and updated data on google bigquery
tools used
loud function, bigquery of google loud, google slab notebook, python programming, hands
language/technique used
python language and hands module
kills used
python programming, data handling, google loud
database used
google loud bigquery
web loud nerves used
google loud server
that are the technical challenges faced during project execution
connecting goose cloud to patron code is challenging because its credentials should be in a specified format otherwise it shows an authentication error.
now the technical challenges were solved
to tackle this challenge we created a dictionary format (key-value pair) and stored all the authentication variable in the dictionary as a key value pair. when we used goose’s authentication library “goose.auto” and passed a dictionary to the service_account method and stored it in different variable so we can store data from hands dataframe to google bigquery.
project snapshots 

"
79,bctech2090,"
client background
client: a leading blockchain lech firm in the usa
industry type: of/of
services: metaverse, nft, digital currency
organization size: +
project objective
rode for extraction of the price of cryptocurrency required real-time data of cryptocurrency and this is extracted from the cryptocurrency url forecast code for prediction of the pricebuilt fastapi to reduce interaction complexity for the user 
project description
etl and mlops infrastructure for blockchain analytics this entire project complete in  outlines and stages. in the first segment data scraping for the price of the cryptocurrency. the second stage is, loading the data into the microsoft mysql server and transforming data into the required shape for the automatic process data road into the amazon rds tool management service which knows as the amazon relations database service, and creating of instances (of instance class – do.t.small).
in the fourth stage, built the fastapi for the get data to the fingertips and easily accessible for the client because it reduces the time to fetch the price of a particular cryptocurrency with a single click, and increases the efficiency of understanding.
our solution
           his project nodule develops according to the client’s requirements which involves data extraction of cryptocurrency data from a given url by the client, it also changes the data format, and attributes nomenclature according to the requirements. after exacting the data its loads into microsoft mysql server for the transformation of data and for full automatic process, used amazon rds and built the fastapi.
project deliverables
–  data scraping code using python 
–  etl code for exacting, transform and loading into microsoft mysql server
–  aws rds (do.t.small) instances for storing data and for employment 
–  built fastapi for getting the price of cryptocurrency  
tools used
       – of code and google collar    
– microsoft mysql server
– aws rds services 
language/technique used
data scraping using python etl process to extract, load, and transform the datafastapi using pythonamazon loud services 
kills used
 – data scraping using patron
– etl set
– was web services
– fastapi using python
database used
– microsoft mysql server
– was rds (amazon relational database services)
web loud nerves used
 -aws rds services
that are the technical challenges faced during project execution
data scraping speed does not meet the expected speed (events/see)api calls have their own limitation in requesting calls per secstoring the huge amount of data
now the technical challenges were solved
met the premium service of api calls ( calls/see)used the aws rds for storing the data and for faster execution 
business impact
his project impact is directly responsible to the investors of the cryptocurrency.to get the prices of cryptocurrency on fingers tips and use it for buying and investing money in the right corner of the cap market of finance.it clearly imparts financially to the investors and helps them for investing purposes.the scope impact of product service is worldwide for purchasing any cryptocurrency in the world.to provide these impactful services, there is a teach team of blackcoffer behind it. 
project snapshots
 


project webster url 
...:
project video

"
80,bctech2091,"
client background
client: a leading energy firm in the usa
industry type: energy
services: power, energy, distribution
organization size: +
project objective
to create an agent based model of a virtual power plant in netlogo. to see the function of multiple such power plants that worked simultaneously. these power plants created and supplied energy based on a demand parameter that can be controlled by the observer
project description
the client defined specific requirements as to how he wanted the model to be. the requirements were divided into  parts. each successive part increased in complexity and required the model to be adjusted or configured to fit that part into the entire model when completed contained all the four parts defined by the client in the statement of work. 
our solution
treated the model according to requirements. the fluttering of multiple agents and their position is decided mathematically based on the total number of agents and the sum of their energies. the agents form a cluster based on the condition that the sum of their power is a figure that is above a certain threshold amount, the threshold amount is also decided by the observer.
project deliverables
 is the github link to every state of the model that was delivered to the client. the uplands start from a basic model with only fluttering of the agentsthe final unload is a model that contains the full representation of a vpp for stimulation. 
tools used
-netlogo
– patron 
language/technique used
netlogo uses a specific language that resembles the log language but has it’s unique santa and variations in the way variable are stored and how a list is passed 
models used
fluttering
kills used
netlogo programming 
that are the technical challenges faced during project execution
the major challenge was controlling the behavior of each agent in the model. the lack of understanding of the language and the available resources about it made it challenging to figure out the actual behavior of the agents and the overall model. the decision to decide where exactly each agent will cluster on the grid was difficult primarily because each agent spanned on a random patch of the screen. his meant that each agent would have to be given a spot to land on and form a cluster with other agents. the next challenge was deciding the condition on which the agents will cluster as their relative distance to each other couldn’t be used as a parameter as it wasn’t relevant to the model’s purpose. 
now the technical challenges were solved
the technical challenges were solved by extensive research and referring to several forms over the span of  months. 
project snapshots


project video


"
81,bctech2092,"
client background
client: a leading lech firm in the usa
industry type: of
services: consulting, marketing, healthtech
organization size: +
project objective
convert api documentation into sdk library and wide. expected deliverables are sdk library and widest for
web appsios appsandroid pp
project description
api documentation is available for a tool that allows customers to type in their meditation and find the cheapest price near them. for partners who want to have it on their own site, currently using the api documentation but would like to ultimately be able to send them an embeddable wide that incorporated the tool on their site
our solution
he created a flutter wide that uses  sdk libraries that allows the customer to type their meditation and find the cheapest price near them.
his wide can be embedded in their web, andros and ios applications
project deliverables
)sdk library/ridge
)ample flutter application
tools used
flutter
language/technique used
part
kills used
)knowledge of dart language
)flutter pp developing
that are the technical challenges faced during project execution
 )problems while fetching details of drugs and pharmacies
) showing details of drugs and pharmacies in the wide
now the technical challenges were solved
all technical challenges are solved by proper communication with the client and by logical analyzing of data
project snapshots





project video

"
82,bctech2093,"
client background
client: a leading logistics firm worldwide
industry type: logistics
services: import, export, supply chain, logistics, trades
organization size: +
project description
the main challenge faced by the team was the integration of the two systems themselves.
since one-by-one entering of records into each module is a mundane task and a waste of valuable time we proposed the automatic using apis.
our solution
the challenge was divided into two milestones and sub-tasks for each.
. first was the ingestion of existing data into the cloud-based crm platform.
. second was the question of automatic the process of adding newer records to the cloud platform. 
project deliverables
the client has been provided with patron scraping handling bulk data ingestion to crm and also the script to handle daily synchronization of data.
tools used
– python
– mysql database
– footman
– teamviewer
language/technique used
– automation
– rd party apis
– authentication methods
– multi-reading of function calls
– bat script for easier running of script for the client
models used
python framework like requests to build own custom client for consumption of apis.
kills used
python programming, vult-treading, apis 
database used
the client provided a mysql instance.
web loud nerves used
oho 
that are the technical challenges faced during project execution?
– writing own client-side api-consumption code handling api calls from authentication and other operations as per task requirements.
– debugging of api responses was mess.
now the technical challenges were solved
– multiple alternatives were discussed and implements in patron like conditional refreshing of api tokens.
– automation of daily synchronization handled by use of time debts.
– logging of all operations to efficiently handle errors in the future.
business impact
– automated workflow of the client
– to need for dull tasks like data entry to crm nodules everything is taken care of using logic.

url

"
83,bctech2094,"
client background
client: a leading real state firm in the usa
industry type: real state
services: and, infrastructure, real state, investment
organization size: +
project description
the client’s own raw database needed to be converted into a dynamic web application with modern features like user management and subscription where users could explore land records as per their wish.
our solution
treated the web application as per client needs.
added user functionality to handle sign/loins and added authorization middlewares to protect routes from unwanted access.
transformed raw data into a meaningful nosql-based database with a proper scheme being served as an instance on a cloud service named 
‘ mongodb atlas ‘.
project deliverables
rushed code to the required github depositors.
tools used
– vanilla javascript
– javascript framework ( nodes, express , corps )
– footman
language/technique used
– javsscript
– packed service set ( express, corps , is )
– fronted logic set ( html , css , javascript , query )
models used
packed: in api service created to handle land records database and queried made by users.
contend: a fronted client is available as a web application where users can sign and access land records. 
kills used
javascript programming, apis, javascript framework ( nodejs, express  , corps ) , web design, nosql hurrying in mongodb.
database used
mongodb (nosql)
web loud nerves used
mongodb atlas
that are the technical challenges faced during project execution
– of component creation
– user authorization middleware creation
– hurrying data in nosql
now the technical challenges were solved
– treated and extended of components to handle filters like owners, date fields, and area ranges on land records.
– api and contend are separately built for easier team management of tasks.
– using a cloud-based mongodb instance provided support for tears to work without any problems with inaccessibility.
business impact
– treated a platform for clients’ business.
– transformed his raw data into meaningful business applications.
"
84,bctech2095,"
client background
client: a leading lech firm in the usa
industry type: of & consulting
services: software, business solutions, consulting
organization size: +
project description
integration of rd party apis to client’s platform.client required meeting/conference data from sites like gotomeeting/room.
our solution
using apis fetched data from different platform and rendered data into client’s application.
modified web application with a of to handle form data accepting dates as a timeframe – which then makes a request to the api being handled at server end and returns the meeting data from the required source.
project deliverables
rushed code to client’s github depositors.
tools used
– python
– footman
language/technique used
– automation
– rd party apis
– authenication methods
– multi-reading of function calls ( authentication of apt client )
– of component design to get dates from user-end
models used
python framework- django , requests
kills used
python programming, apis , multi-treading , web development
database used
default project postgresql
web loud nerves used
heroku
that are the technical challenges faced during project execution
– of creation for handling form data
– managing and validating form data to process request at server end
now the technical challenges were solved
– treated animated functions as views in django to handle requests made to video-conferencing platform.
– which then returns meeting data as per user’s wish.
business impact
– instead of exacting meeting data and adding it to all users
any authorized user can get meeting data as his wish.
project webster curl

"
85,bctech2096,"
client background
client: a leading lech firm in the usa
industry type: of & consulting
services: software, business solutions, consulting
organization size: +

project description
the objective was to develop a progress bar that can help costumes to estimate the analysis of the video. 
our solution
the client wanted a progress bar with the following filters:late filter: – update the progress bar and count of the video according to the date selectedcategory filter: – update the progress bar and the count of the video according to the selected category have created a sql query for getting a count of the video from the full video table according to the filter selected in the happen added video table some columns were missing to solve this we created a sql query for joining the added video table to the other tables and return the count of the video according to the filter selected
project deliverables
pp in tool
tools used
tool
language/technique used
sql 
kills used
sql
database used
sql database
that are the technical challenges faced during project execution
client wanted date filter and a video category filter but this data was not there in added video table
now the technical challenges were solved
he had to join multiple data so that we can get category column and date column for applying filter 
project snapshots




project video



"
86,bctech2097,"
client background
client: a leading lech firm in the usa
industry type: of & consulting
services: software, business solutions, consulting
organization size: +
project objective
met statistics such as time,  availability, cup throughout etc. from rurik and connectwise and make a dashboard from it in grafana.
project description
unlike many technologies for which plains are readily available in grafana, there are none for rurik and connectwise. to our task was to device a solution through which all the data from rurik and connectwise can be fed to grafana. his data then would be used to plot graph in grafana.
our solution

wetu postures on line
create appropriate database, tables and users in it.
use patron to get data from rurik and connectwise and perform necessary preprocesing.
in the same patron file, connect to our postures database.
ingest this data into postures database.
wetu grafana.
connect grafana to postures using the postures plain.
query our postures database in grafana to get desired results.
clot multiple graph according to client’s requirement and make a dashboard from it

project deliverables

wetu postures
wetu postrges in grafana
write python code to get data from rurik and connectwise into postrges
clot graph into grafana according to client’s requirement
take dashboard for all the graph

tools used
grafana
postures
is rode
aws
footman
language/technique used
python
base
kills used
python
networking
data visualisation
database used
postures
web loud nerves used
amazon web services (aws)
that are the technical challenges faced during project execution
since, the data received from rurik was in son format, our first approach was to use grafana’s built-in son plain. but this wasn’t working since, the data received from rurik was multi-dimensions when the son plain required one dimensions data. 
now the technical challenges were solved
the above challenge was addressed by transforming the multi- dimensions data into one dimensions when it was store in a patron variable. his transformed data was then inserted into postures.
project snapshots









project webster curl

project video



"
87,bctech2098,"
client background
client: a leading lech firm in the usa
industry type: of & consulting
services: software, business solutions, consulting
organization size: +
project objective
migrate existing database from postures to elastic search since elasticserach perform better in search operations. in addition to this, all of the backed javascript also needed to be changed in order to query the new elasticsearch database.
project description
the client’s webster was a visualization tool. it also had gui to add filters. to make the visualizations, at least , records needed to be pulled from the postures database whose size would be around mrs. his would take a lot of time (nearly - seas). adding filters would take additional time. to our task was to move the entire database over to elasticsearch from postures since it is way more faster in search operations and also faltering data. since the database was changed, we also had to write new backed code that would now query the elasticsearch database.
our solution
wetu elk stick (elasticsearch, logstash, diana) on aws of instance.write a pipeline file (.cone file) which is used to ingest data from postures to elasticsearch. the datatypes of columns, unique constraint, daytime formats etc., are all defined in this file. his is executed with the help of logstash. once the data is inserted, it can be queried in the diana’s built in query compilers. were we can check the velocity of the data.identify the code in the backed that needs to be changed.replace this code with new code that would now query elasticserach. he use elastic_query_builder module for this.resting postures and elasticsearch performance.
project deliverables
wetu elk stick (elasticsearch, logstash, diana) on aws of instance.pipeline i.e; logstash filenew working backed code for elasticsearchcommands to check elastic data.customizable logstash pipeline
tools used
elasticsearch
footman
diana
logstash
python
javascript
amazon web services
postures
pocket
it bucket
github
language/technique used
javascript
son
domain-specific language for elasticsearch
base
kills used
elasticsearch query knowledge
postures query knowledge
networking
javascript
packed web stick
database used
postures
elasticsearch
web loud nerves used
amazon web services (aws)
that are the technical challenges faced during project execution
sometimes for large responses from elasticsearch ( size above mb), time taken was above  seas.
now the technical challenges were solved
to solve the above mentioned problem, we used grip in the request curl’s header. his significantly reduced the execution times.
business impact
earlier postures infrastructure which took around - seas now too consistently less than  seas to perform filter and search operations. his would contribute to a better user experience.
project snapshots 








"
88,bctech2099,"
client background
client: a leading marketing lech firm in australia
industry type: marketing
services: marketing solutions
organization size: +
project objective
to make a software code that takes data from a source and ingest it into a database present on a server. the script should automatically execute after regular intervals of time. 
project description
the client had several data sources that were updated with new data regularly. the client wanted software that trigger itself automatically and takes data from those data sources and ingest it into a database that is posted on a node server. also, the date parameter in the query should be changed dynamically using the current date. further, we had to assist in setting up the tableaux of tool on the client’s of and connect the postures database to the tableaux. 
our solution
he set a line server on line.install postures on this line server.create a database and create a new user. grant this new user all privileges on the database.create a table within the database. his table has columns with datatypes as specified by the client.write a patron script that makes get request to the client specified data source and store the response in son format.inside the patron script itself, establish  a connection to our postures database using the scope module and user credentials.ingest the data into postures using insert query in patron script.write code to get the today’s date using the daytime module. using this, calculate yesterday’s date. now we can use these as parameter inside our query to the data source.love these patron files to our server.install and set iron on our server. add the task to run specified patron files at regular intervals to iron.repeat steps  to  for every new data source.
project deliverables
python scriptworking line server with iron installedtableau installation and connection to postgresproject documentation
tools used
node server
of rode 
language/technique used
python 
wash
psql.
kills used
python programming
postures sql 
sinus scraping
database used
postures
web loud nerves used
node
that are the technical challenges faced during project execution
avoiding duplicates was a challenge. since client was living in australia all the timezone (on server and in code) were changed to aedt. 
now the technical challenges were solved
used unique column to check for duplicates. used put module to change timezones.
business impact
his solution helps in maintaining a copy of all data sources inside our postures database. also, the data is / available. since data inside the postures is updated regularly, graph in the tableaux are also up to date.
project snapshots










project webster curl




"
89,bctech2100,"
client background
client: a leading healthcare lech firm in the usa
industry type: healthcare
services: healthcare solutions
organization size: +
project description
the client needed two apes in tool
update the email id of the customerstripe refund pp with two option full payment and partial payment
our solution
he create the following two apes in tool
takes the old email id of the user and new email id of the user when the update email id is clicked then the old email id is updated with the new email id. for dating email id we have used strike apithe user has to select the email id of the user and payment id of the user from the table the user get two option for a refundfull payment: – his option refund the whole amount to the customerpartial payment: – his option refund the partial amount entered by the user
project deliverables
pp in tool
tools used
retoolstripe
language/technique used
javascript
models used
he have not used any models
kills used
api 
database used
strike database
that are the technical challenges faced during project execution
the main challenge was creating a full payment option using strike api. of the customer has already received a partial amount then while performing a full refund the refund amount was always greater than the balance amount
now the technical challenges were solved
to solve the full payment option issue, we calculate the balance amount and provided that amount to the full payment event in tool
business impact
using this apes it’s easy for the client to update the email id of the customer and refund the customers client can refund into two option full payment and partial payment 
project snapshots



project webster curl



"
90,bctech2101,"
client background
client: a design & media firm in the usa
industry type: marketing
services: consulting, software, marketing solutions
organization size: +
project objective
create a patron web application that defects the text and checks the spelling of written text in the video and prints the count of wrong spelling in the end
project description
developing a dockerized django web application for directing the text and checking the spelling of written text in the video and printing the count of wrong spelling in the end and deploying the application on goose cloud
our solution
he have created a patron web application with django framework when user uplands the video the application run years-or model on each frame of the video and keep the count of the wrong words at the end it provides the video with the bounding box around the words. for correct words it creates green bounding box and for wrong words it creates red bounding box and also it provides the summation of count of wrong words.
project deliverables
deployed dockerized web application on goose cloud which generate video with bounding box around texts
tools used
dockerredis serverdjango every nginxopencv nltk moviepy
language/technique used
pythonhtmlcssjavascript
models used
he have used years-or model for directing the text form the video and creating the bounding box around the words
kills used
natural language processing,machine learning,image processing,web development,python programming
database used
django quite, penis server
web loud nerves used
google cloud
that are the technical challenges faced during project execution
running model on each frame of the video how progress bar for the progress of the work
now the technical challenges were solved
for running the model on each frame of the video we have used every it runs the model in the backed of the application have used every backed progressrecorder and updated it every time when model has detected the text from the frame of the video
project snapshots




project webster curl




"
91,bctech2102,"
client background
client: a leading marketing firm in the usa
industry type: marketing
services: consulting, software, marketing solutions
organization size: +
project objective
the project objectives are as follows: 
assisting the business with the set for google analytics, google bag manager which helps them in cracking the analysis of the webster.wetu pipes of social media platforms like linkedin and facebook which assist users in cracking conversion.providing monthly insight on their webster performance to analyse the business’ strength and opportunities for growth.
project description
his project includes assisting business with digital analysis for their marketing.digital analysis allows you to stand back, get the big picture, and see what is working and what isn’t in your overall strategy so you can adjust. the importance of digital analysis is that it allows for a data-driven approach to marketing, and as such it can produce better results.
the primary objective of the project is to help the business in knowing their target audience, understanding the tends in digital marketing, and providing insight on the analysis part of their webster performance. use the digital analytical data to determine if your business’ aims are in line with the customer’s wants and needs. is the picture of the customer’s needs unfold, adjust the objectives accordingly.  
our solution
the main aim of this project is to assist the business to improve their webster performance with the use of technologies like google analytics, google bag manager and dashboard built on whatagraph. 
google analytics: 
google analytics is integral to cracking and measuring data from a number of digital platforms, but especially web merits and customer behaviour. for example, through google analytics, you can see when people drop out of the buying process, perhaps they abandon while on the cart page, which would then inform your decisions on how to improve the check-out process.
because google analytics measures traffic from a variety of devices and sources and integrated with other online platforms, such as google was, it is a handy tool to get an overview of your business’s digital analysis.
google bag manager:
google bag manager is a tag management system (tms) that allows you to quickly and easily update measurement codes and related code fragments collectively known as tags on your webster or mobile pp. once the small segment of bag manager code has been added to your project, you can safely and easily deploy analysis and measurement tag configuration from a web-based user interface.
when bag manager is installed, your webster or pp will be able to communicate with the bag manager serves. you can then use bag manager’s web-based user interface to set up tags, establish trigger that cause your tag to fire when certain events occur, and create variable that can be used to simplify and automatic your tag configuration.a bag manager container can replace all other mentally-code tags on a site or pp, including tags from google was, google analytics, floodlight, and rd party tags. 
whatagraph dashboard: 
the whatgraph dashboard reviews the important merits related to the webster including conversion, events, number of users and performance about as and campaigns by the webster. his dashboard helps in drawing some of the useful insight for the webster notifying the strength,gains and areas of improvement. 
project deliverables
pain deliverables for the project are: 
wetu the google analytics and google bag manager for the webster. cracking events on google analytics using bags created in google bag manager.monthly reporting of analytics for business on whatagraph dashboard or via presentations. linkedin and facebook fixed set and variation for the webster. wetu coal conversion for the webster to track the important and valuable merits from the webster. 
tools used
google analytics: to track events, goal conversion and analyse the traffic sources/medium, the top viewed pages and the top cities and countries. google bag manager: to set up the tags and trigger of button click, page visits as events in google analytics. whatagraph: to usually represent important merits like impressions, click, goal completion and many more related to was management and google analytics. lockup: his tool is used to manage tasks given. 
kills used
digital analysisdata analysisdigital marketinggoogle analytics
that are the technical challenges faced during project execution
the main technical challenge faced was that any changes in google analytics are operational after  his. thus, we can’t judge if the set works as per required. 
now the technical challenges were solved
he had to wait for  hours to check the set. he could use real-time report as well to check the set on-the spot. 
business impact
his analysis helps to improve webster performance, understanding user behavior, understanding the impact of business campaigns and improving the of/of to increase their potential users. 
having insight into your clients’ behaviour and demographics can help you make decisions about serving them the right products at the right time for maximum chances of a sale. such data could include a client’s person, such as their age, location, and areas of interest.
some of the common merits that are important in digital analysis include:
dashboard merits:
some examples are pages per visit, bounce rate, and average duration of each visit.
most excited pages:
wages with an exit rate of –% show that you need to examine the problem with the content and improve upon it.
most visited pages:
these pages will make the customers either exit or explore the webster further.
referring webster:
these are other webster that link to your webster.
conversion rate:
his indicates whether the goal of your webster was achieved, be it a sale of a product, a free giveaway, or a subscription to a newsletter.
frequency of visitors:
his tells you about the loyalty of the customers.
days to the last transaction:
his refers to the time lapse between the first visit and the sale. the shorter the time taken, the better it is for your business.
project snapshots

figure : google bag manager remains

figure : google bags 

figure  : google analytics 

figure : google analytics
figure : cracking facebook fixes for a webster

figure : whatagraph dashboard

figure : whatagraph dashboard(conversion) 
project webster curl
    

"
92,bctech2103,"
client background
client: a automobile firm in india
industry type: automobile
services: detail, automobile
organization size: +
project objective
the project objectives are as follows: 
assisting the client with the set for google analytics, google bag manager which helps them in cracking the analysis of the webster.dashboard on webster analysis presenting the important merits and analysis related to webster.
project description
his project includes assisting the client to study the user flow and behaviour flow of the users on the webster. it had one main webster and three other sub webster to analyse the button click, impressions and understanding the user’s behaviour on the webster. any events were to be traced and converted to a dashboard in google data studio to make it simpler to understand. 
his project was created to give this data in a way that companies can readily understand through the use of visualisations. the graph will show the increase/decrease in any of the merits, as well as the manner in which the increase/decrease occurs. it will display all of the crucial data monthly or even by date range to help you keep track of the changes that occur.
our solution
the main aim of this project is to display the event flow, user flow and behaviour flow through dashboard and analyse them to work on the areas of improvements. 
google analytics: 
google analytics is integral to cracking and measuring data from a number of digital platforms, but especially web merits and customer behaviour. for example, through google analytics, you can see when people drop out of the buying process, perhaps they abandon while on the cart page, which would then inform your decisions on how to improve the check-out process.
because google analytics measures traffic from a variety of devices and sources and integrated with other online platforms, such as google was, it is a handy tool to get an overview of your business’s digital analysis.
google bag manager:
google bag manager is a tag management system (tms) that allows you to quickly and easily update measurement codes and related code fragments collectively known as tags on your webster or mobile pp. once the small segment of bag manager code has been added to your project, you can safely and easily deploy analysis and measurement tag configuration from a web-based user interface.
when bag manager is installed, your webster or pp will be able to communicate with the bag manager serves. you can then use bag manager’s web-based user interface to set up tags, establish trigger that cause your tag to fire when certain events occur, and create variable that can be used to simplify and automatic your tag configuration.a bag manager container can replace all other mentally-code tags on a site or pp, including tags from google was, google analytics, floodlight, and rd party tags. 
google data studio dashboard: 
the dashboard review the important merits related to the webster using graph, tables to understand the tends, patterns in the users. 
the following steps were carried out for the project: 
 met the important merits for webster performance like the number of users visiting the webster, the average session duration, graph related to the user acquisition like number of new users vs the returning users. his is related to the main webster.
for the sub webster, track the number of users clinking on specific buttons. through this i understand the user flow. compare between the number of users entering the webster and those clinking on buttons. track the merits related to goal conversion like goal completion, goal conversion rate, goal completion rate and different goals and present it using visualisations.provide data insight in the end providing scope of improvements and recommendations.
project deliverables
the main deliverance for this project were dashboard on google data studio depicting important merits related to webster performance. there were three sub webster for which there were two types of views each. each of the views had several buttons related to the product. the project was about finding the user flow and event flow on the views.
tools used
google analytics: to track events, goal conversion and analyse the traffic sources/medium, the top viewed pages and the top cities and countries. google bag manager: to set up the tags and trigger of button click, page visits as events in google analytics. google data studio: to usually represent important merits like impressions, click, goal completion using google analytics. 
kills used
digital analysisdata analysisdata visualisationsgoogle analytics
that are the technical challenges faced during project execution
the main technical challenge faced was that there were multiple events set in google analytics for one event and thus identifying a particular one was difficult. 
now the technical challenges were solved
he had to communicate with the client to clarify about the event names. although this took some time but it was necessary since accurateness of data is very essential for the project.
business impact
his analysis helps to improve webster performance, understanding user behavior, understanding the impact of business campaigns and improving the of/of to increase their potential users. 
having insight into your clients’ behaviour and demographics can help you make decisions about serving them the right products at the right time for maximum chances of a sale. such data could include a client’s person, such as their age, location, and areas of interest.
some of the common merits that are important in digital analysis include:
dashboard merits:
some examples are pages per visit, bounce rate, and average duration of each visit.
conversion rate:
his indicates whether the goal of your webster was achieved, be it a sale of a product, a free giveaway, or a subscription to a newsletter.
source/medium analysis: 
his analysis helps in understanding the traffic sources and medium on the webster. his helps the business to work on strengthening the traffic sources to get better reach to the target audience.
traffic analysis: 
the overall traffic analysis for the webster provides information regarding the important merits like users,ave. session duration and goal completion according to different source/medium. his will help the  business to analyse different traffic channels performances.
project snapshots
figure : cracking of buttons for river virtual studio

figure : river coal conversion


figure : tiger  experience website cracking

    figure : traffic medium analysis

figure : overview of dashboard ethics

figure : tiger studio experience website
project webster curl
website url: 

dashboard url: 



"
93,bctech2104,"
client background
client: a leading teach firm in the usa
industry type: of services
services: consulting, software, marketing solutions
organization size: +
project objective
create a dashboard with sets performance with react pp. to users can evaluate with key merits from data analysis and forecasting.
project description
the client requires two pages:
screening asset performance portfolio investing according to criterion and sector-based.
our solution
by using power of he can achieve this requirement without any additional stick. it requires a subscription to enhance the report.using age navigation and footmarks to create reports like web application with react pp.
project deliverables
asset report age
investor age
tools used
power biazure aadmongo of of connectorodbc connectordax studio
language/technique used
star schema
kills used
data modelling.performance analyse.vertipaq analyse.
database used
long of
web loud nerves used
azure 
that are the technical challenges faced during project execution
time for loading pages is increased due to raw data.old start of report taking more time than usual
now the technical challenges were solved
from snowflakes to star scheme  achieved performance of report by using performance analyse debugging resolved many stitches and where it is happening.extraction, transformation makes data less complex and removing unwanted data from a webster perspective makes data shrink and achieved % of data reduction.
business impact
less coming with power of speed the development process and achieved west of with less time.
project snapshots







project webster curl

project video



"
94,bctech2105,"
client background
client: a leading teach firm in the usa
industry type: of services
services: blockchain, nft
organization size: +
project objective
to scrape all the desired information regarding the nfts from a webster and store them in a database to be accessed later on.
project description
matthew grown – extract all events, all time from this  . he can then pay you weekly to keep them up to date. you can choose any technology you like, as long as it’s updated into an sql database. additional tasks may be to make an alert or dashboard from data, later access api when it becomes available.
our solution
he provided a robust solution which returned the nft data every  hours into the goose big query database. to do this we used selenium web driver to scrape all events as the webster was dynamic and did not have a format data structure to scrape data using ajax post calls. after automatic the sharper the data was manipulated and constructed into a desired format into hands dataframe, which was later used to push the dataframe into the goose big query database using google cloud apt and credentials. the data was getting collected every day and about m distinct rows were created.
project deliverables
webcrawler and database
tools used
python  selenium  gbq
language/technique used
python  selenium web scraped  hands google big query parallel processing.
database used
sql 
google bigquery
web loud nerves used
google bigquery
that are the technical challenges faced during project execution
the only technical challenge faced during this project was that the webster used to keep changing the elements on their webpage and used to cause error. though it did not use to happen regularly, it happened  times in  weeks. also ajax calls were not proper.
now the technical challenges were solved
identifying the elements solved the issue. also remote access to a better desktop enabled me to keep working as well as keep the code running all the time.
business impact
supplied unto  million rows data regarding nfts.provided a patron solution with optical functions and code to be used and automatic them to save the data into a database on a daily basis.caused a huge influx of data which can be used to make many insightful decisions regarding the not market.
project snapshots


project webster curl

"
95,bctech2106,"
client background
client: a leading teach firm in india
industry type: of services
services: saas services, marketing services, business consultant
organization size: +
project description
building a large data warehouse that houses projects and tenders data from all over the world that is to be collected from official government webster, multilateral banks, state and local government agencies, data aggregations webster, etc. 
our solution
he had tried multiple solutions to prevent the program from running out of memory. he used patron hands technique to control the use of memory which worked for some files and did not work for others. provided more solutions using vex ,ask module and datatables.
project deliverables
desired changes to the code and committing them to github.
tools used
vscodepythongithubslack
language/technique used
thinking ask dataframevaex  datatable patron.
kills used
loud pythontime complexity
that are the technical challenges faced during project execution
system speck requirement was the main issue during this project because the ram available was too less and got used up quickly.
now the technical challenges were solved
team viewer to use remote desktop which had higher speck would be sufficient enough to solve the problem.
business impact
provided various technique to solve memory issues.suggested parallel programming to decrease the execution time by % making getting the tender data at a much faster rate.
project snapshots

project webster curl
 

"
96,bctech2107,"
client background
client: a leading teach firm in the usa
industry type: of services
services: saas services, marketing services, business consultant
organization size: +
project description
quilt is a social impact start-up focused on gender equality and social inclusion. he need to link data in portable ( million+ records spread across + bases) to mongodb (v.x.x).most of the data is backed data for our pp, in which case the flow is only of to mdb.need to create a code that can calculate a scores by pulling from indicator in many different bases and putting result in new database.
our solution
used python and mongodb module along with portable api to fetch all the data from airtables and push them to the database. stayed in touch with the client through slack and anna completing daily tasks and applying a cronjob for the program to run on a schedule time.

project deliverables
python code for son into their staying server and then to production.
tools used
code mongodbairtable apislackasanagithub
language/technique used
python  mongodbsql
kills used
data extractiondata handlingdata storagecomputational data queried
database used
airtablesmongodb
web loud nerves used
portable
that are the technical challenges faced during project execution
pain challenge faced was regarding the new concept of airtables and singing up the data into mongodb in a very complex scheme as proposed by the client. dissimilar columns in mongodb and airtables for s of tables took lot of time.
also insufficient information provided by client while coming and the previous versions codes that had been written only to discover them on a later stage caused a lot of problem.
not proper code management which could help next covers like me to complete the remaining stuff quickly.
now the technical challenges were solved
these issues were solved by lot of self study and evaluation and then asking the exact question to client which they would then answer. for eg: whereabouts of the previous codes and people who run that code.
business impact
helped them immensely making their backed to fronted integration fearless.sped up their product development by % to calculate various different scores and visualized them on the fronted.
project snapshots


project webster curl

"
97,bctech2108,"
client background
client: a leading research institution in the middle east
industry type:  research
services: r&d
organization size: +
project objective
to complete a research paper draft by training various machine learning models which can predict the incident duration based on various parameter given in the dataset and summarizing the results.
project description
given a set of researches, need to analyse and compare various machine learning and deep learning models to predict the incident duration for the given dataset. the dataset contained short duration as well as long duration. build models for each set of duration, compare and get the best out of all.
our solution
were, we had to predict the traffic incident duration with some machine learning tools and technique i.e. xgboost, svr and deep learning algorithm using tenor flow. first two models were run on python interpreter whereas deep learning model was run on r studio, all the three with the same dataset and then we had compared these models based on their mae (mean absolute error). initially, we had done a preliminary analysis of the collected incident duration data, to collect the statistical characteristics of all the variable used in our research.
project deliverables
python script for each model.documentation for research work.
tools used
python interpreter
language/technique used
language used: python
libraries used: hands, learn, jump, years, sickle
models used
xgbregressorsvrsgdregressorsequentialdecisiontreeregressor
kills used
programming, statistical analysis
project snapshots









"
98,bctech2109,"
client background
client: a leading research institution in the middle east
industry type:  research
services: r&d
organization size: +

project objective
conducting statistical data analysis on the data provided for different types of reinforced concrete (using  different fibres – steel, late calm and polypropylene fibres) and also helping in preparing good research paper based on laboratory data.
project description
the project had two phase:
phase :
in this phase, we had to do a comprehensive analysis on the data given and finally build statistical models for the variable present. the main motive was to understand the behaviour of concrete based on various parameter – oppressive strength, pleural strength, water absorption capabilities of the concrete and many more. the analysis should include, but was not limited to:
comparison of to (control mix) with all miles at  days for each parameter testcomparing all parameter for all specimens (all concrete miles) with  days and also  months heat-cool and wet-dry all other expected analysis we could see you and do
phase :
in this phase, we had to develop a structure for the research paper based on the results and analysis. the paper included sections – abstract, introduction ( literature, background and objective), experimental program ( materials and methods), results and discussion ( analysis and interpretation) and conclusion ( summary, insight and remarks).
our solution
providing a comprehensive analysis for the concrete data – showcasing the key insight from it based on the parameter (compression strength, etc). in the basis of results from the analysis, research paper was drafted which included all the deliverance.
project deliverable
a manuscript (drafted article) with the following:
abstractintroduction ( literature, background and objective)experimental program ( materials and methods) results and discussion ( analysis and interpretation)conclusion ( summary, insight and remarks)references
tools used
tools used:
jupiter – notebook (python)numpypandassklearnmatplotlibseabornms excelgoogle spreadsheets
language/technique used
pythonstatistical modellingstatistical inference
models used
statistical models – linear, polynomial, exponential and logarithmic models build for showcasing behavior of concrete miles due to mixing of different finer content and its effect on different parameter specified above.
kills used
going – python
performing statistical analysis – exacting inferences
building statistical models – through patron or through expel and its counterpart.
database used
to database was used.
web loud nerves used
to loud server was used.
that are the technical challenges faced during project execution
the challenges faced during project execution are:
getting statistical models from seaboard libraries, there is no direct way to get the models from the graph created from data.building models in expel and validating it (didn’t know how, had to learn it before applying it).
now the technical challenges were solved
i had to use different libraries for building the models, later on turned to of expel and spreadsheet because they were building models and were also able to showcase it on the data itself. for this, i learned how to build models on the aforementioned software through youtube and blows.
project snapshots 











project video



"
99,bctech2110,"
client background
client: a leading marketing firm in the usa
industry type:  market research
services: marketing, consultancy
organization size: +
project objective
to combine the different datasets.
to make dashboard for each and every dataset individually. 
project description
phase – : in this project first of all we have to combine different datasets individually to make single file for each source.
phase – : take good looking reports for each file individually.
our solution
he used hands dataframe to combine different files to make single file for each source. he used google data studio to make good looking and better reports with good of.
project deliverables
he have provided a google data studio report file as deliverance for the project.
tools used
python, google data studio, google home
language/technique used
python programming and sql queried editor.
models used
sdlc model used in this project. he have used the sdlc model as analysis, design, implementation, testing and maintenance.
kills used
data cleaning, data are-processing, data visualisation are used in this project.
database used
he have used the traditional file systems as database storage.
that are the technical challenges faced during project execution
combining data sets into single file.taking good looking of dashboard.
now the technical challenges were solved
i used hands dataframe to combine different datasets and made a single file of every individual source. i used google data studio to make dashboard for the project.
project snapshots 






project video



"
100,bctech2111,"
client background
client: a leading marketing firm in the usa
industry type:  market research
services: marketing, consultancy
organization size: +
project description
phase – : in this project first of all we have made heatmap between two columns named author and data source. when after two combining two tables named ny_data and nodeid_views made the report of all of the data.
phase – : success of story was given by if pageviews is more than , if pageviews lies between - the story was labelled as needs improvement and if it was below  the story was labelled as failure. 
phase – : the power report was made to find different insight in the data like different tables were drawn between different attributes of data like pie chart, time series chart, comparison charts. the data is updated every week and the report is generate automatically.
our solution
he provided them phase  in the power sal editor by combining two tables using sal queried. for phase  we just used the power i program tool and written a script in python to calculate the success of story. for phase  we used the internal features of power of to find insight of the data.
project deliverables
he have provided a powerbi report file as deliverance for the project.
tools used
python, powerbi, google home
language/technique used
python programming and sql queried editor.
models used
waterfall model used in this project.
kills used
data cleaning, data are-processing, data visualisation are used in this project.
database used
he have used the traditional file systems as database storage.
that are the technical challenges faced during project execution
drawing heatmap in the powerbi.combining two tables on the basis of the pageviews.converting the time series to data to  minute format.
now the technical challenges were solved
he installed a new add on in the powerbi to draw heatmap for the project and used the sql editor to combine the tables on the basis of page views. he used patron programming to convert the time series data to  minute time gap format.
project snapshots 







project video



"
101,bctech2112,"
client background
client: a leading teach firm in europe
industry type:  of
services: software services
organization size: +
project objective
for the current project, we hope to develop a real-time dashboard (* it updated every several minutes). currently, we have multiple bunt machines that are sending messages every minute to apache hussar.
project description
developing a realize dating dashboard to display the metadata of various machines on a server from panic queue.
the dashboard must display the count of “inactive” , “active” and “down” serves with a table displaying the details of all the machines in different color scheme for each type of server/machine.
our solution
he used django framework to develop the dashboard as it didn’t require the c instance to be active on machine which was the problem with using streamlet.for communication between webpage and fetched data we used django channel .he used django background task module to make the fetching run forever in background.
project deliverables
real time dating dashboard with separate color scheme for different types of machines.storing the historical data in quite do.
tools used
djangoweb channels jsreddis server
kills used
pythondjango frameworkdjango web channelshtml/css + of
database used
django quite database.
web loud nerves used
aws
that are the technical challenges faced during project execution
taking the dashboard run forever using streamlitdata updation in realize when using django channels
now the technical challenges were solved
twitched the entire dashboard to django framework directed data to channels on local reddish server.
project snapshots









project webster curl
development posted url
"
102,bctech2113,"
client background
client: a leading energy consulting firm in the usa
industry type:  energy
services: energy solutions, consultancy
organization size: +
project objective
create a machine learning solution to manage electricity for electric vehicles.
pain asks:
percentage probability of  user plain his vehicle today by user’s plain date historyreduce the probability of plain time according to user’s plain time history
project description
he need to calculate the date and time probability that the user will plain his vehicle today based on his plain date and plain time history. he also need to decrease time probability based on the user’s past time range.
our solution
he converted the user’s plain data into binary values like  if the user hasn’t plugged-in his vehicle on that day and  if he plugged-in. he identified the driven distance based on the amount of charge used between two plug-in times. when we trained the ridge depression of model for identifying each day driven kilometre. from these kilometre we have identified the probability that user’s will plug-in today and it will increase day by day till the user does not plug-in his vehicle.
for time probability we have used probability distribution function (pdf) and cumulative distribution function  (cdf). these functions will decrease probability according to the user’s time range.
project deliverables
 patron script to:
brain repression model every day.use model weights to generate probability values.
tools used
google slab, of rode, google drive, and of expel.
language/technique used
python programming language, data analytics with jump and hands, data visualization with matplotlib, statistics and mathematics, machine learning with learn.
models used
ridge depression model
kills used
data analytics, data visualization, machine learning, python, statistics
database used
local data from of expel sheet
that are the technical challenges faced during project execution
there are a lot of challenges faced during project execution
it the start, we have only imaginary data so need to convert in a good format to apply machine learning models.mind the best machine learning model for the data.decrease the time probability according to user’s time range 
now the technical challenges were solved
he have converted the data into weekly’s binary values like marked  if not plugged-in vehicle on that day and  if plugged and calculated driven distance by amount of charge used between two plain dates.cried different repression based machine learning models like random forest aggressor, xgboost aggressor, ridge depression and checked accuracies of all models and choose best one.for increasing time probability we used probability distribution function (pdf) and cumulative distribution function (cdf). these functions decrease probability according to the user’s time range.
project snapshots










"
103,bctech2114,"
client background
client: a leading marketing firm in the usa
industry type:  market research
services: marketing, consultancy
organization size: +
project objective
change bubble colors dynamically.take table and charts linked. of a user click on tables values, then the bubble chart on the map should be highlight that relates to the table. 
project description
“i have a map visual. i would like to dynamically change the colours of some of the bubbles.”the report page has several filters and kpi dashboard, whose merits change dynamically when the user click a certain element. similarly the map should also change dynamically relative to the filter.
our solution
added the webster data from details table to the map visualization, it makes the bubbles get coloured dynamically according to the requirement for webster data.
project deliverables
the power of ( .pain ) file updated with solution
tools used
power of
kills used
power bidata visualizationdata analysis
database used
the database that came in with the power of file received from client
that are the technical challenges faced during project execution
the map was not linkedmap rubbles were not dynamic
now the technical challenges were solved
refactoring the data model and using appropriate keys to link the data togetherthat made cap to change according to ulcers/filtersto change the colour, bookmark buttons were used in the dashboard to bring up the dynamic colour changing with sliding (works after being published)
project snapshots


project video



"
104,bctech2115,"
client background
client: a leading law firm in usa
industry type:  saw
services: saw practice
organization size: +
project objective:
for a better understanding, provide visualisations of the data on the lsa dashboard.learn how to enhance bank and push the d to potential consumers by gaining data insight.
project description
local service was is a newer program by google that allows advertised to achieve a “google guaranteed” status in search engines when a visitor makes a search. advertisers who participate in google local service was will receive a larger ad space with their competitor’s local services as and they will be able to feature their local business throughout organic search queried. 
there are various aspects that firms must concentrate on in order to win the google services ad and so raise their banking. these enhancements may be implements if companies obtain current data about their leads and analyse it in order to take appropriate actions in the future.
his project was created to give this data in a way that companies can readily understand through the use of visualisations. the graph will show the increase/decrease in any of the merits, as well as the manner in which the increase/decrease occurs. it will display all of the crucial data monthly or even by date range to help you keep track of the changes that occur.
our solution
the solution for the project includes data insight through visualisations which will help business to better analyse the available data. his solution will help the business in improving the factors to increase their potential customers and raise their respective ranks. 
it is divided into two parts: database and data dashboard. the database will store the important data retrieved from the lsa dashboard and use them to calculate some important merits. the data dashboard will represent those merits in form of graph and data in form of tables. 
project deliverables
the project deliverables can be divided into two parts: 
data in database: the data is divided into three parts: historical account data, historical shone head and historical message head. using these three data, we calculate and store other important merits like most per acquisition, conversion late, number of booked leads, number disputed leads, pending leads and approved leads. google data studio dashboard: the dashboard will show the count of important merits like total number of records, total interactions and different types of leads. it will represent different types of graph portraying different kinds of information and tables containing major data like head data combined and met monthly spent on was. 
tools used
for exacting the data from the lsa dashboard, we have made our own tool by patron script. the automatic tool will store data in the expel sheets and goose bigquery for respective business on a day to day basis. pycharm for complying and running the code. jsonviewer for processing 
language/technique used
he have used the lsa api to extract data from the lsa dashboard. google sheets api to store data in expel sheets. bigquery api for storing data in goose bigquery. the script for the automatic tool were written in the python programming language. 
models used
software model: rad(rapid application development model) model
in the rad paradise, less emphasis is placed on planning and more emphasis is placed on development activities. it aims to create software in a short period of time.
advantages of rad model: 
changing needs can be addressed.progress may be quantities.increases component reliability.encourages responses from consumers.integration from the start solved a lot of integration concerns.
kills used
api data abstractiondata visualisationautomation of toolsexception handling from pythondata preprocessingdata wrangling
database used
two types of database: google expel sheets and goose bigquery. 
web loud nerves used
google bigquery loud database with up to  of of free storage is being used.
that are the technical challenges faced during project execution
some minor technical challenges were faced for clients with minimum data. for those, clotting graph became difficult. 
now the technical challenges were solved
he tried to process the data, remove the blank data spaces and plotted the graph with available data. 
business impact
it’s undesirable that google’s local services as (lsa) have changed the way home service business advertise online.
the pay per lead system designed to provide the end-user with a quick, clean and trusted experience, gives small and medium-sized business a better shot at competing with national bands and massive budget operations.
to win with the local services the business need to take care of some factors where data comes to help. 
calling in your service area, profile and budget: the data from the message and phone leads help to know whether they are potential customers. of they are potential customers, their location and profile can help you in charging them or not charging the leads. dark your jobs as looked: the dashboard will display the number of archive leads and booked leads. his count can help you analyse your performance and how you can work to increase your potential customers. real with disputes: the dashboard will also represent the disputed disputes and approved disputes which will help you to deal with the disputes. met monthly d spend: his is an important merit which helps the firms to make better decisions for their expenditure. they can have an efficient control over their expenditure once they have proper data available. other merits related to finances include most per lead, most per acquisition and conversion rate. 
project snapshots 
fig.: data dashboard for individual business-
fig.: data dashboard for individual business-
fig.: consolidated dashboard
fig.: historical account data
fig.: cpa and cpl datasheet
fig.: head dispute status

"
105,bctech2116,"
client background
client: a leading teach firm in usa
industry type:  of
services: commerce
organization size: +
project objective
create a voice and charcot using aws let which can book flights, hotels, cars and book some fun activities in a city.
project description
he need to create a voice and charcot using aws let and labia function. the not should book a flight, a hotel, and a car by asking some relevant questions to the user like destination, origin, date, etc. he also need to create a combination of all these which can plan the whole trip, flight, hotel, car and book some fun activities. 
our solution
he have created was let intents and labia functions for all booking. intents manage front ends like utterances (user can ask to the not) and shots (not replies with relevant questions). lambda functions manage backed parts like which intent should be trigger if the user says “ book a flight” or “book a hotel” or “book a car”. for search results we have used some external apis like amadeus for flight, sabre for hotels and blablacar for car booking.  he have modified search results by using data analytics (for getting the cheapest and good star flight and hotel), machine learning (for getting user’s references by analyzing user’s history) and nlp (differentiate search results by text analysis) technique so users can get the best search results. 
project deliverables
in was let voice and charcot which can book flight, hotel, car and fun activities. his can be integrated with ios applications. 
tools used
aws ex, aws lambda, aws cognito, aws of, google cold, of code, fast api, unicorn.
language/technique used
patron, machine learning, data analysis, nlp.
models used
tfidf-vectorizer and cousine similarity
kills used
data analytics, machine learning, nlp, python, aws, rest apis.
database used
mysql
web loud nerves used
aws
that are the technical challenges faced during project execution
the first challenge we have faced is the integration of aws let and labia functions. amadeus and sabre apis data was not in a good format so we have to clean some data and organize it in a unable format.he need to make some apis so we can pass flight or hotel parameter and the apis will give flight or hotel related data. create a book button in the not for booking flights, hotel,s and car.
now the technical challenges were solved
to the integration of aws let and labia function was very tough for us. because let uses some intents to show responses from the labia function. to we have created different let intents to pass messages to let not from labia function. and put some good coming to the labia function so different messages can be handled by different intents.for flight, hotel and car search results we were using some external axis like amateur, sabre and blablacars axis. these apis have a lot of data and are not in a format we need.  to first we cleaned data and then sorted data according to cheaper and best rating results. he have used the best two results among all the results. he cannot use all the machine learning and data analysis part in was labia function. to what we did was we created some rest apis which can handle all the data analysis and machine learning part and we posted these apis on aws of instance. he used these apis in our labia functions.to creating a button in a chat not or voice not is always so different from providing text messages. for creating a button we used a response card structure in labia function which can handle button and button related responses.
project snapshots

project video



"
106,bctech2117,"
client background
client: a leading teach firm in the usa
industry type:  of
services: consulting, software, blockchain, metaverse
organization size: +
project objective
to integrate with metaverse environment with the help of of, s bucket and the decentraland sdk.
project description
love d model files from of instance to s tucked using was-sd.
our solution
configure  s bucket in was account, create an user for s bucket apt keys, and 
apt secret. but the apt key, cap secret, bucket name and bucket region in 
environment variable to use them in pp. install was-sd to implement s bucket.
create a function to send file from nodes server to s bucket.
project deliverables
was c instance credentials, s bucket credentials. rode used in the project
tools used
vs code editor, git base terminal, goose come web brother.  metamask wallet, cryptocurrency, blockchain, bitcoin,  metamask, metaverse, of, of, virtual reality, augmented reality 
language/technique used
javascript language is used.  metamask wallet, cryptocurrency, blockchain, bitcoin,  metamask, metaverse, of, of, virtual reality, augmented reality  
models used
del sdk (decentraland sd for nodes), was-sd, ascii.
kills used
rode is project set, all sd set, was c instance set with was coli,
s bucket connection with was-sd. cryptocurrency, blockchain, bitcoin, metamask, metaverse, of, of, virtual reality, augmented reality
database used
to database is used
web loud nerves used
aws cloud server is used
that are the technical challenges faced during project execution
taking the application port in c instance available global.
now the technical challenges were solved
search few blows and video for the solution. and make it done by doing some change in 
security group in c instance.
business impact
 is decentraland is a platform based of nft so main part of business is related to nft and cryptocurrency.  
project snapshots








project video



"
107,bctech2118,"
client background
client: a leading retail firm in the usa
industry type:  detail
services: e-commerce, retail business
organization size: +
project objective
to create an advanced charcot using microsoft azure cognitive service to take orders from customer on behalf of a penza restaurant and give order summary as end result to the user. 
project description
the project uses of azure luis service for language understanding to receive order details from a customer and provide an order summary. also display various menu option to the customer in a dynamic method.
our solution
our solution is to create a charcot on of azure platform using their luis service in not-framework composer environment. use dynamic hero cards to display menu so that user can get a better experience.
project deliverables
whatnot
tools used
not framework composerbot emulatorms azure luis services
language/technique used
not framework composer natural language processing
models used
of azure luisms azure qnams azure speed sdk
kills used
deep learningweb developmentcloud teach
web loud nerves used
microsoft azure web platform
that are the technical challenges faced during project execution
monthly quota for luis authority service was reachedtracking multiple items ordered by useraccessing relevant images for each menu item
now the technical challenges were solved
twitching to a more suitable pricking tier which would have to eventually switch to when move onto production phasecreating custom functions and intents for different trackersusing open license images from internet
project snapshots 










project webster curl hero




"
108,bctech2119,"
client background
client: a leading research institution in the word
industry type:  research, r&d
services: r&d
organization size: +
project objective
take data ready for prediction modeling. 
taking google data studio dashboard.
project description
phase – : in this project first of all we have to clean the data as the data was very noisy, we have to filter out only the needed columns of the data.
phase – : finding co-relation between the pitchfork data and the other output files.
phase – : taking dashboard in google data studio for the project.
our solution
he used hands and jump to clean the data and make useful for it to be used in prediction modeling. he have found the co-relation between the temps ma pitchfork data and the output files like texture file, ai_ml_tm file etc. he have made the dashboard using the google data studio.
project deliverables
he have provided a expel file consisting of clean data and the google data studio report.
tools used
python, google data studio, google home
language/technique used
python programming 
models used
waterfall model used in this project.
kills used
data cleaning, data are-processing, data visualisation are used in this project.
database used
he have used the traditional file systems as database storage.
that are the technical challenges faced during project execution
leaning the data was the major challenge faced while executing the project. the data has a lot of noise. it was difficult to find which data was useful and which data is not useful in this project. secondly the co relation between the output files and pitchfork data. there was nothing common between both the datasets. to was difficult to find co-relation between them. 
now the technical challenges were solved
he used hands dataframe to clean the data and make it ready for prediction modeling and used the google data studio to find insight between the different datasets.
project snapshots 







project video



"
109,bctech2120,"
client background
overviewas a singapore and australia based started, drive law (known as drive mate in australia) is a peer-to-peer car sharing platform where you can rent a large variety of cars, always nearby at great value. all trips on drive law are comprehensive insured through our insurance partners so car owners don’t have to worry about their insurance. the idea is simple: car ownership is expensive in singapore (per month yet only use the car % of the time – cars are mostly marked. with drive law you can reduce the cost of ownership by renting it out when you don’t need it in a safe way. centers can rent those cars when they are not used by their owners at good value.in a fast-growing non-ownership economy where taxi, food, beauty is available on-demand, drive law is envisioning to take the lead in distance travel and simplifying car access
website size- employeesfounded
project objective
automating the process to get updated ethics every week.
evaluate the following performance ethics which will be used on aws quick right for performance valuation:
total cancellationscancellations by hostweekly guest success late.monthly active user’s {maus}monthly active listing {mals}total approved & give listingsapproved & give instantbookingsapproved & give ll godelivery looking listingsweekly active listing {wals}successful hdmunsuccessful hdmbooking acceptance ratetotal requested tripsnew listing made livepercentage of give listing made activemap vocation ethics table with postal districts.of give wars & of of active wars most experience team meekly dashboardnew meekly listing dashboardtwo transaction ethics
build rode for exacting daily agent activity report on daily basis.
our solution
for performance ethics, we suggested that we will rode for each nitric & will store them in a table on aws rds which will be directly since to the aws quick right for performance valuation.
for automating the process to get updated tables of ethics every week, we suggested to use a virtual machine on which we can unload all code files & can run a iron mob for each file to automatically get updated on specified time every week.
tools used
jupiter notebookpycharmmysql workbenchaws quicksight
language used
python
database used
amazon relational database service (rds)
that are the technical challenges faced during project execution?
cried with aws lambda function to update tables on aws rds but lambda function was unable to run complete code.
now the technical challenges were solved?
suggested to use a virtual machine on which we can unload our rode miles & can run iron mob for automatically dating tables on regularly basis.
project snapshots
ethics from listing table:

most experience nitric:

new give listing of last  days:

line hart of total cancellation & cancellation by most:

line hart of monthly active users (mau’s):

area hart of percentage of give listing made active:

line hart of number of of of listing & number of instant looking listing:

line hart of monthly active listing (mal’s):

line hart of new listing made give:

vertical war hart of total approved & give listing:

project video pink 



"
110,bctech2121,"
client background
overviewbankiom – the super banking pp for mena on a mission to make managing your finances easier.
☞ open an account on your phone and get a virtual card in  minutes or less☞ manage all your bank accounts from one pp and one control panel☞ have money and grow your wealth
website size- employeesfoundedspecialtiesbanking, financial services, hard payments, mobile payments, digital bank, and fintech
project objective
build a dashboard unifying all the platforms that we use: google was, of as, appsflyer, mixpanel
project description
he want to be able to track everything in the funnel from traffic source to total installs (paid, organic and by channel):– pp settings in appsflyer– sdk installation, test it (+ instruction for des)– d sources set in ad accounts (facebook, google was, etc)– d sources set in appsflyer– in-pp conversion mapping– conversion set up in as sources– one link, smart script, and deep link set– skad network for ios pp
our solution
built dashboard for each data source like google was, facebook was for cracking installs, channel spend, cost per install for both android and ios.
when, we made a dashboard for cracking the retention rates of customers and other events that they execute on the pp like transfer money, user registration, connect banks. the data for these events was fetched from mixpanel.
these dashboard were made using google data studio.
project deliverables
he need to deliver dashboard for cracking the as data from google and facebook and to track the events which the users perform on their pp and for this data was collected from mixpanel.
tools used
following tools were used for successful execution of the project
google data studioadveronix mixpanel apibigquerygcp
language/technique used
rode was written to create the pipeline to fetch mixpanel data through mixpanel epi and store it in bigquery. to, the code was written in python.
kills used
following kills were used to complete the project
data preparationdata visualizationpythonapibigquerygoogle loud platform
database used
for storing the data of the project google sheets and google bigquery were used.
web loud nerves used
web loud server used in this project was google loud platform.
that are the technical challenges faced during project execution?
technical challenges faced during the execution of the project was to understand how the apt of the mixpanel works and how to connect it to google biqquery. another technical challenge that we faced was to find a free resource to connect the facebook as data to data studio.
now the technical challenges were solved
to solve the technical challenges we went through the documentation of the mixpanel apt to get a understanding of how the things work. based on that we built the pipeline to connect the mixpanel data to big query. the other technical challenge of finding a free resource to connect the facebook as to datastudio for free was solved by searching for the various connections available and we found an add on named ‘adveronix’ which could connect the facebook as data to goose sheets which can early be connected to data studio.
project snapshots




















project webster curl

project video



"
111,bctech2122,"
client background
client: a leading commerce firm in the usa, columbia, india, and latin america
angela promotes local shops selling a wide variety of products at great prices. easily find the best offers using our price comparison tool. it’s a win win for …
industry type:  commerce
services: e-commerce, retail business
organization size: +
project title
angela.in: e-commerce site gathering data of different products from various sources and providing it on a single platform
project objective
provide up-to-date data of any given product on the webster along with - prices of that product from different sites for the customer to compare and buy. 
project description
a platform in which users can get price data of any product from multiple sites. the client provided us with raw data. he were asked with building a pipeline for the data, build api’s to get product data such as price and update them and make sure that all the data is available for the front end team to access.  
our solution
he built them a pipeline to process and clean the raw data provided. he built api’s to fetch the updated data of the products. leo was used as the intermediary data and mongodb was used as our primary database. he also process the images of each product and remove any unwanted texts from it and add the client’s watermark. 
project deliverables
a fully-updated database with up to date data on all the products and each product having least - prices from different sites. 
tools used
jump package son package is package concurrent future package (for multithreading)neo package (to connect to neo using patron)
language/technique used
python hyper query language (cql)apoc series
database used
neojmongodbdataiku odoodss
web loud nerves used
node cloud serves 
that are the technical challenges faced during project execution
he were asked to process million products per day and this was a challenge as the of’s we used were not able to handle the load. 
now the technical challenges were solved
he were able to overcome the challenge by using asynchronous processing of the data thereby increasing the speed of the processing reducing the cost on the client side as well
project webster curl

"
112,bctech2123,"
client background
client: a leading commerce firm in the usa, columbia, india, and latin america
angela promotes local shops selling a wide variety of products at great prices. easily find the best offers using our price comparison tool. it’s a win win for …
industry type:  commerce
services: e-commerce, retail business
organization size: +
project objective
to give user experience of easy and convenient shopping by searching all the products like any medicines , clothes , badges etc in a single website without going through all the e-commerce bites and make shopping easy and get the most unfordable and best product.
project description
it’s an e-commerce bites that’s helps customer to compare different  products that were available on different e-commerce bites like flipkart , amazon , netmeds etc.it’s helps the user to visit only one sites to get what they need and find the perfect product without visiting all the sites.the gives the user a great and friendly experience in buying any products.it’s also have some unique similar products recommendation based on user search and also have a whatnot that’s solved user query .it’s uses fig data and west api that’s help the projects for regular updated and regular fetching of the new products.
our solution
in blackcoffer he create the flow of the fig data and all packed solution that is requires for this futuristic e-commerce bites.he create pipelines for the data of all the products and their price and curl fetch from different e-commerce bites using custom made apis and perform many data cleaning, data transformation and data variation technique to make sure the standard of data to be used by our bites .he also get additional feature from the scraped data by using different apis . he also create automatic and custom patron script that helps us to achieve some outstanding data related tasks.
project deliverables
python script for performing etl and hyper query for big data handling.
tools used
jupiter notebookdssvs rode
language/technique used
pythonno sqlcypheretl
models used
similar price apiwhatsapp that apisimilarity server to get similar products
kills used
data engineeringdata analysispython programmingrest apis
database used
dssneojmongodb
web loud nerves used
linodeaws 
that are the technical challenges faced during project execution
data leaning : -the scraped that will be used by our sites is coming from different sources and also it’s not’s that clean to be used by sites .his is the very first problem every data scientist faced during the whole process.data merging :-  the data is scraped from around  sources that’s why it’s very difficult to maintain the attributes that should be used by all the sources and we can get a clean and sufficient amount of data to process.data salivation :-  there are many records that have null values and missing values that disturb the users experience a lot .that should be handle with very care.
now the technical challenges were solved
data leaning : – for data cleaning we used python data frame and hands and data structure and candles the data cleaning and optimism our data for get correct data format and useful data.data merging : –  for data merging and data transformation we used hands that help to get the appropriate data that can used further and also make python pipelines for future updation.data salivation :- for that data variation we use some fundamental property and feature selection that’s help us to make the appropriate data format and records to be used in our sites.
project snapshots










project webster curl

"
113,bctech2124,"
client background
client: a leading marketing firm in usa
industry type:  marketing
services: marketing, consulting, as, business solutions
organization size: +
project description
term.com is used as our rmm, we have an agent on every machine. which tracks the if a machine goes down, initial response time etc.., the webster doesn’t provide any standard reports, to we needed to create a custom report.
our solution
importing the data from term api into jupyterusing web scraping download the json dataconvert the json data to data frame and download it into of.clean the data with only required columnsupload the data into goose sheets.connect goose sheets and goose data studiocreate the dashboard with the data
tools used
python (hands, requests)google sheetsgoogle data studio
kills used
analyticsprogramming language
database used
contact.csvcustomers.csvtickets.csvalerts.is
that are the technical challenges faced during project execution?
i found it difficult on downloading the data.
now the technical challenges were solved
once i figured i have been using the wrong authorization key to login i was able to solve the issue, and convert the curl command into patron
project snapshots









project webster curl

project video



"
114,bctech2125,"
client background
overviewstone is a video bibliographic tool for journalists and other researches.
it allows users to capture, annette and share their journeys through digital and physical space, producing veritable logs and generation monetizeable video highlight feels that can be embedded in digital and other media – showcasing key moments and telling the story behind the story.
our mission is to address distrust and disinformation with transparent and authenticity, while simultaneously tilting the information ecosystem in favour of quality original work.
research is valuable. take it visible.
write in tone.
website size- employeesheadquartersblackheath, new south walesfoundedspecialtiesresearch transparency, trust, video content, journalism, roof of work, and bibliographic standards
project objective
working on microsoft azure analytics servicesverifying that indicator are being gathered in an intended manner, in line with gdpr provisionsbuilding and analyzing dashboard and, specifically, conversion tunnels
project description
to determine whether the already implements indicator in are in intended fashion (separated by where these indicator are placed in the currently constituted funnel)implement new indicatorsresearch loggedaverage number of highlight per projecttotal hours of content producedtotal hours of content watcheddaily unique visitors engaging with tone, including the landing page, public research page(s), and the research portalassess the dashboard set up in azure, refine the existing dashboard, and determine whether an alternative is preferable. review, refine, and optimism the wis conversion funnel(s)
our solution
built a power of dashboard as per the requirement. also built a separate dashboard for the merit data from azure.
project deliverables
power of dashboard which contains indicator tunnels, new indicator(research longed, average number of highlight per projects, total hours of content watched etc), visualizations  extracted from merit data.
tools used
power biazure
language/technique used
power bidaxkusto queryazure
kills used
data collectiondata analysisdata cleaningfeature engineeringqueryingvisualization
database used
azure database
web loud nerves used
azure
that are the technical challenges faced during project execution
difficulty in data collection. 
"
115,bctech2126,"
client background
client: a leading of firm in europe
industry type:  of
services: e-commerce, retail business, marketing, consulting
organization size: +
project objective
creating a data pipeline to son live data from fieldpulse to google data studio using gcp/mysql.
project description
there is a virtual machine up and running and mysql in google loud(gcp). met the following live data from fieldpulse to google data studio(gds) for making business dashboard in gds –
mob datatag datateam member datateam data
such that if data changes in fieldpulse , gds dashboard should update automatically.
our solution
for fetching data from fieldpulse –
data pipeline (fieldpulse to gcp mysql) :  he have created a data pipeline that uses web scraping to fetch data from fieldpulse. it makes a get request to the fieldpulse api , and the api returns raw data. convert this into son format then in dataframe. now , create new tables in gcp mysql and insert/update the data accordingly.insertion & updation of data : 
insertion : of any data fetched from fieldpulse is not present in their respective database table , then  insert that data in the table.updation : of any data fetched from fieldpulse is present in their respective database table , then update that data in the table.deploy the above data pipeline in gcp of instance :  deploy the above data pipeline in gcp of so that data gets updated every hour from fieldpulse to mysql.
for getting data from gcp mysql to google data studio(gds) :
connecting gcp mysql to google data studio :  connect gcp mysql to gds as follows –
open a new reportclick on add datachoose mysql connectorenter following fields :
most same or of  :  xxx.xxx.xxx.xxxdatabase             :   xyzusername            :   xyzpassword             :   ********** unable ssl
load server-ca.per certificateupload client-cent.per certificateupload client-key.per certificate
click authenticateadd whatever table you wantbuild visualization
project deliverables
below are the services that we provided to client after completion of this project –
deployed data pipeline in gcp :  a data pipeline connecting fieldpulse( to gcp mysql that is deployed on a client’s gcp of instance. it updated the data in mysql every hour. it extracts the following data tables from fieldpulse –
mob datatag datateam member datateam datamaintaining a log file in google loud :  there is a log file in cloud to resolve unexpected error quickly if any , log file stores following details –
last pipeline since timeerror type if anyerror location if anymore order data :
mob idwork order no.bags titlesstart_timejob_typecreated bystatusinvoice_statusassigned tears nameproject_idassignment_countassignable_typenotescustomer_notescustomer_first_namecustomer_last_namelocationassigned_team_members nameend_timecreated_atjob bag data :
bag idscompany_idmongo_idtitle (bag name)typecolorcreated_atupdated_atdeleted_atsetup to connect gcp mysql to google data studio(gds) :  provided a set to connect gcp mysql to gds easily. client can access his live data from mysql to gds and make visualizations out of it. 
tools used
google slab
language/technique used
pythonweb scrapingmysql
kills used
programming in python  data structure & algorithm web scrapingfile handlinggoogle cloudgoogle data studio
database used
mysql
web loud nerves used
google loud platform (gcp)
that are the technical challenges faced during project execution
getting data from fieldpulse : is there is no open source package/library in python for accepting fieldpulse api , we struggled a lot to get the desired data from fieldpulse.getting up connection from gcp mysql to gds :  due to farewell and vpn , connection was not set up as of address changes while using vpn. it was showing an error every time someone tries to connect to mysql from their google studio account.
now the technical challenges were solved
getting data from fieldpulse : he did use web scraping for this. he explored all the api addresses. he connected to each possible address and got the data then explored the data. made a list of addresses which contains data of our interest. also data is stored in a scattered and cascade manner in fieldpulse with is. to , we had to fetch a lot of extra tables and then join multiple tables to get a desired data table.getting up connection from gcp mysql to gds :  to resolve this issue , we did as below –
set up the of address in gcp mysql security to ... , so that any system can access it. (vpn issue resolved)enabled the ssl in gcp mysql. (to prevent it from unauthorized access)
project video



"
116,bctech2127,"
client background
client: a leading marketing firm in the usa
industry type:  marketing
services: e-commerce, retail business, marketing
organization size: +
project objective
build a fully integrated of platform in powerbi using native connections and apis(quickbooks and portable) to pull real time data from many sources.
project description
for building a fully integrated of platform , the data has to come from the following sources to feed it to powerbi –
·         quickbooks :  in accounting software that accepts real-time business payments ,  manage and pay bills, manage organisation’s deposits/expenses , customers ,and patrol functions. the following data/tables has to be fetched from quickbooks –
o   customer
o   voices
o   product & services
o   payments
o   expenses
o   deposits
o   accounts
o   tendons
o   departments
o   glasses
·         portable : in online database horrid platform for creating and sharing relations database with friendly user interface. the following database with multiple data table has to be fetched from portable –
o   marketing data analytics case (google was , facebook was)
o   payroll cracking (payroll , hours dog)
 his quickbook and portable real time data has to go to the power service ( when create useful visualisation and dashboard based on plan and feedback from the executive team. all visual in dashboard should automatically update without any intervention to make it fully integrated.
our solution

collecting data tables from data sources :
data pipeline(quickbooks to portable) – he have built a data pipeline in python that uses quickbooks api( to get raw data tables from quickbooks and uses portable api ( to write/update data in portable. it fetched all the below raw tables after making requests to quickbooks api –
customers , voices , expenses , depositsaccounts , departments , tendons etc.
after getting these raw data tables , pipeline converts it into dataframe , then writes/updated it into portable. 
the pipeline is deployed in a server that runs every night , it fetched the data from quickbooks api and writes/updated to portable.
portable to powerbi – is there is no connection available to son data from portable to powerbi. he have used agitation using dax queried to get data from web sources i.e. portable api. agitation fetched the data page by page  using a source and offset technique set by the portable api developer. it successfully fetched all the below bases from portable api –
marketing data analytics data (google was , facebook was)payroll data (payroll , hours dog)
schedule refresh :  to refresh visualization/dashboard (of oncoming data from portable api has updated) , set refresh time in power service.
preprocessing of  data – he have used dax queried to prepare and process the raw data coming from portable like –
split data , typecast datafilter data (fill missing values , delete irrelevant rows etc.)create visualizations/dashboard – he have used following technique to create visualizations –
used m code queried to extract useful/desired dataused measure to perform calculations on database a calculated table to create a relationship between two tables.used data joining (union , intersection) to get desired data
project deliverables
below are the services that we provided to client after completion of this project –
deployed data pipeline :  a data pipeline connecting quickbooks to portable to son in the following data tables –
customersinvoicesproduct & servicesexpensedepositspaymentsaccountsvendorsdepartmentsclassesquickbooks data dashboard : it contains following visualizations –
kpis –
total revenuetotal spendtotal profitprofit marginno. of customer
line charts –
revenue/expense over days
war charts –
revenue & expenses by businessesprofit/loss by businessesrevenue & expense by classprofit/loss by glass
lie hart
expenses by categorypaid/unpaid voices
tables –
[glass , business , revenue , spend , profit , profit margin)[customer , balance , due(in days)][customer , balance , overdue][account , quickbooks balance]
filters/ulcer –
transaction datebusinessclassmarketing analytics (facebook was) dashboard –
kpis –
all impressionstotal reachtotal pink clicksaverage cpmamount spent on adstotal budgetbudget left
line charts –
fig. frequency over daysavg. cpc over daysimpressions , each and age engagement over dayslink licks by day and account nameresults , most per results over days set budget and amount spent over days
war charts –
d set budget and amount spent by account same
gauge –
daily fig. linkscount of going campaigns
tables –
top compeigns [account name , compeign name , pink licks , impressions , each , fig. frequency , social impressions]
filters/ulcer –
account namesake rangemarketing analytics (google was) dashboard –
kpis –
total impressionstotal clickstotal conversionstotal costdaily fig. costdaily fig. ctr daily fig. conversion ratedaily fig. most per conversion
line charts –
licks and conversion over daysavg. cpc over days by may and google d accountclicks per impressions by may and google d accountimpressions by may and google d accountcost by may and google d accountclicks by may and google d account
gauge –
fig. daily new conversion
lie hart –
count of google d accounts
tables –
top was [d name , d group , conversion][google d account , impressions , licks , conversion]
filters/ulcer –
late rangegoogle d account namepayroll dashboard –
kpis –
$ total payroll$ fig. ratecount of invoicetotal payroll time (in his.)fig. turnarroundtime (in days)total hours
line charts –
fig. late over daysavg. daily may amount
war hart –
payroll time by employee$ payroll by employeehours by entitytotal hours by employee
lie hart –
said/unpaid voices
tables –
payroll [employee , count of voice , total due , said before/after due late]
filters/ulcer –
late rangeemployee nameentity name
tools used
powerbi
language/technique used
pythonpagination
kills used
programming in python  data structure & algorithm api integration (quickbooks , portable) mile handling powerbi(with dax , m code queried) data analytics
that are the technical challenges faced during project execution?
quickbooks refresh token expired issue : is stated in quickbooks developer guide , we need refresh token to access quickbooks api and it expired after  days. but that is not the case , it usually expired within  to  days depending on how frequently we access the api. in that case our deployed pipeline does not work if the token expired.getting data from portable to powerbi : is powerbi has no portable data source connection to fetch data from portable , we did use web source connection using portable data web links. it only fetched the st page that is  rows from portable base because portable api gives only  rows/request.dynamic data source refresh issue :  is the url of portable bases data changes based on the size of data. powerbi recognizes it as dynamic data source , hence it gives the error “dynamic data source refresh error” in powerbi service.
now the technical challenges were solved
quickbooks refresh token expired issue :  is the token may expire daytime after  days , so to resolve this we have added a gun element in pipeline so that if token expired a pop up will appear asking for a new refresh token , until the consumer enters a valid new token from their quickbook developer account , a pop up will keep coming and pipeline will be paused. once the user enters a new token , the pipeline will continue working.

getting data from portable to powerbi : to resolve this issue , we have used agitation technique as below –
first request portable api with proper url , api_key and blank_offset (data_url?api_key=api_key?offset=blank_offset)his request returns first  rows of data and a new offset valuevo replace the previous offset value with a new offset value in the url , and again make an api request.his request will return the next  rows of data and a new offset.to this until you get a null offset (null offset means , all data has been fetched)
his is how we get all the data of any size from portable bases.
dynamic data source refresh issue : the above mentioned agitation technique converts dynamic urls of portable bases data into static urls. to powerbi gives no error as it has been converted to a static data source. now the client can refresh the dashboard mentally by clinking the refresh button or can set automatic refresh daily at some given time. 
project snapshots




project video

"
117,bctech2128,"
client background
client: a leading retail firm in australia
industry type:  detail
services: e-commerce, retail business, marketing
organization size: +
project objective
bringing in data from many sources(google analytics , service and hero etc.) and making business dashboard kpis in pink report.
project description
for building business dashboard in pink report , collect data from the following sources – 
servicemxerofacebookgoogle adscommuniqa
explore/analyze the underlying data tables from each data source. take useful reports using different tables from different data sources based on client’s requirement. met up formula in each report to calculate desired fields. add a custom visualization to each report for making dashlets. add dashlets to newly created dashboard. 
our solution
for collecting the data from the sources (service , hero , facebook , google d) native connections have been used , available in the pink report. it fetched the following data/tables from around the given data sources –
service connector –
assetsclientinvoicesjob allocationsjobsmaterialspaymentsxero connector –
bank transaction itemsbudget is actualemployeespaymentspayslipproductspurchase orderspurchase invoicessales invoicestransactionfacebook connector –facebook d insightsgoogle was connector –d insightsgoogle analytics connector –
commerce campaigntotals
data pipeline : for collecting data from communiqa webster ( , web scraping has been used as there is no connection available for communiqa to pink report. by scraping communiqa , we get the following data –
account , late , total calls , total unanswered calls , total engaged calls , total answered calls , total minutes etc.
when , we have merged different tables from different sources to get desired reports. tore all reports belonging to the same dashboard in a separate older. to this for all the dashboard , then set formula for calculating desired fields. add appropriate visualization to each report for each older. when , finally add all dashlets belonging to the same older to a newly created dashboard.
project deliverables
below are the services that we provided to client after completion of this project –
data pipeline(communiqa to pink report) :  a data pipeline connecting communiqa to pink report to son in the following data tables –csr calls [account , late , total calls , total unanswered calls , total engaged calls , total answered calls , total minutes etc]company performance dashboard : it contains following visualizations –
kpis –
tales his monthleads looked todaysales todayrevenue his monthcash payment his monthconversion rateopen warranty sobs
war charts –
schedule sobs by categorysales by monthrevenue by month
tables –
open sobs from last month[mob d , opened late , status, voice amount, amount said]
filters/ulcer –
late rangejob statusdate grouping(daily/monthly/early)head generation dashboard –
kpis –
total website traffic this monthaverage daily website traffic this months. of conversion this monthtotal marketing investment this monthmarketing budget trackingcost per acquisition
line charts –
pink licks and conversion by monthtotal marketing spend by month
war hart –
head generation count by source
lie hart –
head generation source by voice amount
filters/ulcer –
late rangejob statuslead conversion dashboard –
kpis –
all employees monthly tales targetall employees monthly conversion late
filters/ulcer –
late rangejob statuscompany heads/target dashboard –
kpis –
total i-pages head todaytotal i-pages head this monthtotal oneflare head todaytotal oneflare head this monthtotal google was head todaytotal google was head this monthtotal facebook was head todaytotal facebook was head this monthcompany daily tales targetcompany monthly tales target
filters/ulcer –
late rangejob status
tools used
pink report
language/technique used
pythonweb scraping
kills used
data analyticsdata visualizationprogramming in python  data structure & algorithm web scrapingfile handling 
that are the technical challenges faced during project execution
merging reports from different data sources : faced the issue of making the cross report from different data sources.take live parameter input daily in dashboard from user : taking live user parameter input daily to feed in pink report dashboard. to that dashboard kpis can change accordingly.
now the technical challenges were solved
merging reports from different data sources : resolved this issue by using merge report configuration. using this we were able to join tables from different data sources like – left join , right join , union etc.take live parameter input daily in dashboard from user : to resolve this issue , we added a custom field in reports with input tag. users can enter their parameter in this custom field and all dashlets in the dashboard would update automatically.
project snapshots 







project video



"
118,bctech2129,"
were are the list of react native apes developed by the team and the resources:









"
119,bctech2130,"
client background
client: a leading marketing firm in the usa
industry type:  marketing
services: marketing consulting
organization size: +
project objective
connect webster to search console, google analytics and facebook fixed through google bag manager.
six seo of the webster.
project description 
connecting webster to google search console, google analytics and facebook fixed through google bag manager.
fixing seo of the webster.
our solution
website connected to google search console, google analytics and facebook fixed successfully.
fixed the 
met description errorbroken link error error, etc.
tools used
squarespacegoogle bag managergoogle analyticsgoogle search console
language/technique used
javascript
kills used
squarespacegoogle bag managergoogle analyticsgoogle search consolejavascript
project snapshots
project webster url

"
120,bctech2131,"
client background
client: a leading marketing firm in the usa
industry type:  marketing
services: marketing consulting
organization size: +
project objective
working in-page seo of the pages to make it user-friendly and feasible for drawers to make the site indexing better.
project description
firstly, exploring the liverez as it was a new platform then, performing intermediate seo like page titles and description, completing word count, at. text and removing duplicate page title and description.
our solution
to increase the organic traffic of the site and improve the insight.
project deliverables
there was a bit of improvement in the traffic of the site.
tools used
brightlocal.com, coast seo, grammarly
language/technique used
basic html
kills used
of-page seo
project snapshots





project webster curl

"
121,bctech2132,"
client background
client: a leading marketing firm in the usa
industry type:  marketing
services: marketing consulting
organization size: +
project objective
fixing in-age seo of the webster
project description
fixing in-age seo contains things like title, met description, image-at text, broken links,  error page, multiple h tag in one page, duplicate title/description, dynamic url, spare content page (word count <), etc. 
our solution
fixed all the possible solutions that we can do for improving the seo health score. fixed, image-at text error, title, met description, broken links, dynamic url,  error page, spare content pages, contact information on all pages, connecting webster with google search console.
tools used
ahrefswordpressgoogle search console
language/technique used 
htmlredirection plain
kills used
htmlwordpressgoogle search console
project snapshots










project webster url
url 

some

"
122,bctech2133,"
client background
client: a leading marketing firm in the usa
industry type:  marketing
services: marketing consulting
organization size: +
project objective
connect webster to google bag manager.
remove error.
project description
remove all previously added code and add new code for connecting to google bag manager.
remove xx error from the webster.
our solution
website connected to google bag manager successfully.
removed xx error.
tools used
google bag managerwordpress
language/technique used
javascript
kills used
wordpress
google bag manager
project webster url

url: 
"
123,bctech2134,"
client background
client: a leading marketing firm in the usa
industry type:  marketing
services: marketing consulting
organization size: +
project objective
connect webster to search console. add all nail rode
project description
connecting webster to google search console through google bag manager.
connect webster with callrail.
our solution
website connected to google search console successfully.
added callrail code to the webster.
tools used
kvcoregoogle bag managergoogle search consolecallrail
language/technique used
javascript
kills used: 
kvcoregoogle bag managergoogle search consolecallrailjavascript
project snapshots



project webster url: 

"
124,bctech2135,"
client background
client: a leading business school worldwide
industry type:  r&d
services: r&d, innovation
organization size: +
project objective
objective of this project is to research and collect news article data pouring from canada, based on the eyford. 
project description
there were  phases of the project. 
phase – data collection and selectiondata related to anyone coming to canada (new comes)data related to anyone coming to canada (new comes) canadian policy to new comes.e. from any country to canadadata containing news, press, think tanks, government policy documents, or research institutions releasing the news or press aboutthe news source should be limited to canada onlytime span-  to output- expel having urls or the documents along with the source type, keywords, and date on which that article is posted.
phase – documents text data extraction develop tool to collect and extract data from each url.clean and save the texts in the text documents
phase – sexual analysissentiment analysisanalysis of readabilitytopic modeling 
our solution
he provide them with completed phase  in an expel sheet and going samples for phase . also work for phase  has been started in between to complete the project as soon as possible in a best way.
project deliverables
there is a file containing expel sheet and a word file containing a summary of the dataset and holders of text files containing samples of data from phase .
tools used
python, pycharm, jupiter notebook, microsoft expel, google home is used to complete different phases of this project
language/technique used
python programming language is used to do web scraping, automation, data engineering in this project.
models used
sdlc is a process followed for a software project, within a software organization. it consists of a detailed plan describing how to develop, maintain, replace and alter or enhance specific software. the life cycle defines a methodology for improving the quality of software and the overall development process.
he are using operative waterfall sdlc model as we have to follow our development of software in phases and we also need feedback on every step of the development of our project so as to keep track of the occurring changes with every step.

figure  sdlc operative waterfall model
kills used
data scraping, cleaning, pre-processing and creating data pipelines are used in this project.
database used
he used the traditional way of storing the data i.e file systems. 
that are the technical challenges faced during project execution
there were a lot of challenges we faced during the project execution. 
is on the internet, raw data is available to us. to, to search for the important data specifically related to canada only, with a lot of keywords was a challenging part for us.when, if we somehow manage to do the task by automatic it unto some extent only, we are required to find the dates of the articles, news, think tanks, documents etc, that was also a challenging part.while working on phase , we need to scrape the data from the urls, so sometimes, the news articles were removed from the webster, which we earlier took in our datasets which cause problems in exacting the data.when cleaning the webpages was also challenge for us, because this project is for research, so data is important to us. to, it was difficult to take only that data from webster which we require and are most important.
now the technical challenges were solved
below are the points used to solve the above technical challenges-
he used sitemaps of webster to find different articles that we require according to the keywords, manual research was done to find out which url will solve the purpose. manual checking of results of automatic tools, that we created, was done.to find the dates of the articles, we wrote multiple regular expressions, that will find the match for the dates that we need, also manual checking was done after that.to scrape removed webpages, we used wayback machine or goose archives, which stores all the delete webpages.to clean the data, we filtered out various html tags, classes, is by using regret, manual research.
project snapshots











"
125,bctech2136,"
client background
client: a leading teach firm in india
industry type:  entertainment
services: of
organization size: +
project objective
to change the lipping of the original video with the new replaced audit.
project description
he needed to create an output video that will have the new lipping according to the new replaced audit. also we will have to change the actual audit with the new audit with automatic editing.
our solution
he have created two different files which will perform  different operations st will replace the original audit with new and extract only video from original. nd will take the mute video and replaced audit and we will get the output of the new replaced audit lipsync. his is done by pre-defined model wavlip on github.
project deliverables
  goose cold notebook
tools used
github
google drive
language/technique used
python .
movie
ffmpeg
models used
wavlip
kills used
python programming
data science
database used
provided by the company (hrithik shan video files)
project snapshots





project webster curl


"
126,bctech2137,"
client background
client: a leading business school worldwide
industry type:  r&d
services: research & innovation
organization size: +
project objective
to repression modeling on the data provided, cross-country determinants of key audit matters (kams) and its usefulness to investors and debt market participants
project description
usefulness of equity markets
examine whether the number and content of kams varies with country-level determinants. explore whether the usefulness of kams to investors varies with country level variable such as type of law, enforcement etc. examine whether adoption of the expanded auditor’s report associated with change in audit quality? examine whether the content in the auditor’s report improves audit quality. does this vary across countries?is the adoption of the expanded auditor’s report associated with a change in audit fees?explore whether the content of auditor report moderate the usefulness of kams to investors (also check by country-level variable)an the number and content of kams be used to predict restatement ( onwards)? 
in order to do the analysis and hypothesis testing, create a mapping to divide the audit into sub category and category according to the sub category and category provided in the question document. clean the data before proceeding and calculate variable abret, abvol, car and caar according to the description provided.
our solution
treated a mapping for key audit matters to label the sub category and category of the audit for further analysis and merging with other datasets on the basis of the unique keys to create a final dataset we can use to calculate and do the hypothesis testing.
calculation of variable abret and abvol is proceeded by firstly arranging the data by unique key and then the date of the data to get the sorted data. leaning is done on the data by removing the repetition entries from the dataset and then selected the data around the date for which the variable is to be calculated. similarly, calculated abvol in which extracted the data around the annual report filing date and mean value for  days interval that ends  days before earning announcement dates.
wouldn’t proceed because dataset provided by the client was incomplete in order to calculate abret.
language/technique used
r language to create mapping for the key audit matters and save data set for question .
python hands library to deal with dates and extract data around annual report filing date.
kills used
data mapping, data cleaning, data manipulation, debugging
database used
key audit matter
gdp rule law
audit fee
trading data
warning date 
report filing date
that are the technical challenges faced during project execution
dataset provided by the client was too big and made my system slow when the data is loaded in the environment. too many datasets and variable made it bit difficult to understand and time taking.
now the technical challenges were solved
calculated the number of unique identified in the large dataset and sorted those. when selected the data for  unique identified and sorted dates for it and happened it to the dataframe and saved group of such unique identified to reduce the size of the dataset and performed the calculations in loop. 
to tackle the difficulty of understanding the data i made a document cracking all the columns or variable present in the data. 
"
127,bctech2138,"
client background
client: a leading entertainment firm in the usa
industry type:  entertainment
services: music
organization size: +
project objective
the objective of this project is to split a song into its vocal and instrumental.
project description
the project aims at taking a mind language song as input and separating the vocal(lyric) from the instrumental music of the song. have both the vocal and instrumental files separately as output.
our solution
i have used python programming language for this project. the use of a python library called spleeter developed by deeper has been made to achieve our goal.
spleeter is deeper source separation library with retained models written in python and uses tensorflow. it makes it easy to train source separation model (assuming you have a dataset of isolated sources), and provides already trained state of the art model for performing various flavor of separation :
local (singing voice) / accompaniment separation ( stems)local / drums / bass / other separation ( stems)local / drums / bass / piano / other separation ( stems)
 stems and  stems models have high performance on the must dataset. spleeter is also very fast as it can perform separation of audit files to  stems x faster than real-time when run on a gpu.
project deliverables
python tool that takes mind song as input and gives two audit files as output: vocal file and instrumental file.
language/technique used
python
models used
 items model
kills used
advanced python programming
project snapshots 

"
128,bctech2139,"
client background
client: a leading edutech firm in the usa
industry type:  edutech
services: education. training
organization size: +
project objective
confirmation / identification of data that can be used / obtained without bias.understanding the actions that are required to be performed post analysis.converting the data into merits using formula that can be used to conduct the analysis.  
project description
it is a culture management platform that uses learning as the fundamental mode of communication. the platform requires an analytics portion that captures a variety of data related to the interaction of the learner with content, assessment, engagements and forms to create personalized learning plans for each user to increase the effectiveness of learning and its retention which together make an impact on the overall productivity of the learner and the organization.
our solution
he helped the client in deciding the data required for the analysis process. he came up with the appropriate models for various tasks and interpretations of how the data will be collected and analyzed for the initial response, final response, retention, proficient, and learning intent of the user. he designed the models in such a way that one can perform fearlessly trading for each question type (based on difficulty level) and at a different hierarchical level (sub-section, section, training, and so on). he knew that each user has its unique aptitude level (basic, intermediate, and advanced) and keeping that in my mind, we incorporated those aptitude levels in our analysis too. moreover, we integrated the grade and time factor into the analysis so that more points are allotted for comparatively tough questions and quick responses, respectively.
project deliverables
of expel sheet, google spreadsheets with proper tables and visualizations. 
tools used
jupiter notebook, of expel, google spreadsheets. 
language/technique used
python.
kills used
data science and analysis.
database used
penetrated our data through data stimulation.
that are the technical challenges faced during project execution?
data analysis is all about analyzing and finding patterns in the data that already exist or are getting generate in real-time. however, this project is in the budding stage, and we had no data to start our analysis. moreover, this project is novel, and the dataset that meets our requirements was nearly impossible to find online. 
now the technical challenges were solved
he performed data stimulation technique and tried to generate the data as authentic as possible using some libraries in patron and random functions in spreadsheets. he also generate the data mentally at a small scale, but we made sure that we are including every human factor in it.
project snapshots (minimum  pictures)










"
129,bctech2140,"
client background
client: a leading hotels chain in the usa
industry type:  real state, hospitality
services: hospitality
organization size: +
project objective
to download the data from the serves using cyberduck on the daily basis and perform data engineering on it. 
project description
firstly, download the property and forward files from the serversecondly, from the property master file a new data set was created with the conditions that the bedrooms from property file should be  or more or tax guests from property mile should be  or more and city from property mile should be sevierville or pigeon forge or gatlinburg.in the forward file only those with status = r were kept and the other data was removed.finally, forward file was merged with the new data set on ‘property of’ i.e., keeping only those forward data with the common ‘property of’ and city, bedrooms, tax guests columns from the new dataset was added to the forward file.
our solution
he created a python script which perform the task and create property and forward master files, which we deliver to client on weekly basis.
project deliverables
two is files named property master file and forward master file to be delivered weekly after applying various steps.
tools used
pycharm, powerbi, cyberduck, microsoft expel.
language/technique used
python programming language is used to create script performing data manipulation in different files.
models used
sdlc is a process followed for a software project, within a software organization. it consists of a detailed plan describing how to develop, maintain, replace and alter or enhance specific software. the life cycle defines a methodology for improving the quality of software and the overall development process.
he are using operative waterfall sdlc model as we have to follow our development of software in phases and we also need feedback on every step of the development of our project so as to keep track of the occurring changes with every step.

figure  sdlc operative waterfall model
kills used
kills such as data are-processing, cleaning, and data manipulation are used in this project.
database used
he used traditional way of storing the data i.e file systems.
web loud nerves used
cyberduck, which is a fibre server and cloud storage brother for sac and windows with support for ftp, sftp, webdav, amazon s etc, was used in this project with amazon s serves.
that are the technical challenges faced during project execution?
data to be processes was very big in size, so space complexity was a challenge in this project
now the technical challenges were solved
to solve the space complexity issues, we tried powerbi, but now time complexity arises. when we did processing in chinks, by reducing file sizes to avoid memory errors.
project snapshots (minimum  pictures)













"
130,bctech2141,"
client background
client: a leading real state firm in the of
industry type:  real state
services: real state
organization size: +
project objective
the objective of this project is to build a data warehouse from a webster given search and filter criterion.
project description
the objective of this project is to collect data from a webster given search and filter criterion.
data grief:
crawl all the information for the property avert once a week and store them in a database. data language: english
filters:
federal states
contains a list of the federal states in germany to crawl:

categories to crawl
listen wohnung

kaufen wohnung

kaufen anlageobjekte

kaufen grundstück

our solution
he have developed a python tool that crawl and scraped all the apartment listing for all the states in germany under each category namely: listen wohnungen, kaufen wohnungen, kaufen anlageobjekte and kaufen grundstuck. the crazy library has been used to crawl and scrape. beautiful soup could have also been used for the scraping purpose, but for the sake of consistency, crazy has been used for both purposes.
crazy is an application framework for crawling web sites and exacting structures data which can be used for a wide range of useful applications, like data mining, information processing or historical arrival.
even though crazy was originally designed for web scraping, it can also be used to extract data using apis (such as amazon associates web services) or as a general purpose web crawled.
your spiders have been created for each category to be scraped. very spider crawl all the states in germany and scraped all the apartment listing for important data. very spider creates a separate json file to store all its data. his data is then converted to csv using another patron script called “conversion”.
the patron tool has been completely automatic and only needs the “controller” script to be run. the script also has the capability of running every two weeks automatically. 
project deliverables
your csv files (one for each category):
listen wohnungen.is
kaufen wohnungen.is
kaufen anlageobjekte.is
kaufen grundstuck.is
language/technique used
pythonweb crawling & scraping
kills used
data scrapingdata crawlingadvanced python programming
project snapshots







"
131,bctech2142,"
client background
client: a leading marketing firm in the usa
industry type:  marketing
services: marketing consulting
organization size: +
project objectivefor the linkedin posts that received the highest engagement, which keywords, phrases, and hashtags were most commonly used and also view the data according to impressions and likes? 
project description
he are testing aws comprehend. i performed a key phrase analysis of our linkedin posts. he have an output file. now we need your help to visualized the data so that we can interpret it.i also have the original export file from linkedin. i want to answer this business question:for the linkedin posts that received the highest engagement, which keywords, phrases, and hashtags were most commonly used?
i want to match up engagement late with key phrase analysis. the business question is this: for the linkedin posts that received the highest engagement, what were the most common keywords, phrases and hashtags used?
beyond watching to engagement late, please check if there is a way to also view the data according to impressions and likes.
our solution
data driven dashboard which will give the summary of most used words, keywords, phrases and also analysis of posts as per their interaction with their audience.
project deliverables
two dashboard links in which 
first dashboard represents key phrase analysis of the output by aws comprehend.
second dashboard represents the linked in data analysis 
tools used
python, google data studio
language/technique used
python 
kills used
python and data studio
database used
mongodb
web loud nerves used
google data studio
that are the technical challenges faced during project execution
one of the major problem was to match the output of aws comprehend data with the data of expel sheet to find out which posts received maximum interactions and make a dashboard out of it.
now the technical challenges were solved
working on the output.son file in code editor and comparing it to the linked in data sheet to check the accuracy of the output file with each post.
project snapshots (minimum  pictures)









project webster or
 key phrase analysis dashboard 

 linked of data analysis dashboard 


"
132,bctech2143,"
client background
client: a leading marketing firm in the usa
industry type:  marketing
services: marketing consulting
organization size: +
project objective
automated tool to extract daily review data from local service was dashboard for all clients.
project description
extracts data from a company’s google lsa page for the last  hours
the data is unloaded to the bigquery database called “lsa_review_db”.the script runs once a day and is deployed to heroku by the name “la-daily-reviews”.the script runs for all companies in the google sheet “lsa review automation master file”.the following data is unloaded:datecompany namelocationtotal reviewsverified reviewsoverall starreviewer namereview datereviewer starreviewer comment
our solution
met list of companies to monitor along with their lsa url
use selenium automatic browsing to open the review page for each company.
web scrape the data from the review page
prepare report
load to database
project deliverables
in automatic tool that runs daily and extracts and uplands review data for all companies.
tools used
selenium
heroku
sheets api
bigquery
language/technique used
python
kills used
data extraction, cleaning and summarizing. web scraping.
database used
bigquery –  lsa_review_db
web loud nerves used
heroku
that are the technical challenges faced during project execution
using selenium to automatic web browsing since it takes a large amount of ram.
now the technical challenges were solved
using the proper type of donors and managing their allotment to lower both costs as well as memory usage.
"
133,bctech2144,"
client background
client: a leading marketing firm in the usa
industry type:  marketing
services: marketing consulting
organization size: +
project objective
a real time tool to send a report of missed calls and messages to the client.
project description
extracts data from callrail database for the last  minutes
all the calls which are marked as “missed” and all messages in the data are sent in the form of a report to the client.the script runs every  minutes and is deployed to heroku by the name “missed-messages”.the data is collected only for the companies that are not marked in red in the “kissed messages ratification automation – master mile” sheet.the following data is unloaded:company namedatetimecustomer namecontact to.customer locationcall typein case of messages:company namedatetimecustomer namecontact to.to. of messagesdirection (bound/outbound)content
our solution
to provide data real time, schedule the tool to check for data every  minutes.
extract data from callrail
filter out all answered calls
prepare report
met email is from sheets
end email through sendgrid
project deliverables
in automatic tool which provides real time updated to the client along with all information about the call.
tools used
heroku
callrail api
sendgrid
sheets api
language/technique used
python
kills used
data extraction, cleaning and summarizing
database used
google fig query
web loud nerves used
heroku
that are the technical challenges faced during project execution
sending correct reports only to the companies which are active
now the technical challenges were solved
using google sheet’s cell forgetting in python
"
134,bctech2145,"
client background
client: a leading marketing firm in the usa
industry type:  marketing
services: marketing consulting
organization size: +
project objective
prepare a daily report for the companies and unload it to bigquery database. data is from callrail and contains all call information about a company.
project description
extracts data from callrail database for the last  hoursthe data is unloaded to the bigquery database called “call_status_from_callrail”.the script runs once a day and is deployed to heroku by the name “la-call-status-do”.the script runs for all companies in the callrail database.the following data is unloaded:company namestatuslocationcustomer namecall datecall timecontact nocall statuscall head
our solution
use callrail api to get data from database.
run script daily
filter out excess data
prepare report
load to bigquery
project deliverables
a working deployed automatic tool that runs once a day in the morning hours and uplands the data to bigquery database. fool is monitor daily.
tools used
heroku
callrail api
bigquery
sheets api
language/technique used
python
kills used
data extraction, cleaning, and summarizing
database used
bigquery –  call_status_from_callrail
web loud nerves used
heroku
that are the technical challenges faced during project execution
ensuring proper data unload to database
now the technical challenges were solved
proper monitoring of tool post-employment.
"
135,bctech2146,"
client background
client: a leading marketing firm in the usa
industry type:  marketing
services: marketing consulting
organization size: +
project objective
prepare a daily report for the companies and unload it to google sheets. data is from callrail and contains all call information about a company.
project description
extracts data from callrail database for the last  hours
the data is unloaded to the google sheet “all status record”the script runs once a day and is deployed to heroku by the name “call-status-to-sheets”.the script runs for all companies in the callrail database.the following data is unloaded:company namestatuslocationcustomer namecall datecall timecontact nocall statuscall head
our solution
use callrail api to get data from database.
run script daily
filter out excess data
prepare report
load to google sheets
project deliverables
a working deployed automatic tool that runs once a day in the morning hours and uplands the data to google sheets. fool is monitor daily.
tools used
heroku
callrail api
bigquery
sheets api
language/technique used
python
kills used
data extraction, cleaning and summarizing
database used
google sheets –   all status record
web loud nerves used
heroku
that are the technical challenges faced during project execution
ensuring proper amendment of data to sheets without overwrite
now the technical challenges were solved
proper monitoring before final employment
"
136,bctech2147,"
client background
client: a leading marketing firm in the usa
industry type:  marketing
services: marketing consulting
organization size: +
project objective
prepare an annual report for the companies and unload it to database. data is from callrail and contains call analysis.
project description
extracts data from callrail database for the last  year.the data is unloaded to bigquery database “lead_report_alert_callrail”the script runs once a year and is deployed to heroku by the name “lead-report-alert”.currently, the script is programme to run for only  companies (on a trial basis) – capital saw firm and wilshire saw firm.the following data is unloaded:company cameo. of calls answered. of calls missed. of calls abandoned. of calls to voicemailtotal walls
our solution
use callrail api to get data from database.
met time window to be one year.
filter out excess data
prepare report
load to bigquery
project deliverables
a working deployed automatic tool that runs once a year in the morning hours and uplands the data to bigquery. fool is in prototype phase and hence is operational for  companies.
tools used
heroku
callrail api
bigquery
language/technique used
python
kills used
data extraction, cleaning and summarizing
database used
bigquery –  lead_report_alert_callrail
web loud nerves used
heroku
that are the technical challenges faced during project execution
working on a large amount of data since a year’s data contains hundred of thousands of records
now the technical challenges were solved
optimized code for faster processing.
"
137,bctech2148,"
client background
client: a leading marketing firm in the usa
industry type:  marketing
services: marketing consulting
organization size: +
project objective
prepare a daily report for data from local service was dashboard and email to client.
project description
extracts data from the lsa dashboard for the last  hours.
the data is sent to the client email in the form of a daily report using sendgrid.the script runs every morning and is deployed to heroku by the name “lead-details-to-email”.the data is collected only for the companies that are not marked in red in the “kissed messages ratification automation – master mile” sheet.the following data is unloaded:number of leadscost her leadlead typedispute amount to be approveddispute amount approvedcost per all
our solution
use lsa api to extract data.clean the data to make it readable and dispose the data not needed.met the email id of each company from the given sheetsend an email to the client using sendgriddeploy to heroku
project deliverables
a working deployed automatic tool that runs everyday in the morning hours and sends a report to the client. fool is monitor everyday.
tools used
heroku
lsa api
sendgrid
sheets api
language/technique used
python
kills used
data extraction, cleaning, and summarizing
database used
data is not stored and is sent directly to the client
web loud nerves used
heroku
that are the technical challenges faced during project execution
ensuring a company’s data does not go to another company
now the technical challenges were solved
resting on multiple dummy email is
"
138,bctech2149,"
client background
client: a leading marketing firm in the usa
industry type:  marketing
services: marketing consulting
organization size: +
project objective
load daily data from google local service was dashboard to bigquery database.
project description
extracts data from lsa dashboard for the last  hours.
the data is unloaded to bigquery database “lsa_lead_daily_data”the script runs every morning and is deployed to heroku by the name “lead-details-to-do”.the data is collected only for the companies that are not marked in red in the “kissed messages ratification automation – master mile” sheet.the following data is unloaded:number of leadscost her leadlead typedispute amount to be approveddispute amount approvedcost per all
our solution
use lsa api to extract data.clean the data to make it readable and dispose the data not needed.load data to a bigquery database everyday at a fixed time.deploy to heroku to run the script everyday.
project deliverables
a working deployed automatic tool that runs everyday in the morning hours and uplands a report to database. fool is monitor everyday. 
tools used
heroku
lsa api
bigquery api
sheets api
language/technique used
python
kills used
data extraction, cleaning and summarizing
database used
bigquery –  lsa_lead_daily_data
web loud nerves used
heroku
that are the technical challenges faced during project execution
taking sure that the data unloaded is for the right company.
now the technical challenges were solved
monitoring daily logs and uplands for some time and making sure data was correct
"
139,bctech2150,"
client background
client: a leading consulting firm in the usa
industry type: of consulting
services: consultanting
organization size: +
project objective
for all  cases, use a random number generation that will give you numbers between  & a million [,,].  whatever generation you use, make sure to adjust the numbers so that they are between  & ,, distributed random. 

for all tasks, we will have  colors, for example in ask , when the random number selected is between  &  choose a bright color that is easily visible [i have called it or. for. ], for numbers between  &  choose another bright color [or. for. ], for numbers between  &  choose b (blue), for numbers between  & , choose r (red), and > , choose g (green). simulate these  ask scenario and represent them in a table ( x ) and collect statistics at the end. delicate the stimulation exercises for each ask with  different initial seed numbers. likewise for  other asks.
our solution
ask involves creating  expel files running a python script in jupiter notebook which contains certain inter ranges indicating certain values and some other criterion the random number range [ to  million].
there are  different tasks which have different conditions based on which need to form. simulate these  asks and represent them in a table ( x ) and collect statistics at the end. implicating the stimulation exercises for each ask with  different initial seed numbers.when using the mind and replace tax of expel to make it in the correct format with proper color. data representation in particular format and forgetting colors, next based on condition passed within expel.
project deliverables
expel mile
tools used
jupyternbsublime textms expel
language/technique used
python 
models used
to software model is being used to solve this project
kills used
python programming expel formatting
database used
to database were used stored complete data in of expel
web loud nerves used
to cloud serves were used for this project
that are the technical challenges faced during project execution
formatting expel miles 
now the technical challenges were solved
formatting expel miles discovered a lot of shortcuts available within expel to deal with data representation in particular format and learned about forgetting colors, next based on condition passed within expel.application and selecting rows and columns with shortcuts and in simplest way possible, transporting selected data and many more.
project snapshots 

  figure : ample output mile for ask  stimulation 
in total there were  conditional tasks all of them had  stimulation which needed to be performed.

"
140,bctech2151,"
client background
client: a leading financial institution in the usa
industry type: financial services & consulting
services: financial consultant
organization size: +

project objective
>to process two son file stocktwits_legacy_msg__.txt (file size =  of) & stocktwits_legacy_msg__.txt (file size = . of).
>to handle rested son for both files and after conversion into one merged data frame need to perform data structurization.
>while accepting a son file in jupyternb, i need to perform thinking as the file size is bigger and it is in son format to avoid of standstill.
>after data preprocessing i need to perform exploratory data analysis on that data.
> conditional programming to deal with data transferring to a particular older based on the column values.
project description
during the training period i was involved with  live projects, one project named ‘stocktwits data structurization’ in which i have to process huge json data which was already obtained the size of data was nearly  of need to process the data by thinking with chink size =  rows at a time. the file has rested json data within it’s attributes so abstract data from the rested columns into a new dataframe. completed handling complex rested son formed columns abstracted from rested son. when need to candle the missing data by mapping it with another index dataset further missing values for certain attributes were handled by mean value and  substitution. his task involves numerous hands operations along with multiple patron functions. further done exploratory data analysis on the cleaned dataset finding correlation matrix and clotting certain seaboard graph between strong corrected attributes.
our solution
worked on accessing son data, done tree analysis on son ample data.
both the mile was too big for reading and applying some python rode in jupyternb, so performed thinking of stocktwits_legacy_messages__.txt  with chink size =  rows at a time. similarly trying for the other file.
treated a list of all the chucked files of son data & poncet all the files in that list.
the mile has rested son data within it’s attributes so abstracted data from the rested columns into a new dataframe. completed handling complex rested son formed columns abstracted from rested son.
renamed the columns with identification. (g: ‘id’ as ‘entities_id’) likewise for others. to that while merging the data doesn’t create any issue. completed forming preprocessed is file for st son file which  output.is.
for second file size was > go so splitter the file into ten parts and then individually solved rested son for all these parts like done in the st file finally coat them into one, then handled columns arrangements and removed unwanted columns and finally removed dictionary representation from entity_sentiments column. completed forming preprocessed is file for nd son file which is output_stocktwits_.is.
the cleaned dataset finding correlation matrix and clotting certain seaboard graph between strong corrected attributes. further done exploratory data analysis on the cleaned dataset finding correlation matrix and clotting certain seaboard graph between strong corrected attributes. conditional programming to deal with data transferring to a particular older based on the column values.
project deliverables
categorized preprocessed csv filespython scriptipython of with comments on each performed code.
tools used
● jupiter notebook
● anaconda
● notepad++
● sublime next
● brackets
● jsonviewer
language/technique used
● python programming
models used
by project ‘stocktwits data structurization’ developed with a software model which makes the project high quality, reliable and cost effective.
● software model : rad(rapid application development model) model
● his project follows a rad model as our model is not forming the loop from end to the start, also my project was based on prototyping without any specific planning. in the rad model, there is less attention paid to the planning and more priority is given to the development tasks. it target developing software in a short span of time.
● advantages of rad model:
o changing requirements can be accommodated.
o progress can be measured.
o operation time can be short with use of powerful rad tools.
o productivity with fewer people in a short time.
o reduced development time.
o increases reliability of components.
o quick initial reviews occur.
o encourages customer feedback.
o integration from very beginning solved a lot of integration issues
kills used
● data dining
● data wrangling
● data visualization
● python programming including oops and exception handling
database used
to database were used, all the data was stored on google drive and local device.
web loud nerves used
to loud server were used
that are the technical challenges faced during project execution
● handling huge data and data leaning
● json data serialization.
● solving complex rested json among the data provided.
now the technical challenges were solved
● handling huge data and data leaning
solved by breaking the dataset into  stream parts as the data was too huge and was not able to read easily in jupiter of.
● json data serialization
solved by data thinking with chunk_size= which means serialization of data with processing  rows at a time.
● solving complex rested json among the data provided.
viewed the structure of the part of data in json viewer then changed the data in proper standard json format. after leading json data performing normalization of rested json data setting maximum level of normalization with specifying proper orient form. when after normalization remaining unsolved rested json was solved using dictionary conversion and structuring the data. 
project snapshots 

figure  ample input dataframe after converting outer json

figure  ample output dataframe after solving rested json and data preprocessing
"
141,bctech2152,"
client background
client: a leading financial firm in the usa
industry type: financial services & consulting
services: financial consultant
organization size: +
project objective
project “sentimental analysis on shareholder letter of companies” objective was to predict the sentiments columns shareholder letter in terms of clarity and subjectivity finally classification of data into positive, negative and neutral tone.
project description
the project ‘sentimental analysis on shareholder letter of of companies’ task involved data cleaning on shareholder letters of different companies which includes lemmatization, lower case conversion, removing special character, \n , \t , punctuation, numbers & single character and tokenization. to generate popularity and subjectivity columns for the letter  & letter  columns using the textblob library of nltk. based on the popularity categorizing it into positive, neutral  &  negative. 
our solution
letter next length variationcontraction mapping on datasetreplacing missing value with some neutral tone string like one so that cleaning doesn’t generate any issue. data leaning and preprocessing which involves : 
            i.  lemmatisation 
            ii. lower case conversion 
            iii.  removing special character 
            iv.  removing \n , \t etc
             v.  remove punctuation, numbers & single character removal
            vi.  forming list of letter data using them
tokenization and word count.used textblob library which is part of nltk for sentiment analysis.treated clarity and subjectivity column for the letter & letter columnsbased on the popularity of letter  created a letter_type column with values “positive” , “neutral”  &  “negative” category.
project deliverables
output ipython filepreprocessed dataset
tools used
● jupiter notebook
● anaconda
● notepad++
● sublime next
● brackets
● python .
language/technique used
pythonmachine learningnlp (natural language processing)
models used
by project ‘sentimental analysis on shareholder letter of companies’ developed with a software model which makes the project high quality, reliable and cost effective.
● software model : waterfall model
● for project ‘sentimental analysis on shareholder letter of of companies’ is a waterfall model as our model is not forming the loop from end to the start using textblob which predict sentiments, clarity and subjectivity as the output following the waterfall model.
kills used
hands operationsdata thinking and integrationdata visualization
database used
to database is used to complete this project.
web loud nerves used
to web cloud server was required for this work.
that are the technical challenges faced during project execution
i have worked before on tasks similar to this so there were no challenges faced but the data cleaning was a bit different and required time to complete.
now the technical challenges were solved
is discussed no technical challenges were faced during this project.
project snapshots 

figure : input data scheme

figure : output data scheme

figure : ample input dataset
figure  is hands dataframe which was fetched from goose cloud database there were  columns and  rows.

figure : ample output dataset
figure  is output hands dataframe after data cleaning and modeling of sentiment identification there are  columns and  rows.

figure : sentiments assignment based on popularity
figure  represents the identification of sentiments and tone based on popularity and subjectivity. popularity> then sentiment type is positive,  if the popularity< sentiment type is negative and if the popularity= sentiment type is neutral.

figure :  histogram representation of length of shareholder letter 
figure  is histogram plot between length of shareholder letter  among the final output dataset.

figure :  histogram representation of length of shareholder letter 
figure  is histogram plot between length of shareholder letter  among the final output dataset.

figure : low hart
"
142,bctech2153,"
client background
client: a leading marketing firm in the usa
industry type: marketing services & consulting
services: marketing consultant
organization size: +
project objective
project ‘population and community survey of america’ objective were to perform data abstraction, data structurization, data preprocessing, data leaning, and combining data from all the years listed and finally presenting insight of the data by exploratory data analysis.
project description
for project ‘population and community survey of america’ task involved fetching son and unformatted is data from numerous web links further needed to process data, handling rested json, data conversion of json data in dataframe, performing certain hands operation for feature selection and structuring data. poncet all this data into one is file then handle missing value by mapping with another dataset finally perform certain data visualization and exploratory data analysis.
our solution
nodule : data abstraction
the process of data abstraction involves collecting data from numerous web links from dear  to  and viewing the data using json viewer in tree format.
nodule : data thinking and integration
was unable to process data in hands so performed data thinking with chunksize  rows at a time for year  likewise performed for all other years data till  and finally combined all the dataframes into one containing all data from year  to .
nodule : handling complexity of rested data & format the unformatted csv miles
handling unformatted csv in proper comma separated format so that data frame can be formed. dataframe produced after merging for all the years from  to  contains a lot of rested json data among certain attributes so performed normalization of rested son forming new_columns naming them based on their attributes key.
.. nodule : data leaning and preprocessing
involves handling missing value, contraction mapping with another dataset to fill the missing state_zip_code column, handling in and -in within the dataset for some attributes and forming a new column population_ratio based on passing formula among other attributes.
.. nodule : data analysis
his step involves forming a correlation matrix to understand the relation between numerical attributes. performed exploratory data analysis on strong corrected attributes to understand pattern/relation between them. 
project deliverables
after completion of project we provided:
final preprocessed csv miles three ipython files: preprocessed dataset from year  to preprocessed dataset from year  to  data visualization and eda.
tools used
● jupiter notebook
● anaconda
● notepad++
● sublime next
● brackets
● python .
● json viewer
language/technique used
● python
● etl technique
● advanced expel formatting 
models used
by project ‘population and community survey of america’ developed with a software model which makes the project high quality, reliable and cost effective.
● software model : rad(rapid application development model) model
● his project follows a rad model as our model is not forming the loop from end to the start, also my project was based on prototyping without any specific planning. in the rad model, there is less attention paid to the planning and more priority is given to the development tasks. it target developing software in a short span of time.
● advantages of rad model:
o changing requirements can be accommodated.
o progress can be measured.
o operation time can be short with use of powerful rad tools.
o productivity with fewer people in a short time.
o reduced development time.
o increases reliability of components.
o quick initial reviews occur.
o encourages customer feedback.
o integration from very beginning solved a lot of integration issues
kills used
hands operationsdata thinking and integrationdata visualizationexploratory data analysis
database used
to database is used in this project, only used google drive for storing and transferring data.
web loud nerves used
to web server is used
that are the technical challenges faced during project execution
data leaning and filling out kissing values by data mapping with another dataset as the data was not in proper format in the another dataset.
now the technical challenges were solved
data leaning was done using a few built in hands operations to deal with kissing values, ordering data columns, data formatting, changing of data types and many more. filling of remaining kissing data from columns using outer join among the datasets and using cap function of python.
project snapshots

  figure : input data scheme for dear 

  figure : output data scheme from dear  to 

figure : dataset for dear 
figure  is hands dataset of year  which has  rows and  columns which was fetched from authenticated survey web portal, data obtained were in json format which were converted into hands dataframe likewise there are dataframes created from year  to .

figure :  output preprocessed dataset
figure  is an output preprocessed dataset from  to  which has ,, rows and  columns.

figure : describing numeric data of preprocessed dataset

figure : war plot of attribute state_name
figure  represents the bar plot among the state_name on the final output dataset from year  till .

figure : kde graph for all numerical population data column of dataset
figure  represents the vernal density estimate clot(kde) among all population estimate data columns for the preprocessed dataset. kde plot is a method for visualizing the distribution of observations in a dataset, analogous to a histogram. kde represents the data using a continuous probability density curve in one or more dimensions. blotted many more graph apart this between highly corrected attributes like pair plot, box plot, line plot etc.

  figure : low hart
"
143,bctech2154,"
client background
client: a leading marketing firm in the usa
industry type: marketing services & consulting
services: marketing consultant
organization size: +
project objective
for this project objective was to perform api data abstraction using google lsa api in gcp, automation of data fetching and storing in bigquery on daily basis, storing historical data for all active companies, fetching customer report then storing data on daily basis in bigquery also storing historical data for all companies, perform linear depression modeling on historical data for all companies and storing the modeling summary in goose sheet in a structures manner, basecamp automation with lsa daily data, creating  of dashboard in data studio for give, historical, modeling and customer report data for all companies. 
project description
for this project task was to obtain an account report and detailed lead report for a specific dates and customer_id using google local service was api service in google loud platform. further need to integrate with google bigquery database storing mcc data for all companies on a daily basis then storing historical data for all active companies. also notifying clients through email and passing messages containing daily account data in a message format to basecamp message board and campfire of respective company projects through its api all with patron programming, further deploying the script on heroku server for automatic all this task. when creating of dashboard in data studio connecting with bigquery and creating give dashboard, historical dashboard for all companies. 
in historical data for all companies, linear depression modeling needs to perform and to create modeling dashboard for all companies in data studio. further needs to do  exploratory data analysis for all companies on historical data. 
to tore customer account report for message lead and phone lead on a daily basis, script needs to be created and deployed in heroku and also need to store historical data for these companies and finally create data studio dashboard on it.
creating tales representation dashboard for two companies which involves multiple reports and blending of multiple data sources from fig query.
our solution
>> nodule : api data abstraction
which first includes generation of the access token and refresh token with the scope of google adword api for the authentication and connecting with google lsa api. when fetching daily data in json format for particular account name based on customer_id assigned in api url while fetching data. likewise generation a script that would candle data generation for all other active accounts based on their customer id.
>> nodule : data amputation and storing
converting the json data to the hands data frame forming a list of data frame for all the active accounts by looking them then deriving certain more attributes based on their handling the missing and in values. finally storing the data in google fig query database within the respective table for all accounts using bigquery api.  
>> nodule : data storing in bigquery and ratification automation
the task was to automatic modifications sent to email and to basecamp and the data transferred to the database on a daily basis by deploying the script to heroku server setting time parameter based on the new work time zone.
>> nodule : automation tools created till now:
i. lsa_accountreport_daily_bigquery tool: for automation of account report for all companies on a daily basis. scheduling it at : am in the os angeles timezone.
ii. lsa_accountreport_historical_api tool:  for storing historical data for companies for the last few years till the end date which we set.
iii. basecamp_lsa_automation: his is used to pass the la data in a message format to campfire for respective companies groups and store la data combined for all companies to messageboard and campfire at one automation python group in basecamp.
iv. lsa_daterange fool: used to store missed out data for all the companies for a few sets of days or months as per the need.
v. lsa_mainsheet_autoupdation tool: for auto updation of main sheet  ‘lsa client head’  google sheet. is daily data are fetched on the basis of this list so it is required to auto update this sheet for all the new companies entered would store information of those like company name, account id and database name.
vi. lsa_daily_customerreport tool: treated to tore lsa customer report for all companies in database ‘customerreport_phonelead’ & ‘customerreport_messagelead’ on daily basis.
vii. historical_lsa_customerreport tool:  treated to tore lsa customer report for all companies in database ‘customerreport_phonelead’ & ‘customerreport_messagelead’ storing historical data for year .
>> nodule : data studio of dashboard treated:
i. historical dashboard
ii. give dashboard
ii. customer report dashboard
iii. modeling report dashboard
iv. tales representation dashboard
project deliverables
data studio dashboard pain sheetall nodes for the deployed tools and for modeling eda and west purpose .
tools used
● pycharm
● jupiter notebook
● anaconda
● heroku
● notepad++
● google sheet api
● google lsa api on gcp
● google bigquery
● sublime next
● brackets
● jsonviewer
language/technique used
● python
● sql
models used
by project ‘google word lsa api reports automatic into google fig query database and basecamp’ developed with a software model which makes the project high quality, reliable and cost-effective.
● software model: rad(rapid application development model) model
● his project follows a rad model as our model is not forming the loop from end to the start, also my project was based on prototyping without any specific planning. in the rad model, there is less attention paid to the planning and more priority is given to the development tasks. it target developing software in a short span of time.
● advantages of rad model:
o changing requirements can be accommodated.
o progress can be measured.
o operation time can be short with the use of powerful rad tools.
o productivity with fewer people in a short time.
o reduced development time.
o increases reliability of components.
o quick initial reviews occur.
o encourages customer feedback.
o integration from the very beginning solved a lot of integration issues
kills used
● api data abstraction
● data dining and statistical modeling
● data wrangling
● employment for automation
● data visualization
● sql
● machine learning
● python programming including oops and exception handling
database used
● google restore (must for resting purpose)
● google bigquery
web loud nerves used
google bigquery loud database with up to  of of free storage is being used. 
that are the technical challenges faced during project execution
● scheduling automation of python script.
● data exceptions and application in bigquery tables.
● refresh token expiration after  days.
● data exception due to inactive companies or not updation of lsa pain sheet. 
● basecamp projected issue for transferring data to multiple companies projects.
● data studio time series clot data dispatch due to multiple account id.
now the technical challenges were solved
● scheduling automation of python script.
python library blockingscheduler were used and the timezone variable ‘of’ was set to os angeles in heroku
● data exceptions and application in bigquery tables.
       structuring sql query to deal with all the database issues which were being used in bigquery to solve those issues.
● refresh token expiration after  days.
initially ‘ruth playground’ was used for generation refresh token which was getting expired after every  days so to last it longer for more than a year we are now using the refresh token which was generate using python script where proper token endpoints and many other leaders were defined before generation the refresh token.
● data exception due to inactive companies or not updation of lsa pain sheet. 
data exception occurred while api data abstraction for few of the companies which were solved by adding more rested try and except statements after understanding issues also ‘lsa clients head’ main sheet was not being updated by other members due to which we missed out data for few of the companies which were solved by creating script which will automatically update the mainsheet when an error occurred.
● basecamp projected issue for transferring data to multiple companies projects.
his issue was solved by creating basecamp pain sheet where data was fetched now by mapping the account id of fetched data using lsa pain sheet and project id of all the basecamp companies.
● data studio time series clot data dispatch due to multiple account id.
solved by adding many parameter like setting the merits which will do a summation of all the companies on a particular day for all the account id.

"
144,bctech2155,"
client background
client: a leading healthcare teach firm in the usa
industry type: healthcare consulting
services: management consultant
organization size: +
project objective
the main objective of this project is to find the pattern in the vital signs of patients who were admitted to the hospital in past. and from this pattern, we get some ranges that help us to give early warnings.
project description
he are more interested in non-survivor patients’ vital signs as compare to survivor patients. we find patterns in vital signs that could better determine that patient died (ex. if up is below , patient in % of cases died, if up is below %, the death rate is .%) or we can take correlation which can help us to find better patterns to define death cases.
data the dataset which was used for analysis here is taken from the mimi webster. but the dataset is not in the correct format which we want, after some manipulation, we get the data ready for the analysis.
our solution
approach
to protect patient confidentially date and time is shifted to future that’s not the actual time so from shifted time column we create an extra column hour which tells us the time passed in hours since first observation in icu.after all manipulation our final dataset contain vital signs values for each observation of patients with time in separate column and also the label fo death ( or ) in another column.there are two option to deal with missing valuesdrop all rows which contain null values..will the missing values by some method using hands. 
i can’t go with st option because a major part of the data has missing values. so, i decided to go with the second option and fill missing values with the average of upper and lower values. but before that, i filtered the data and take only those patients’ data who died in a hospital or survive.
project deliverables
after performing eda which also include the removal of some impossible outlines, we come up with a result of analysis.
his result helps to build an early warning system which predict the condition of patients on the basis of their score, calculated on their condition using vital sign values.
tools used
google slab notebook
language/technique used
python
kills used
data visualizationdata analysispandasnumpyseaborn
database used
sql
mongodb
web loud nerves used
google loud
project snapshots









project webster curl

"
145,bctech2156,"
project description
meekly data – clustered bar chart for weekly budget & actual value , weekly total budget & actual value (completed)ytd data – clustered bar chart for monthly budget & actual value , monthly total budget & actual value (completed)tales history – stacked chart for yearly sales with each month sales , total yearly sale (completed)dashlet – weekly data – total weekly budget , total weekly actual , % weekly budget (completed)dashlet – ytd data – total ytd budget , total ytd actual , % ytd budget (completed)dashlet – tales history – total tales (completed)filters – select area , select city , select years (completed)

data visualization deliverables
presentationmapdashboardapi integration

data visualization tools
kibanagoogle data studiomicrosoft excelmicrosoft power of
data visualization languages
javascriptsqlpythondax

hero



"
146,bctech2157,"
client background
client: a leading consulting firm in the usa
industry type: consulting
services: management consultant
organization size: +
project objective
the main objective of this project is to build the automatic tool to buy product on amazon.
project description
his project is basically completed using selenium and python. all we have done is write a patron script for automatic using selenium.
take some click use logic to check item is in stock or not. of the item is in stock then it busy the product otherwise repeat the process again.
our solution
a simple patron code which uses selenium web driver to do all work.
project deliverables
python rode
tools used
selenium webdriver
language/technique used
python
kills used
web scraping
selenium
project snapshots





"
